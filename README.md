# TrichoCompare

Reproducible workflow for my MSc Thesis project: "Clarifying the transition from free-living to pathogenic lifestyles in Trichomonad parasites using comparative genomics"

Author: Vi Varga

Posted: 11.09.2022


## Program Installation

### General Environment

Creating a generalized `conda` environment to contain installations of programs that don't require specific dependencies (ex. old Python distributions):

```bash
conda create -n trich_thesis
conda activate trich_thesis
###
#the pandas python module was not automatically installed
#it can easily be installed via anaconda
conda install pandas
```

### Functional Annotation & Targeting Prediction

#### EggNOG

The EggNOG database is used for orthology prediction and functional annotation. The website is available here: http://eggnog5.embl.de/#/app/home

Download information for emapper, which is the associated tool used for annotation of a collection of proteins, is available here: http://eggnog5.embl.de/#/app/downloads

Installed on the server at: /usr/local/bin/emapper.py

```bash
#while EggNOG is installed on the bioinformatics server, some dependencies are missing
conda create -n eggNOG-NEW-env
conda activate eggNOG-NEW-env
conda install psutil
conda install -c conda-forge biopython

```

#### InterProScan

This program is installed on the SNIC Kebnekaise HPC. Running it requires batch scripts. 
	- Reference page for program: https://interproscan-docs.readthedocs.io/en/latest/HowToRun.html

#### DeepLoc

Not on the server, so needs to be installed:

Files obtained from DeepLoc website: https://services.healthtech.dtu.dk/service.php?DeepLoc-1.0

A license is necessary in order to install the program, but it is free for academic use.

Tool can be used online at: http://www.cbs.dtu.dk/services/DeepLoc/

```bash
#place files in the bin/ directory with trich_parab environment activated
#unpack tar file
tar -xvf deeploc-1.0.All.tar.gz
#installing dependencies
#it turns out some of these dependencies are quite old
#so need an environment with an older version of python
conda create -n DeepLoc_usage python=3.5.1
conda activate DeepLoc_usage
#Lasagne==0.2.dev1
pip install -r https://raw.githubusercontent.com/Lasagne/Lasagne/master/requirements.txt
pip install https://github.com/Lasagne/Lasagne/archive/master.zip
#installing theano generally installs the other requirements, too:
#Numpy
conda install -c anaconda numpy
#Scipy
conda install -c anaconda scipy
#Theano==1.0.1
conda install -c conda-forge theano=1.0.1
#six==1.11.0
conda install -c conda-forge six=1.11.0
#also need to install the following:
#mkl-service
conda install mkl-service
#the above isn't mentioned in the installation instructions,
#but an error code informed me it needed to be installed
#add deeploc to the .bashrc file, like so:
#export PATH="$PATH:/home/inf-47-2020/bin/DeepLoc/deeploc-1.0/bin"
#and then source it:
source ~/.bashrc
```

#### MitoFates

Website: http://mitf.cbrc.jp/MitoFates/cgi-bin/top.cgi

Download: http://mitf.cbrc.jp/MitoFates/program.html

```bash
#working from the bin/ 
tar -xvf MitoFates_1.2.tar.gz
#and it looks like this one has old dependencies, too
#so new conda environment it is
conda create -n MitoFates-env
conda activate MitoFates-env
#installing dependencies, as per the README
conda install -c conda-forge libsvm==3.0
#so that didn't work because conda doesn't go that far back
#lowest conda can do is
conda install -c conda-forge libsvm==3.16
#install necessary perl libraries
#have to do some installation of perl stuff first
#ref: https://www.cpan.org/modules/INSTALL.html
cpan App::cpanminus
#now install libraries
cpanm Math::Cephes
cpanm Perl6::Slurp
cpanm Inline::C
#this appears to have worked
#so now add the folder to the .bashrc file
#like so:
#export PATH="$PATH:/home/inf-47-2020/bin/MitoFates"
#then activate the .bashrc
source ~/.bashrc

```

#### SignalP

SignalP is a signal peptide detection software that predicts protein targeting for secretion. The program homepage is available here: http://www.cbs.dtu.dk/services/SignalP/

The executable file download link is available for academic use from here: https://services.healthtech.dtu.dk/software.php

A license is necessary in order to install the program, but it is free for academic use.

Installed on the server at: /usr/local/bin/signalp

#### TargetP




#### YLoc

Website: https://abi-services.informatik.uni-tuebingen.de/yloc/webloc.cgi?page=info

GitHub: https://github.com/KohlbacherLab/YLoc

Tutorial for the web version: https://abi-services.informatik.uni-tuebingen.de/yloc/webloc.cgi?page=help
	- This is useful to keep track of because it has explanations for the models
	- Quoting:
  		- YLoc-LowRes	predicts into 4 locations (nucleus, cytoplasm, mitochodrion, secretory pathway for the animal and fungi version) or 5 locations (in addition chloroplast for the plant version), respectively.
		- YLoc-HighRes	predicts into 9 or 10 locations, respectively. These are nucleus, cytoplasm, mitochodrion, plasma membrane, extracellular space, endoplasmic reticulum, peroxisome, and Golgi apparatus for all models. In addition, lysosome for the animal model, vacuole for the fungi model, and vacuole and chloroplast for the plant model.
  		- YLoc+	predicts into 9 or 10 locations, as described above. In addition, it allows to predict multiple locations. It was trained, in addition to the 11 main eukaryotic location classes, on 7 multi-location classes.
	- Of the above, chose to use YLoc+ since it's the most specific. The YLoc-HighRes can be done later if YLoc+ has outputs that are too complex.

o do all of the above, I do need to figure out how to access files on the host machine from within Docker, and how to copy files generated in Docker back onto the host machine (server).

Accessing files stored on the host machine/server can be done with mounting:
	- https://stackoverflow.com/questions/44876778/how-can-i-use-a-local-file-on-container
		- According to the above:
		- `docker run -v /Users/andy/mydata:/mnt/mydata myimage`
		- Makes it so that "/mnt/mydata inside the container will have access to /Users/andy/mydata on my host"
	- https://docs.docker.com/storage/bind-mounts/
		- This, though, makes it seem like `--mount` is a better option than `-v` so let's go with that

Copying files from the Docker image back to the host server is more straightforward:
	- https://stackoverflow.com/questions/22049212/docker-copying-files-from-docker-container-to-host
		- From top answer: `docker cp <containerId>:/file/path/within/container /host/path/target`

Also, note that the same Docker can be opened on multiple terminals simultaneously:
	- https://stackoverflow.com/questions/39794509/how-to-open-multiple-terminals-in-docker
	- "You can run `docker exec -it <container> bash` from multiple terminals to launch several sessions connected to the same container."

Notes for later use:
	- adding a container name: https://www.tecmint.com/name-docker-containers/
	- Basically `docker run --name <container_name> -ARGUMENTS image_id`)

Notes for editing Dockerfile:
	- https://docs.docker.com/engine/reference/builder/#user
	- https://stackoverflow.com/questions/27701930/how-to-add-users-to-docker-container
	- https://stackoverflow.com/questions/39855304/how-to-add-user-with-dockerfile
	- f

The yloc.py file can be accessed here: https://github.com/KohlbacherLab/YLoc/blob/master/YLoc/yloc.py

Singularity references: 
	- Docs: https://sylabs.io/guides/3.9/user-guide.pdf
	- Opening Docker images with Singularity: https://sylabs.io/guides/2.6/user-guide/singularity_and_docker.html
	- Instances (ie. running containers in the background): https://sylabs.io/guides/3.0/user-guide/running_services.html
	- running Docker image not in the registry: https://github.com/apptainer/singularity/issues/1537
	- Converting Dockerfile to Singularity recipe: https://stackoverflow.com/questions/60314664/how-to-build-singularity-container-from-dockerfile
	- Spython: https://pypi.org/project/spython/
	- Singularity official Spython docs: https://github.com/singularityhub/singularity-cli/blob/master/docs/pages/recipes.md
	- `--fakeroot`: https://sylabs.io/guides/3.3/user-guide/fakeroot.html
	- Building and running new container: https://singularity-tutorial.github.io/03-building/
	- The docker image that theoretically converts docker images to Singularity: https://quay.io/repository/singularity/docker2singularity?tab=info

```bash
#YLoc is best run using Docker
docker run --mount type=bind,source=/home/inf-47-2020/ThesisTrich/DataFiles/InProgressEncoding,target=/YLoc/Vi_hostDataNEW,readonly --mount type=bind,source=/home/inf-47-2020/Trich_Parab/Data_Files/DataEncoding,target=/YLoc/Vi_hostDataOG,readonly -it f8ff9f3b1898 /bin/bash
#above, the Docker command to create a Docker container 
#opening the YLoc Docker image
#with the proteome FASTA files copied into the image
#since Docker containers are separate from the computer they are run on, 
#files cannot be copied in later - files need to be copied in during container creation
#need to mount two folder because Docker can't follow symlinks 
#so need to mount the original folders containing the necessary files

```

One benefit of using the desktop version of Docker, rather than the command-line version, is that with Docker Desktop, it is possible to restart containers that have been shut down, making them accessible again. This way, new containers do not have to be made each time, and created files can be accessed later, should some issue occur.


### Orthologous Clustering 

#### Broccoli

GitHub source: https://github.com/rderelle/Broccoli

Manual found here: https://github.com/rderelle/Broccoli/blob/master/manual_Broccoli_v1.2.pdf

Broccoli has 3 main dependencies:
  - ete3 library
  - Diamond version 0.9.30 or above (https://github.com/bbuchfink/diamond)
  - FastTree version 2.1.11 or above (single-thread version) (http://www.microbesonline.org/fasttree/)

```bash
#clone the repository into the bin/
git clone https://github.com/rderelle/Broccoli
#now add the folder to the .bashrc
#like so:
#export PATH="$PATH:/home/inf-47-2020/bin/Broccoli"
#and source it
source ~/.bashrc
#going to create an environment for it, as per the instructions
#(whew, I'm going to have A Time keeping all of these conda environments straight)
conda create -n env-broccoli python=3.6 ete3
conda activate env-broccoli
#next install Diamond
#in the manual Diamond & FatTree are installed before the environment is made
#but as I'm installing them via conda...
conda install -c bioconda diamond
#next install FastTree
conda install -c bioconda fasttree

```

#### OrthoFinder

OrthoFinder can be installed from the source on GitHub (https://github.com/davidemms/OrthoFinder), or via `conda`:

```bash
#in the trich_parab conda environment
conda install orthofinder
conda update orthofinder
```

#### ProteinOrtho

Website/GitLab: https://gitlab.com/paulklemm_PHD/proteinortho#readme

Manual: https://www.bioinf.uni-leipzig.de/Software/proteinortho/manual.html

```bash
#installation with conda
conda create -n env-protOrtho
conda activate env-protOrtho
conda install proteinortho
```

#### SonicParanoid

Updated version of InParanoid. Link: http://iwasakilab.k.u-tokyo.ac.jp/sonicparanoid/

```bash
#in the main trich_thesis conda environment
conda install -c bioconda sonicparanoid
###
#or not, there's more specific instructions than that
conda create -n sonicparanoid python=3.8
conda activate sonicparanoid
conda install numpy filetype pandas scipy biopython mypy psutil scikit-learn
conda install -c bioconda sonicparanoid
#now test installation
sonicparanoid-get-test-data -o .
# /home/inf-47-2020/sonicparanoid_test/
# INFO: all test files were succesfully copied to
# /home/inf-47-2020/sonicparanoid_test/
# Go inside the directory
# /home/inf-47-2020/sonicparanoid_test/
# and type
# sonicparanoid -i ./test_input -o ./test_output -m fast -t 4
cd sonicparanoid_test
sonicparanoid -i ./test_input -o ./test_output -p my_first_run -t 4
#yup that all seems to have worked properly
```


### Ancestral State Reconstruction

#### COUNT

From the website (here: http://www.iro.umontreal.ca/~csuros/gene_content/count.html): Count is a software package for the evolutionary analysis of homolog family sizes (phylogenetic profiles), or other numerical census-type characters along a phylogeny.

Note that this program requires Java to be installed in order for it to run.

```bash
#program installation
#following directions on the program website: http://www.iro.umontreal.ca/~csuros/gene_content/count.html
#placed the .tgz file into the bin/ directory on the server
tar -zxvf Count.tgz
#need to add the new ~/bin/Count/ directory to the .bashrc file
#then can use program like so:
java -jar Count.jar
###
#the program runs on Java, and has an interactive window
#which can be accessed remotely using Xming & PuTTY to do X-11 forwarding
#however, it is much simpler to install and use the program from my local Windows computer
#in that case, I merely unpacked the download Count.zip file
#this created a folder from which the program can be opened by clicking on an icon

```

#### MAFFT

Installation: 
 - Homepage: https://mafft.cbrc.jp/alignment/software/
 - On the command line for Linux, can be installed using `conda`
   - https://anaconda.org/bioconda/mafft
   - https://bioconda.github.io/recipes/mafft/README.html
 - Manual: https://mafft.cbrc.jp/alignment/software/manual/manual.html
 - Tips (not in manual): https://mafft.cbrc.jp/alignment/software/tips0.html

```bash
#creating a new environment for the program
conda create -n env-MAFFT
conda activate env-MAFFT
conda install -c bioconda mafft
#just to be safe: 
conda update mafft
#ok, good to go
```

#### trimAl

Alignment trimming tool: 
 - Article: https://academic.oup.com/bioinformatics/article/25/15/1972/213148
 - Homepage: http://trimal.cgenomics.org/
 - GitHub: https://github.com/inab/trimal
 - Command line usage manual: http://trimal.cgenomics.org/use_of_the_command_line_trimal_v1.2
 - Supplementary material: http://trimal.cgenomics.org/_media/manual.b.pdf

```bash
#installation - trying first with conda
conda create -n env-trimAl
conda activate env-trimAl
conda install trimal
```

#### IQ-TREE

 - Homepage: http://www.iqtree.org/
 - Manual: http://www.iqtree.org/doc/iqtree-doc.pdf
 - Creates Maximum Likelihood (ML) tree, but Courtney says this is what everyone she's seen use ALE uses for input, so it should be fine
 - has ultrafast bootstraps (unclear if needs `iqtree -s example.phy -m TIM2+I+G -B 1000` or just need to specify the `-B` argument)
 - The `-B` is the argument that's needed; in comparison, `-b` does non-parametric bootstraps

```bash
#installation can be done using conda
conda create -n env-IQ-TREE
conda activate env-IQ-TREE
conda install -c bioconda iqtree
```

#### ALE 

Installation:
 - Article: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3797637/
 - GitHub page: https://github.com/ssolo/ALE
 - Docker registry: https://hub.docker.com/r/boussau/alesuite
 - Martijn et al. 2020 ALE workflow: https://github.com/maxemil/haloarchaea-evolution/blob/master/ALE_reconstruction.sh

```bash
#on my local computer: 
singularity pull ale.sif docker://boussau/alesuite:latest
#well, it made a file, at least! now we have to see whether it works on Rackham, I guess
#transferred over with FileZilla
#using the Singularity image created locally & transferred to the HPC
#need to make it executable
chmod +x ale.sif

```


### Other

#### BUSCO

Manual: https://busco.ezlab.org/busco_userguide.html


```bash
#installation
conda create -n env-BUSCO -c conda-forge -c bioconda busco=5.3.0
conda activate env-BUSCO

```

#### Seqkit

Allows the manipulation and filtration of FASTA files

Reference page: https://bioinf.shenwei.me/seqkit/

```bash
#can be installed via conda
conda install seqkit
```

#### Transdecoder

According to their own description on GitHub, "TransDecoder identifies candidate coding regions within transcript sequences" (source: https://github.com/TransDecoder/TransDecoder/wiki).

GitHub: https://github.com/TransDecoder/TransDecoder

```bash
#there is a conda package for the program
#install within the trich_thesis conda environment
conda install -c bioconda transdecoder
```

#### Python & Spyder

The Spyder IDE was used on a local Windows computer for testing Python scripts.

Two non-standard Python modules were utilized in this research process.

The numpy library aids in manipulation of arrays. More information can be found at their website, here: https://numpy.org/

The pandas library facilitates the manipulation of dataframes in Python. More information is available on their website, here: https://pandas.pydata.org/

Both libraries can be installed with `conda`, like so:

```bash
#install pandas
conda install -c anaconda pandas
#install numpy
conda install -c anaconda numpy
```


#### R & RStudio

The RStudio IDE was used on a local Windows computer for analysis of data.

The UpSetR package was used to create an UpSet plot. The package is distributed by CRAN; more information is available here: https://cran.r-project.org/web/packages/UpSetR/index.html

It can be installed as follows:

```R
#package installation
install.packages("upsetr")
```


#### FigTree

This is a phylogenetic tree visualization and analysis software. The program code can be downloaded from here: https://github.com/rambaut/figtree/releases

**Note**: FigTree requires at least Java 1.5 to run. Java can be downloaded from the website, here: https://www.oracle.com/java/technologies/javase-downloads.html

```bash
#in the bin/ in the fistulina environment on the server
tar -zxvf FigTree_v1.4.4.tgz
#adding the program to the PATH:
#export PATH="$PATH:/home/inf-47-2020/bin/FigTree_v1.4.4/bin"
#source the .bashrc
source ~/.bashrc
###
#for installation on Windows 10:
#download the FigTree_v1.4.4.zip file from the GitHub site above
#extract the files from the archive
#copy the FigTree_v1.4.4/ directory to a desired Program storage directory
#run the application by double-clicking the application icon
#which is in the unzipped folder
```


#### Inkscape

Inkscape is a vector graphics editor that was used in order to edit SVG output files into the finalized figures that were used in the report. The GUI can be downloaded from here: https://inkscape.org/release/inkscape-1.1/


#### draw.io/diagrams.net

The draw.io software is a freely available software usable for creating simple diagrams. Both an online-only and desktop version of the tool are available from the website, here: https://drawio-app.com/





## Data Collection

Data for this project obtained from the NCBI database, GiardiaDB, EukProt and in-house sources.

Protein datasets were downloaded from the NCBI as follows:
  - Accessed BioProject page for organismal genome
  - Accessed protein list
  - Downloaded proteins using "Send to:" option at top of page
    - Choose Destination: File
    - Format: FASTA
    - Sort by: Default order
    - Create File
  - This creates generates a FASTA file of the protein sequences in multiline FASTA format, with the default name "sequence.fasta"
  - Files were renamed according to the name of the organism before being transferred to the server

Proteome and transcriptome datasets from GiardiaDB (https://giardiadb.org/giardiadb/app) and EukProt (https://figshare.com/articles/dataset/EukProt_a_database_of_genome-scale_predicted_proteins_across_the_diversity_of_eukaryotic_life/12417881) were downloaded directly from the databases. 

Below, a table summarizing the sources of data and the format that data was obtained in: 

| **Species/Strain** | **Phylum** | **Accession Information** | **Database** | **Data Format** |
| ------------ | ------------- | ------------- | ------------ | --------- |
| _Aduncisulcus paluster_ | Fornicata | EP00764 | EukProt | Proteome |
| _Anaeramoeba flamelloides_ (BS) | Anaeramobidae | _obtained in-house_ | _In-House_ | Proteome |
| _Anaeramoeba flamelloides_ (SC) | Anaeramobidae | _obtained in-house_ | _In-House_ | Proteome |
| _Anaeramoeba ignava (BM)_ | Anaeramobidae | _obtained in-house_ | _In-House_ | Proteome |
| _Anaeramoeba lanta_ | Anaeramobidae | _obtained in-house_ | _In-House_ | Proteome |
| _Barthelona_ sp. PAP020 | Barthelona | EP00792 | EukProt | Transcriptome |
| _Carpediemonas membranifera_ | Fornicata | PRJNA719540 | NCBI | Proteome |
| _Chilomastix caulleryi_ | Fornicata | EP00766 | EukProt | Proteome |
| _Chilomastix cuspidata_ | Fornicata | EP00767 | EukProt | Proteome |
| _Dientamoeba fragilis_ | Parabasalia | PRJNA284312 | NCBI | Transcriptome (predicted by Stairs) |
| _Dysnectes brevis_ | Fornicata | EP00768 | EukProt | Proteome |
| _Ergobibamus cyprinoides_ | Fornicata | EP00769 | EukProt | Proteome |
| _Giardia intestinalis_ - EukProt version | Fornicata | EP00701 | EukProt | Proteome |
| _Giardia intestinalis_ - NCBI version | Fornicata | PRJNA1439 | NCBI | Proteome |
| _Giardia intestinalis_ ADH | Fornicata | PRJNA77981 | GiardiaDB | Proteome |
| _Giardia intestinalis_ BGS | Fornicata | PRJNA33815 | GiardiaDB | Proteome |
| _Giardia intestinalis_ BGS B | Fornicata | PRJNA77979 | GiardiaDB | Proteome |
| _Giardia intestinalis_ EP15 | Fornicata | PRJNA39315 | GiardiaDB | Proteome |
| _Giardia muris_ | Fornicata | PRJNA524057 | NCBI | Proteome |
| _Histomonas meleagridis_ 2021 version | Parabasalia | PRJNA594289 | NCBI | Proteome |
| _Histomonas meleagridis_ 2018 version | Parabasalia | PRJEB19109 | NCBI | Transcriptome (predicted by Stairs) |
| _Monocercomonoides exilis_ | Preaxostyla | PRJNA304271 | GiardiaDB | Proteome |
| _Kipferlia bialata_ | Fornicata | PRJDB5223 | NCBI | Proteome |
| _Paratrimastix pyriformis_ | Preaxostyla | EP00708 | EukProt | Proteome |
| _Pentatrichomonas hominis_ | Parabasalia | PRJNA341413 | NCBI | Transcriptome (predicted by Stairs) |
| _Spironucleus salmonicida_ | Fornicata | PRJNA60811 | NCBI | Proteome |
| _Tetratrichomonas gallinarum_ | Parabasalia | PRJNA305708 | NCBI | Transcriptome (predicted by Stairs) |
| _Trepomonas_ sp. PC1 | Fornicata | EP00703 | EukProt | Proteome |
| _Trichomonas vaginalis_ - GenBank version | Parabasalia | PRJNA343635 | NCBI | Proteome |
| _Trichomonas vaginalis_ - RefSeq version | Parabasalia | PRJNA16084 | NCBI | Proteome |
| _Trimastix marina_ | Preaxostyla | EP00771 | EukProt | Proteome |
| _Tritrichomonas foetus_ | Parabasalia | PRJNA345179 | NCBI | Proteome |

Note that datasets indicated to be "Transcriptome (predicted by Stairs)" are transcriptomic datasets predicted into proteomes by Dr. Courtney Stairs, prior to this project. The gene prediction process for these proteomes is therefore not included in this workflow. 

In total, the dataset utilized in this project included 32 proteomes.

The protein prediction from the transcriptome for the Barthelona species was accomplished with the Transdecoder software: 

```bash
#need this program to translate the Barthelona transcriptome to proteome form
#following along with the instructions in the manual:
TransDecoder.LongOrfs -t target_transcripts.fasta
TransDecoder.Predict -t target_transcripts.fasta
#now applying it
TransDecoder.LongOrfs -t Trinity_BarthelonaPAP020_Hiseq.fasta
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/compute_base_probs.pl Trinity_BarthelonaPAP020_Hiseq.fasta 0 > /home/inf-47-2020/ThesisTrich/DataFiles/RawData/Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/base_freqs.dat
# -first extracting base frequencies, we'll need them later.
# - extracting ORFs from transcripts.
# -total transcripts to examine: 29251
# [29200/29251] = 99.83% done    CMD: touch /home/inf-47-2020/ThesisTrich/DataFiles/RawData/Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir.__checkpoints_longorfs/TD.longorfs.ok
# #################################
# ### Done preparing long ORFs.  ###
# ##################################
#         Use file: /home/inf-47-2020/ThesisTrich/DataFiles/RawData/Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.pep  for Pfam and/or BlastP searches to enable homology-based coding region identification.
#         Then, run TransDecoder.Predict for your final coding region predictions.
TransDecoder.Predict -t Trinity_BarthelonaPAP020_Hiseq.fasta
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/get_top_longest_fasta_entries.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds 5000 5000 > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_longest_5000
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/exclude_similar_proteins.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_longest_5000 > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_longest_5000.nr
# -skipping training candidate: c1133_g1_i1.p1, not unique enough
#[[MANY SIMILAR LINES REMOVED]]
# -skipping training candidate: c8616_g1_i1.p1, not unique enough
# -skipping training candidate: c8678_g1_i1.p1, not unique enough
# -skipping training candidate: c8681_g1_i2.p1, not unique enough
# -skipping training candidate: c12690_g1_i1.p1, not unique enough
#         -redundancy-minimized set includes 2751 / 5000 = 55.02%
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/get_top_longest_fasta_entries.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_longest_5000.nr 500 > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_500_longest
# PCT_GC: 31.6
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/seq_n_baseprobs_to_loglikelihood_vals.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_500_longest Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/base_freqs.dat > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/hexamer.scores
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/score_CDS_likelihood_all_6_frames.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/hexamer.scores > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.scores
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/select_best_ORFs_per_transcript.pl --gff3_file Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.gff3 --cds_scores Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.scores  --min_length_auto_accept 555  > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.best_candidates.gff3
# Selecting best orfs
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/train_start_PWM.pl --transcripts Trinity_BarthelonaPAP020_Hiseq.fasta --selected_orfs Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_500_longest --out_prefix Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement
# Training start codon pattern recognition* Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/build_atgPWM_+-.pl  --transcripts Trinity_BarthelonaPAP020_Hiseq.fasta  --selected_orfs Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.top_500_longest  --out_prefix Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement --pwm_left 20 --pwm_right 10
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/feature_scoring.+-.pl  --features_plus Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.+.features  --features_minus Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.-.features  --atg_position 20  > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.feature.scores
# -round: 1
# -round: 2
# -round: 3
# -round: 4
# -round: 5
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/feature_scores_to_ROC.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.feature.scores > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.feature.scores.roc
# -parsing scores
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/plot_ROC.Rscript Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.feature.scores.roc || :
# Error in library(ggplot2) : there is no package called ‘ggplot2’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/compute_AUC.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.feature.scores.roc
# Error in library(ggplot2) : there is no package called ‘ggplot2’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/make_seqLogo.Rscript Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.+.pwm || :
# Error in library(seqLogo) : there is no package called ‘seqLogo’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/make_seqLogo.Rscript Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.-.pwm || :
# Error in library(seqLogo) : there is no package called ‘seqLogo’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/deplete_feature_noise.pl  --features_plus Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.+.features  --pwm_minus Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.-.pwm  --out_prefix Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced
# num features: 110       num_incorporate: 33
# -feature swap of score: 0.807710411477514 instead of -2.85760199240649
# -feature swap of score: 0.263269757338389 instead of -2.12183171948163
# -feature swap of score: 0.654841227006891 instead of -0.528398995118775
# -feature swap of score: 4.74042297604209 instead of -0.304318253083072
# -feature swap of score: 2.96942391583435 instead of -0.0900141561327352
# -feature swap of score: 1.26732172499095 instead of -0.0364113315403367
# -feature swap of score: 1.30665066025331 instead of 0.0554953874286377
# -feature swap of score: 2.24535900476146 instead of 0.563983236956542
# -feature swap of score: 2.01744511747088 instead of 0.0686634793168993
# -feature swap of score: 2.20021337904899 instead of 0.806454736733007
# -feature swap of score: 1.74684198611861 instead of 0.831690742669409
# -feature swap of score: 3.24145339063041 instead of 0.537409469284042
# -feature swap of score: 2.83874955142257 instead of 2.07025674985932
# -feature swap of score: 3.55105182103825 instead of 1.69406089923935
# -feature swap of score: 5.25106418753495 instead of 2.29358684391978
# -feature swap of score: 3.1891029752027 instead of 2.1359749957043
# -feature swap of score: 7.91367547088255 instead of 2.02937867390441
# -feature swap of score: 5.52591247423573 instead of 2.08467118054264
# -feature swap of score: 6.2683868559992 instead of 2.4081783731738
# -feature swap of score: 6.34476244515701 instead of 2.71136706113914
# -feature swap of score: 4.40359504017915 instead of 2.61693796141225
# -num feature swaps: 21
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/feature_scoring.+-.pl  --features_plus Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.+.features  --features_minus Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.-.features  --atg_position 20  > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.feature.scores
# -round: 1
# -round: 2
# -round: 3
# -round: 4
# -round: 5
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/feature_scores_to_ROC.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.feature.scores > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.feature.scores.roc
# -parsing scores
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/plot_ROC.Rscript Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.feature.scores.roc || :
# Error in library(ggplot2) : there is no package called ‘ggplot2’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/compute_AUC.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.feature.scores.roc
# Error in library(ggplot2) : there is no package called ‘ggplot2’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/PWM/make_seqLogo.Rscript Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/start_refinement.enhanced.+.pwm || :
# Error in library(seqLogo) : there is no package called ‘seqLogo’
# Execution halted
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/start_codon_refinement.pl --transcripts Trinity_BarthelonaPAP020_Hiseq.fasta --gff3_file Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.best_candidates.gff3 --workdir Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.best_candidates.gff3.revised_starts.gff3
# Refining start codon selections.
# -number of revised start positions: 1317
# * Running CMD: cp Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/longest_orfs.cds.best_candidates.gff3.revised_starts.gff3 Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.gff3
# copying output to final output file: Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.gff3* Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/gff3_file_to_bed.pl Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.gff3 > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.bed
# Making bed file: Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.bed
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/gff3_file_to_proteins.pl --gff3 Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.gff3 --fasta Trinity_BarthelonaPAP020_Hiseq.fasta  --genetic_code Universal > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.pep
# Making pep file: Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.pep
# * Running CMD: /home/inf-47-2020/miniconda3/envs/trich_thesis/opt/transdecoder/util/gff3_file_to_proteins.pl --gff3 Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.gff3 --fasta Trinity_BarthelonaPAP020_Hiseq.fasta --seqType CDS  --genetic_code Universal > Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.cds
# Making cds file: Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.cds
# transdecoder is finished.  See output files Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.*
cd Barthelona/
#above, made a folder for all of the files, then transferred them in with `mv`
#folder looks like below:
ls
# pipeliner.1569480.cmds
# pipeliner.1569691.cmds
# pipeliner.1569738.cmds
# Trinity_BarthelonaPAP020_Hiseq.fasta
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.bed
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.cds
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir/
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir.__checkpoints/
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder_dir.__checkpoints_longorfs/
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.gff3
# Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.pep
#Of these, the .pep file is the one we need
#Quoting the wiki: "transcripts.fasta.transdecoder.pep : peptide sequences for the final candidate ORFs; all shorter candidates within longer ORFs were removed."
#for ease of use, let's make a copy of the file with a new name
cp Trinity_BarthelonaPAP020_Hiseq.fasta.transdecoder.pep EP00792_Barthelona_sp_PAP020.fasta
###
### NOTE: Chose the Hiseq instead of the Miseq file because it was larger, and had simpler FASTA header names
###
#let's check real quick if multi-line or single-line FASTA, so I know where to head next
wc -l EP00792_Barthelona_sp_PAP020.fasta
#145939 EP00792_Barthelona_sp_PAP020.fasta
grep -c ">" EP00792_Barthelona_sp_PAP020.fasta
#21618
#which means it's multi-line FASTA format
#so that's where we begin reformatting

```

### File Linking & Restructuring

The amino acid FASTA files needed to be set up in the same format, in order to streamline the analysis.

```bash
#The following are copied from the preliminary project's file preparation
#I don't like multi-line, so I'll convert them to 1-line FASTAs
#not possible to overwrite the files, so saving to new files
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GCA_000002825.1_ASM282v1_protein.faa \
  > Trichomonas_vaginalis_GenBank_start.PRJNA16084.fasta
awk '{if (NR==1 && NF==0) next};1' Trichomonas_vaginalis_GenBank_start.PRJNA16084.fasta \
  > Trichomonas_vaginalis_GenBank.PRJNA16084.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GCF_000002825.2_ASM282v1_protein.faa \
  > Trichomonas_vaginalis_RefSeq_start.G3.fasta
awk '{if (NR==1 && NF==0) next};1' Trichomonas_vaginalis_RefSeq_start.G3.fasta \
  > Trichomonas_vaginalis_RefSeq.G3.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GCA_001839685.1_ASM183968v1_protein.faa \
  > Trichomonas_foetus_start.PRJNA345179.fasta
awk '{if (NR==1 && NF==0) next};1' Trichomonas_foetus_start.PRJNA345179.fasta \
  > Trichomonas_foetus.PRJNA345179.fasta
#final checks
grep -c ">" *.fasta
#Dientamoeba_fragilis.43352.aa.fasta:6558
#Histomonas_meleagridis.135588.aa.fasta:6982
#Pentatrichomonas_hominis.5728.aa.fasta:44819
#Tetratrichomonas_gallinarum.5730.aa.fasta:67510
#Trichomonas_foetus.PRJNA345179.fasta:25030
#Trichomonas_vaginalis_GenBank.PRJNA16084.fasta:59681
#Trichomonas_vaginalis_RefSeq.G3.fasta:59679
wc -l *.fasta
#    13116 Dientamoeba_fragilis.43352.aa.fasta
#    13964 Histomonas_meleagridis.135588.aa.fasta
#    89638 Pentatrichomonas_hominis.5728.aa.fasta
#   135020 Tetratrichomonas_gallinarum.5730.aa.fasta
#    50060 Trichomonas_foetus.PRJNA345179.fasta
#   119362 Trichomonas_vaginalis_GenBank.PRJNA16084.fasta
#   119358 Trichomonas_vaginalis_RefSeq.G3.fasta
#   540518 total
#they are now all 1-line FASTAs
###
#now I need to check that the files from Jon are also 1-line
grep -c ">" SC_newprots_may21.fasta
#29853
wc -l SC_newprots_may21.fasta
#352094 SC_newprots_may21.fasta
grep -c ">" BM_newprots_may21.fasta
#14817
wc -l BM_newprots_may21.fasta
#161786 BM_newprots_may21.fasta
grep -c ">" BS_newprot_may21.fasta
#29791
wc -l BS_newprot_may21.fasta
#346782 BS_newprot_may21.fasta
#so no, these are multi-line FASTAs
#time to turn them into 1-line FASTAs
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' < SC_newprots_may21.fasta \
  > SC_newprots_may21.anaeramoeba_start.fasta
awk '{if (NR==1 && NF==0) next};1' SC_newprots_may21.anaeromoeba_start.fasta > SC_newprots_may21.anaeromoeba.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' < BM_newprots_may21.fasta \
  > BM_newprots_may21.anaeromoeba_start.fasta
awk '{if (NR==1 && NF==0) next};1' BM_newprots_may21.anaeromoeba_start.fasta > BM_newprots_may21.anaeromoeba.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' < BS_newprot_may21.fasta \
  > BS_newprots_may21.anaeromoeba_start.fasta
awk '{if (NR==1 && NF==0) next};1' BS_newprots_may21.anaeromoeba_start.fasta > BS_newprots_may21.anaeromoeba.fasta
#final checks:
grep -c ">" *.fasta
#BM_newprots_may21.anaeromoeba.fasta:14817
#BS_newprots_may21.anaeromoeba.fasta:29791
#Dientamoeba_fragilis.43352.aa.fasta:6558
#Histomonas_meleagridis.135588.aa.fasta:6982
#Pentatrichomonas_hominis.5728.aa.fasta:44819
#SC_newprots_may21.anaeromoeba.fasta:29853
#Tetratrichomonas_gallinarum.5730.aa.fasta:67510
#Trichomonas_foetus.PRJNA345179.fasta:25030
#Trichomonas_vaginalis_GenBank.PRJNA16084.fasta:59681
#Trichomonas_vaginalis_RefSeq.G3.fasta:59679
wc -l *.fasta
#    29634 BM_newprots_may21.anaeromoeba.fasta
#    59582 BS_newprots_may21.anaeromoeba.fasta
#    13116 Dientamoeba_fragilis.43352.aa.fasta
#    13964 Histomonas_meleagridis.135588.aa.fasta
#    89638 Pentatrichomonas_hominis.5728.aa.fasta
#    59706 SC_newprots_may21.anaeromoeba.fasta
#   135020 Tetratrichomonas_gallinarum.5730.aa.fasta
#    50060 Trichomonas_foetus.PRJNA345179.fasta
#   119362 Trichomonas_vaginalis_GenBank.PRJNA16084.fasta
#   119358 Trichomonas_vaginalis_RefSeq.G3.fasta
#   689440 total
#looks good!
###
#The following are the file prep steps for the new files
#first, restructure new files into single-line FASTA format
#for each of these, run the first line of awk code in the RawData/ directory
#then run the second line in the DataFiles/ directory
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Carpediemonas_membranifera.PRJNA719540.fasta \
  > Carpediemonas_membranifera_start.PRJNA719540.fasta
#note that the above awk code converts multi-line fast to single-line
#however, a blank line is included at the start of the files,
#which is removed with the following line of awk code
#also note that awk doesn't overwrite files easily,
#which is why multiple files are created here
awk '{if (NR==1 && NF==0) next};1' RawData/Carpediemonas_membranifera_start.PRJNA719540.fasta \
  > DataReformatted/Carpediemonas_membranifera.PRJNA719540.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Histomonas_meleagridis.PRJNA594289.fasta \
  > Histomonas_meleagridis_start.PRJNA594289.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Histomonas_meleagridis_start.PRJNA594289.fasta \
  > DataReformatted/Histomonas_meleagridis.PRJNA594289.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Giardia_intestinalis.PRJNA1439.fasta \
  > Giardia_intestinalis_start.PRJNA1439.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Giardia_intestinalis_start.PRJNA1439.fasta \
  > DataReformatted/Giardia_intestinalis.PRJNA1439.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Giardia_muris.PRJNA524057.fasta \
  > Giardia_muris_start.PRJNA524057.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Giardia_muris_start.PRJNA524057.fasta \
  > DataReformatted/Giardia_muris.PRJNA524057.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Kipferlia_bialata.PRJDB5223.fasta \
  > Kipferlia_bialata_start.PRJDB5223.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Kipferlia_bialata_start.PRJDB5223.fasta \
  > DataReformatted/Kipferlia_bialata.PRJDB5223.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Spironucleus_salmonicida.PRJNA60811.fasta \
  > Spironucleus_salmonicida_start.PRJNA60811.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Spironucleus_salmonicida_start.PRJNA60811.fasta \
  > DataReformatted/Spironucleus_salmonicida.PRJNA60811.fasta
#now check in the DataReformatted/ directory if everything worked
wc -l *
#16544 Carpediemonas_membranifera.PRJNA719540.fasta
# 9930 Giardia_intestinalis.PRJNA1439.fasta
# 9320 Giardia_muris.PRJNA524057.fasta
#44512 Histomonas_meleagridis.PRJNA594289.fasta
#34538 Kipferlia_bialata.PRJDB5223.fasta
#17334 Spironucleus_salmonicida.PRJNA60811.fasta
#132178 total
grep -c ">" *
#Carpediemonas_membranifera.PRJNA719540.fasta:8272
#Giardia_intestinalis.PRJNA1439.fasta:4965
#Giardia_muris.PRJNA524057.fasta:4660
#Histomonas_meleagridis.PRJNA594289.fasta:22256
#Kipferlia_bialata.PRJDB5223.fasta:17269
#Spironucleus_salmonicida.PRJNA60811.fasta:8667
#next, link FASTA files from preliminary project to new file locations
ln -s /home/inf-47-2020/Trich_Parab/Data_Files/MainData/* .
#double check that it worked
ls
#BM_newprots_may21.anaeromoeba.fasta     SC_newprots_may21.anaeromoeba.fasta
#BS_newprots_may21.anaeromoeba.fasta     Tetratrichomonas_gallinarum.5730.aa.fasta
#Dientamoeba_fragilis.43352.aa.fasta     Trichomonas_foetus.PRJNA345179.fasta
#Histomonas_meleagridis.135588.aa.fasta  Trichomonas_vaginalis_GenBank.PRJNA16084.fasta
#Pentatrichomonas_hominis.5728.aa.fasta  Trichomonas_vaginalis_RefSeq.G3.fasta
#ok after some data reorganization, too, let's reformat the new files
#EukProt database:
ls
#EP00701_Giardia_intestinalis.fasta        EP00767_Chilomastix_cuspidata.fasta
#EP00708_Paratrimastix_pyriformis.fasta    EP00768_Dysnectes_brevis.fasta
#EP00764_Aduncisulcus_paluster.fasta       EP00769_Ergobibamus_cyprinoides.fasta
#EP00765_Carpediemonas_membranifera.fasta  EP00770_Monocercomonoides_exilis.fasta
#EP00766_Chilomastix_caulleryi.fasta       EP00771_Trimastix_marina.fasta
wc -l *
#   44750 EP00701_Giardia_intestinalis.fasta
#   42688 EP00708_Paratrimastix_pyriformis.fasta
#   81035 EP00764_Aduncisulcus_paluster.fasta
#   82280 EP00765_Carpediemonas_membranifera.fasta
#   24698 EP00766_Chilomastix_caulleryi.fasta
#   72335 EP00767_Chilomastix_cuspidata.fasta
#   72669 EP00768_Dysnectes_brevis.fasta
#   24219 EP00769_Ergobibamus_cyprinoides.fasta
#  244566 EP00770_Monocercomonoides_exilis.fasta
#   34967 EP00771_Trimastix_marina.fasta
#  724207 total
grep -c ">" *
#EP00701_Giardia_intestinalis.fasta:6502
#EP00708_Paratrimastix_pyriformis.fasta:7108
#EP00764_Aduncisulcus_paluster.fasta:15474
#EP00765_Carpediemonas_membranifera.fasta:13729
#EP00766_Chilomastix_caulleryi.fasta:6869
#EP00767_Chilomastix_cuspidata.fasta:8607
#EP00768_Dysnectes_brevis.fasta:9689
#EP00769_Ergobibamus_cyprinoides.fasta:4809
#EP00770_Monocercomonoides_exilis.fasta:16767
#EP00771_Trimastix_marina.fasta:4251
#this is multi-line, so need to do the same as above
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00701_Giardia_intestinalis.fasta \
  > EP00701_Giardia_intestinalis_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00701_Giardia_intestinalis_start.fasta \
  > DataReformatted/EP00701_Giardia_intestinalis.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00708_Paratrimastix_pyriformis.fasta \
  > EP00708_Paratrimastix_pyriformis_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00708_Paratrimastix_pyriformis_start.fasta \
  > DataReformatted/EP00708_Paratrimastix_pyriformis.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00764_Aduncisulcus_paluster.fasta \
  > EP00764_Aduncisulcus_paluster_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00764_Aduncisulcus_paluster_start.fasta \
  > DataReformatted/EP00764_Aduncisulcus_paluster.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00765_Carpediemonas_membranifera.fasta \
  > EP00765_Carpediemonas_membranifera_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00765_Carpediemonas_membranifera_start.fasta \
  > DataReformatted/EP00765_Carpediemonas_membranifera.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00766_Chilomastix_caulleryi.fasta \
  > EP00766_Chilomastix_caulleryi_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00766_Chilomastix_caulleryi_start.fasta \
  > DataReformatted/EP00766_Chilomastix_caulleryi.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00767_Chilomastix_cuspidata.fasta \
  > EP00767_Chilomastix_cuspidata_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00767_Chilomastix_cuspidata_start.fasta \
  > DataReformatted/EP00767_Chilomastix_cuspidata.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00768_Dysnectes_brevis.fasta \
  > EP00768_Dysnectes_brevis_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00768_Dysnectes_brevis_start.fasta \
  > DataReformatted/EP00768_Dysnectes_brevis.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00769_Ergobibamus_cyprinoides.fasta \
  > EP00769_Ergobibamus_cyprinoides_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00769_Ergobibamus_cyprinoides_start.fasta \
  > DataReformatted/EP00769_Ergobibamus_cyprinoides.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00770_Monocercomonoides_exilis.fasta \
  > EP00770_Monocercomonoides_exilis_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00770_Monocercomonoides_exilis_start.fasta \
  > DataReformatted/EP00770_Monocercomonoides_exilis.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00771_Trimastix_marina.fasta \
  > EP00771_Trimastix_marina_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00771_Trimastix_marina_start.fasta \
  > DataReformatted/EP00771_Trimastix_marina.fasta
#now let's check the GiardiaDB files in Source_GiardiaDB/
wc -l *
#   58384 GiardiaDB-55_GintestinalisAssemblageADH_AnnotatedProteins.fasta
#   71215 GiardiaDB-55_GintestinalisAssemblageAWB_AnnotatedProteins.fasta
#   52239 GiardiaDB-55_GintestinalisAssemblageBGS_AnnotatedProteins.fasta
#   63959 GiardiaDB-55_GintestinalisAssemblageBGS_B_AnnotatedProteins.fasta
#   58509 GiardiaDB-55_GintestinalisAssemblageEP15_AnnotatedProteins.fasta
#  304306 total
grep -c ">" *
#GiardiaDB-55_GintestinalisAssemblageADH_AnnotatedProteins.fasta:5147
#GiardiaDB-55_GintestinalisAssemblageAWB_AnnotatedProteins.fasta:9667
#GiardiaDB-55_GintestinalisAssemblageBGS_AnnotatedProteins.fasta:4470
#GiardiaDB-55_GintestinalisAssemblageBGS_B_AnnotatedProteins.fasta:6098
#GiardiaDB-55_GintestinalisAssemblageEP15_AnnotatedProteins.fasta:5007
#these also need reformatting
#and while I'm at it, I'm going to work with the file names a bit
###
#this first one is the weird one that needs follow-up
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GiardiaDB-55_GintestinalisAssemblageAWB_AnnotatedProteins.fasta \
  > GiardiaDB_GintestinalisAWB_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_GiardiaDB/GiardiaDB_GintestinalisAssemblageAWB_start.fasta \
  > DataReformatted/GiardiaDB_GintestinalisAWB.fasta
###
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GiardiaDB-55_GintestinalisAssemblageBGS_AnnotatedProteins.fasta \
  > GiardiaDB_GintestinalisBGS_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_GiardiaDB/GiardiaDB_GintestinalisBGS_start.fasta \
  > DataReformatted/GiardiaDB_GintestinalisBGS.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GiardiaDB-55_GintestinalisAssemblageBGS_B_AnnotatedProteins.fasta \
  > GiardiaDB_GintestinalisBGS_B_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_GiardiaDB/GiardiaDB_GintestinalisBGS_B_start.fasta \
  > DataReformatted/GiardiaDB_GintestinalisBGS_B.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GiardiaDB-55_GintestinalisAssemblageADH_AnnotatedProteins.fasta \
  > GiardiaDB_GintestinalisADH_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_GiardiaDB/GiardiaDB_GintestinalisADH_start.fasta \
  > DataReformatted/GiardiaDB_GintestinalisADH.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < GiardiaDB-55_GintestinalisAssemblageEP15_AnnotatedProteins.fasta \
  > GiardiaDB_GintestinalisEP15_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_GiardiaDB/GiardiaDB_GintestinalisEP15_start.fasta \
  > DataReformatted/GiardiaDB_GintestinalisEP15.fasta
###
#Genomes found later
#Trepomonas
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00703_Trepomonas_sp_PC1.fasta \
  > EP00703_Trepomonas_sp_PC1_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Source_EukProt/EP00703_Trepomonas_sp_PC1_start.fasta \
  > DataReformatted/EP00703_Trepomonas_sp_PC1.fasta
#Barthelona strain
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < EP00792_Barthelona_sp_PAP020.fasta \
  > EP00792_Barthelona_sp_PAP020_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/Barthelona/EP00792_Barthelona_sp_PAP020_start.fasta \
  > DataReformatted/EP00792_Barthelona_sp_PAP020.fasta
###
#Anaeramoba lanta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < LANTA_protein.fas \
  > Anaeramoeba_lanta_160522_start.fasta
awk '{if (NR==1 && NF==0) next};1' RawData/LANTA/Anaeramoeba_lanta_160522_start.fasta \
  > DataReformatted/Anaeramoeba_lanta_160522.fasta
#and it turns out there's a protein that has "#" characters in it
#so I will replace these with "*" characters
cat Anaeramoeba_lanta_160522_start.fasta | tr "\#" "\*" > Anaeramoeba_lanta_160522_start_correct.fasta
grep -B 1 "\*" Anaeramoeba_lanta_160522_start_correct.fasta
# >tig00000139_segment0.cds1.1 tig00000139_segment0.path1.gene1 undefined product 2979920:2980113 forward MW:6488
# LTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLLS*PI*TFY*TIR*TGLGCWLDTTFTGLSRV
# --
# >tig00006013_segment0.cds1.1 tig00006013_segment0.path1.gene1 undefined product 24791768:24792552 forward MW:28639
# ALLDFCANRGIGLRFSRQVPTFASISSSPVHRFRSPPTNKAP*LRPHCGSTSSC*QRPRPSHLRPRRRSSSSTSLATRPSSLRPRRSTQGACRTARGRRSRYLTSSLRFRSVPRAGTRRSGCCWERSIRPSRLPSALTRTPSSRSYSSRPSRFSSLHQTRAARSSGTFSTRWRKIGKSSAPLSSHGILWSRELPAGSSGISTAHRKGRTQQSSSHLSPPSSPPFSLPELESPETLVFFFHKLLFLLARPGLIASIKAPELF
grep -B 1 "\#" Anaeramoeba_lanta_160522_start_correct.fasta
# [nothing - as it should be]
awk '{if (NR==1 && NF==0) next};1' RawData/LANTA/Anaeramoeba_lanta_160522_start_correct.fasta \
  > DataReformatted/Anaeramoeba_lanta_160522.fasta

```

### Protein Encoding

For ease of program usage and consistency, all FASTA headers will be renamed to a randomly-assigned alphanumeric code. The following Python script, named assignFASTAheaders_v2.py, was used to accomplish this task. This script is a development of the assignFASTAheaders.py written for the exploratory project. That earlier script assigned the alphanumeric headers independently for each FASTA file; as a result, there was no built-in safeguard to prevent an alphanumeric code from being repeated, and manual checks had to be performed subsequent to file creation to ensure that this had not occurred. This new version of the script uses and builds upon a large reference file to ensure that alphanumeric codes are not repeated.

```python
#!/bin/python
"""

Title: assignFASTAheaders_v2.py
Date: 28.11.2021
Author: Virág Varga

Description:
	This program replaces FASTA headers in a FASTA file with a 16-character random
		alphanumeric code. A reference file is also printed that links the
		random code to the original FASTA header. A larger reference file is used
		as input and then appended to in order to ensure that no alphanumeric header
		is repeated.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	re
	random
	string
  pandas

Procedure:
	1. Loading required modules & assigning command line argument.
    2. Contents of the large reference database are loaded into a Pandas dataframe,
		and existing alphanumeric headers are extracted to ensure no repitition.
	3. Parsing the input FASTA file in order to extract headers and generate
		random alphanumeric codes to replace them.
	4. Writing out the new FASTA file with the alphanumeric code headers,
		accompanied by the reference file.


Known bugs and limitations:
	- There is no quality-checking integrated into the code.

Version: 2.0
	The previous version of this script (assignFASTAheaders.py) did not include any
		mechanism for ensuring that the alphanumeric headers were not repeated.

Usage
	./assignFASTAheaders.py input_fasta ref_file
	OR
	python assignFASTAheaders.py input_fasta ref_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#import necessary modules
import sys #allows execution of script from command line
import re #enables regex pattern matching
import random #enables random number & variable generation
import string #imports a collection of string constants
import pandas as pd #allows manipulation of dataframes


#load input and output files
input_fasta = sys.argv[1]
ref_db_file = sys.argv[2]
#ref_db_file = "Extract__encoding_summary_ref.txt"
#input_fasta = "Extract__Carpediemonas_membranifera.PRJNA719540.fasta"
output_fasta = ".".join(input_fasta.split('.')[:-1]) + '_edit.fasta'
ref_doc = ".".join(input_fasta.split('.')[:-1]) + '_ref.txt'


#write the program
with open(input_fasta, "r") as infile, open(output_fasta, "w") as outfile, open(ref_doc, "w") as outref, open(ref_db_file, "r+") as ref_db:
	#open the input and output files
	#start by importing the large reference database into a Pandas dataframe
	ref_df = pd.read_csv(ref_db, sep="\t", header=None)
	encoding_list = ref_df[0].tolist()
	#open the input and output files
	for line in infile:
		#iterate through the input file line by line
		if line.startswith(">"):
			#identify the header lines
			header = line
			#remove the ">" character at the start of the line
			#this enables easier manipulation of the FASTA header
			header = re.sub(">", "", header)
			while True:
				#generate a random 16-character alphanumeric string to replace the original header
				assigned_header = ''.join(random.choices(string.ascii_letters + string.digits, k=16))
				if len(encoding_list) > 0:
					#check if the reference database-derived encoding list exists
					#if it does, check that the same alphanumeric code hasn't already been used somewhere
					if assigned_header not in encoding_list:
						break
			#now print the new header to the outfile
			outfile.write(">" + assigned_header + "\n")
			#and print the assigned reference to the outref file
			outref.write(assigned_header + "\t" + header)
			#add the header to the large reference dataframe
			ref_db.write(assigned_header + "\t" + header)
		else:
			#sequence lines are copied to the outfile without changes
			outfile.write(line)

```

The script was used as follows:

```bash
#link the already-encoded files into InProgressEncoding/
ln -s /home/inf-47-2020/Trich_Parab/Data_Files/DataEncoding/* .
rm encoding_summary_ref.txt
#and copy over the encoding_summary_ref.txt file (NOT symbolic link)
cp /home/inf-47-2020/Trich_Parab/Data_Files/DataEncoding/encoding_summary_ref.txt .
#working within the ThesisTrich/DataFiles/ directory
#model:
python assignFASTAheaders_v2.py input_fasta ref_file
#adapting it
#within the DataFiles/ directory
#NCBI files
python assignFASTAheaders_v2.py DataReformatted/Carpediemonas_membranifera.PRJNA719540.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/Giardia_intestinalis.PRJNA1439.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/Giardia_muris.PRJNA524057.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/Histomonas_meleagridis.PRJNA594289.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/Kipferlia_bialata.PRJDB5223.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/Spironucleus_salmonicida.PRJNA60811.fasta encoding_summary_ref.txt
#a quick sanity check, to ensure no duplicates
awk -F '\t' 'a[$1]++{print $1}' encoding_summary_ref.txt
#nothing was returned, so we're good to go
#EukProt
python assignFASTAheaders_v2.py DataReformatted/EP00701_Giardia_intestinalis.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00708_Paratrimastix_pyriformis.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00764_Aduncisulcus_paluster.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00765_Carpediemonas_membranifera.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00766_Chilomastix_caulleryi.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00767_Chilomastix_cuspidata.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00768_Dysnectes_brevis.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00769_Ergobibamus_cyprinoides.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00770_Monocercomonoides_exilis.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/EP00771_Trimastix_marina.fasta encoding_summary_ref.txt
#GiardiaDB
python assignFASTAheaders_v2.py DataReformatted/GiardiaDB_GintestinalisBGS.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/GiardiaDB_GintestinalisBGS_B.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/GiardiaDB_GintestinalisADH.fasta encoding_summary_ref.txt
python assignFASTAheaders_v2.py DataReformatted/GiardiaDB_GintestinalisEP15.fasta encoding_summary_ref.txt
#now move all .txt & _edit.fasta files to the InProgressEncoding/ folder
mv DataReformatted/*_edit.fasta InProgressEncoding/
mv DataReformatted/*.txt InProgressEncoding/
#then link _edit.fasta files to the EncodedData directory
#working from inside the EncodedData/ directory
ln -s ../InProgressEncoding/*.fasta .
#ok, time to start analyzing!
###
#later genomes
python assignFASTAheaders_v2.py DataReformatted/EP00703_Trepomonas_sp_PC1.fasta encoding_summary_ref.txt
mv DataReformatted/EP00703_Trepomonas_sp_PC1_* InProgressEncoding/
python assignFASTAheaders_v2.py DataReformatted/EP00792_Barthelona_sp_PAP020.fasta encoding_summary_ref.txt
mv DataReformatted/EP00792_Barthelona_sp_PAP020_* InProgressEncoding/
###
#A. lanta
python assignFASTAheaders_v2.py DataReformatted/Anaeramoeba_lanta_160522.fasta encoding_summary_ref.txt
mv DataReformatted/Anaeramoeba_lanta_160522_* InProgressEncoding/
#now linking to the necessary places: EncodedData & SortedEncodedData
ln -s /home/inf-47-2020/ThesisTrich/DataFiles/InProgressEncoding/Anaeramoeba_lanta_160522_edit.fasta .
#NOTE: 14842 proteins in this genome
#so the Metamonad database should contain 556048 proteins once A. lanta is integrated
#fixing the "#" character issue
cat Anaeramoeba_lanta_160522_edit.fasta | tr "\#" "\*" > Anaeramoeba_lanta_160522_fixed_edit.fasta
diff Anaeramoeba_lanta_160522_edit.fasta Anaeramoeba_lanta_160522_fixed_edit.fasta
# 3224c3224
# < LTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLLS#PI#TFY#TIR*TGLGCWLDTTFTGLSRV
# ---
# > LTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLLS*PI*TFY*TIR*TGLGCWLDTTFTGLSRV
rm Anaeramoeba_lanta_160522_edit.fasta
mv Anaeramoeba_lanta_160522_fixed_edit.fasta Anaeramoeba_lanta_160522_edit.fasta
grep "\#" Anaeramoeba_lanta_160522_edit.fasta
# nothing is produced here - this is correct
grep "\*" Anaeramoeba_lanta_160522_edit.fasta
# LTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLTLLS*PI*TFY*TIR*TGLGCWLDTTFTGLSRV
# ALLDFCANRGIGLRFSRQVPTFASISSSPVHRFRSPPTNKAP*LRPHCGSTSSC*QRPRPSHLRPRRRSSSSTSLATRPSSLRPRRSTQGACRTARGRRSRYLTSSLRFRSVPRAGTRRSGCCWERSIRPSRLPSALTRTPSSRSYSSRPSRFSSLHQTRAARSSGTFSTRWRKIGKSSAPLSSHGILWSRELPAGSSGISTAHRKGRTQQSSSHLSPPSSPPFSLPELESPETLVFFFHKLLFLLARPGLIASIKAPELF
#ok, we're good

```

```python
#!/bin/python
"""

Title: reverseHeaders.py
Date: 28.07.2021
Author: Virág Varga

Description:
	This program replaces the random alphanumeric headers assigned by the
		assignFASTAheaders.py program with the original FASTA headers, using the
		reference file created by the other program as a guide.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	re
	pandas

Procedure:
	1. Loading required modules & assigning command line arguments.
	2. Using Pandas to load the contents of the reference file into a dictionary.
	3. Parsing the input FASTA file in order to match the random headers to the
		original headers via the reference file.
	4. Writing out the new FASTA file with the original FASTA headers.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The user needs to check that the correct reference file is assigned.

Usage
	./reverseHeaders.py input_fasta ref_doc
	OR
	python reverseHeaders.py input_fasta ref_doc

This script was written for Python 3.8.10, in Spyder 5.0.5.

"""

#import necessary modules
import sys #allows execution of script from command line
import re #enables regex pattern matching
import pandas as pd #allow manipulation of data files


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "D_fragilis.20test_edit.fasta"
#the user choses the specific file, in case it isn't the original FASTA being used for the reversal
ref_doc = sys.argv[2]
#ref_doc = "D_fragilis.20test_ref.txt"
#output_fasta name is based on the input_fasta name
output_fasta = ".".join(input_fasta.split('.')[:-1]) + '_ogs.fasta'


#create and populate dictionary using the reference file
with open(ref_doc, "r") as inref:
	REF = pd.read_csv(inref, sep="\t", header=None)
	REF = REF.set_index(0)
	ref_dict = REF.T.to_dict('list')
	#note that this manner of conversion makes the dictionary value a list


#write the program
with open(input_fasta, "r") as infile, open(output_fasta, "w") as outfile:
	#open the input and output fasta files
	for line in infile:
		#iterate through the input file line by line
		if line.startswith(">"):
			#identify the header lines
			header = line.strip()
			#remove the ">" character at the start of the line
			#this enables easier manipulation of the FASTA header
			header = re.sub(">", "", header)
			#iterate through the reference dictionary keys
			for k in ref_dict.keys():
				#print(k)
				if header == k:
					og_header = ref_dict[k]
					#the og_header will be in list form, so need to convert to string
					og_header2 = ' '.join([str(item) for item in og_header])
					#now print the new header to the outfile
					outfile.write(">" + og_header2 + "\n")
		else:
			#sequence lines are copied to the outfile without changes
			outfile.write(line)

```

Modified decoder script for protein names (saved in decodeHeaders.py):
  - Based on/modification of reverseHeaders.py

```python
#!/bin/python
"""

Title: decodeHeaders.py
Date: 28.07.2021
Author: Virág Varga

Description:
	This program decodes the random alphanumeric headers assigned by the
		assignFASTAheaders.py program with the original FASTA headers, using the
		reference file created by the other program as a guide. It is intended
		for use on the *_OGall*.csv species & strain databases.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	re
	pandas

Procedure:
	1. Loading required modules & assigning command line arguments.
	2. Using Pandas to load the contents of the reference file into a dictionary.
	3. Parsing the input database file in order to match the random headers to the
		original headers via the reference file.
	4. Writing out the new database file with the original FASTA headers.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The user needs to check that the correct reference file is assigned.

Usage
	./decodeHeaders.py input_db ref_doc
	OR
	python decodeHeaders.py input_db ref_doc

This script was written for Python 3.8.10, in Spyder 5.0.5.

"""

#import necessary modules
import sys #allows execution of script from command line
import re #enables regex pattern matching
import pandas as pd #allow manipulation of data files


#load input and output files
input_db = sys.argv[1]
#input_db = "BM_anaeromoeba_DB_OFall.csv"
ref_doc = sys.argv[2]
#ref_doc = "encoding_summary_ref.txt"
#output_db name is based on the input_fasta name
output_db = ".".join(input_db.split('.')[:-1]) + '__decode.csv'


#create and populate dictionary using the reference file
with open(ref_doc, "r") as inref:
	REF = pd.read_csv(inref, sep="\t", header=None)
	REF = REF.set_index(0)
	ref_dict = REF.T.to_dict('list')
	#note that this manner of conversion makes the dictionary value a list

#reading in the input database into a Pandas dataframe
species_df = pd.read_csv(input_db, header=0)
#set the first column (containing the protein query ids) as an index
species_df.set_index('Query')


#replace the spaces (' ') in the FASTA headers with underscores ('_')
for key, value in ref_dict.items():
	#iterate through the ref_dict dictionary
	if ' ' in value[0]:
		#identify values which contain spaces (' ')
		value[0] = re.sub(' ', '_', value[0])
		#replace spaces in the values with underscores ('_')


#iterate through the species_df dataframe
#where the protein query id matches a key in the ref_dict dictionary, fill in the empty cell (value)
#in the final column with the corresponding dictionary value (ie. original FASTA header)
for id_num, prot_id in enumerate(species_df['Query']):
	#iterate over the contents of the first column of the species_df dataframe (header/column name  = 'Query')
	#prot_id contains the value in the cell; id_num contains the index of that value
	for ref_key in ref_dict:
		#iterate through the og_dict dictionary using its keys
		if prot_id == ref_key:
			#if the protein id in the species_df dataframe matches the key in the og_dict dictionary
			#extract the associated value from dictionary og_dict (ie. orthologous group assignment)
			#and place it in the appropriately indexed cell
			species_df.iat[id_num, 0] = ref_dict[ref_key][0]
			#indexing the ref_dict value (FASTA header) makes the contents print as a string instead of a list


#write out the results to a new .csv file
species_df.to_csv(output_db, index=False)

```

Script to remake the encoded headers (saved to remakeHeaders.py):

```python
#!/bin/python
"""

Title: remakeHeaders.py
Date: 2022-02-08
Author: Virág Varga

Description:
	This program replaces the original FASTA headers with the random alphanumeric
		headers assigned by the assignFASTAheaders.py program, using the
		reference file created by the other program as a guide.
	This program was created as a result of the accidental deletion of a number of
		the encoded FASTA files, in order to recreate them.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	re
	pandas

Procedure:
	1. Loading required modules & assigning command line arguments.
	2. Using Pandas to load the contents of the reference file into a dictionary.
	3. Parsing the input FASTA file in order to match the random headers to the
		original headers via the reference file.
	4. Writing out the new FASTA file with the original FASTA headers.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The user needs to check that the correct reference file is assigned.

Usage
	./remakeHeaders.py input_fasta ref_doc
	OR
	python remakeHeaders.py input_fasta ref_doc

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#import necessary modules
import sys #allows execution of script from command line
import re #enables regex pattern matching
import pandas as pd #allow manipulation of data files


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "Test_Files/EP00771_Trimastix_marina.fasta"
#the user choses the specific file, in case it isn't the original FASTA being used for the reversal
ref_doc = sys.argv[2]
#ref_doc = "Test_Files/EP00771_Trimastix_marina_ref.txt"
#output_fasta name is based on the input_fasta name
output_fasta = ".".join(input_fasta.split('.')[:-1]) + '_edit.fasta'


#create and populate dictionary using the reference file
with open(ref_doc, "r") as inref:
	REF = pd.read_csv(inref, sep="\t", header=None)
	REF = REF.set_index(0)
	ref_dict = REF.T.to_dict('list')
	#note that this manner of conversion makes the dictionary value a list


#write the program
with open(input_fasta, "r") as infile, open(output_fasta, "w") as outfile:
	#open the input and output fasta files
	for line in infile:
		#iterate through the input file line by line
		if line.startswith(">"):
			#identify the header lines
			header = line.strip()
			#remove the ">" character at the start of the line
			#this enables easier manipulation of the FASTA header
			header = re.sub(">", "", header)
			#iterate through the reference dictionary keys
			for k in ref_dict.keys():
				#save the original header to a variable
				og_header = ref_dict[k]
				#the original header will be in list form, so need to convert to string
				og_header = ' '.join([str(item) for item in og_header])
				if header == og_header:
					#match the original headers in the FASTA & reference files
					#now print the new header to the outfile
					outfile.write(">" + k + "\n")
		else:
			#sequence lines are copied to the outfile without changes
			outfile.write(line)

```


## Functional Annotation

### EggNOG

```bash
conda activate eggNOG-NEW-env
#directory for temp files
mkdir OG_comp_tmp
#Anaeramoebidae
ls /home/inf-47-2020/ThesisTrich/SortedEncodedData/Anaeramoebidae/*.fasta | while read file; do
  dir_file="${file##*/}"; #this line removes the path before the file name
  dir_name="${dir_file%.*}"; #this line removes the file extension
  mkdir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  cd /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  emapper.py -m diamond -i $file --report_orthologs --pfam_realign denovo -o ${dir_name}.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4;
done &
#Fornicata
ls /home/inf-47-2020/ThesisTrich/SortedEncodedData/Fornicata/*.fasta | while read file; do
  dir_file="${file##*/}"; #this line removes the path before the file name
  dir_name="${dir_file%.*}"; #this line removes the file extension
  mkdir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  cd /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  emapper.py -m diamond -i $file --report_orthologs --pfam_realign denovo -o ${dir_name}.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4;
done &
#Parabasalia
ls /home/inf-47-2020/ThesisTrich/SortedEncodedData/Parabasalia/*.fasta | while read file; do
  dir_file="${file##*/}"; #this line removes the path before the file name
  dir_name="${dir_file%.*}"; #this line removes the file extension
  mkdir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  cd /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  emapper.py -m diamond -i $file --report_orthologs --pfam_realign denovo -o ${dir_name}.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4;
done &
#here's the printout for one of the files as an example...
# #  emapper-2.1.6
# # emapper.py  -m diamond -i /home/inf-47-2020/ThesisTrich/SortedEncodedData/Parabasalia/Trichomonas_vaginalis_RefSeq.G3_edit.fasta --report_orthologs --pfam_realign denovo -o Trichomonas_vaginalis_RefSeq.G3_edit.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4
#   /home/inf-47-2020/miniconda3/envs/eggNOG-NEW-env/bin/diamond blastp -d /home2/resources/binp28/Programs/eggNOG/newest/data/eggnog_proteins.dmnd -q /home/inf-47-2020/Trich_Parab/Data_Files/DataEncoding/Trichomonas_vaginalis_RefSeq.G3_edit.fasta --threads 4 -o /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.hits  --sensitive --iterate -e 0.001 --top 3  --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp scovhsp
# Functional annotation of hits...
# 0 1.6689300537109375e-06 0.00 q/s (% mem usage: 1.20, % mem avail: 98.81)
# De novo scan of PFAM domains
# CREATED ESL_REFORMAT DB. USING HMMSEARCH
# Namespace(call_info='## Wed Feb  9 19:43:45 2022\n## emapper-2.1.6\n## /resources/binp28/Programs/eggNOG/newest/emapper.py -m diamond -i /home/inf-47-2020/ThesisTrich/SortedEncodedData/Parabasalia/Trichomonas_vaginalis_RefSeq.G3_edit.fasta --report_orthologs --pfam_realign denovo -o Trichomonas_vaginalis_RefSeq.G3_edit.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4\n##', cpu=4, usemem=True, port=51700, end_port=53200, num_servers=1, num_workers=1, cpus_per_worker=4, scan_type='mem', db='/home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp/tmpuhbax91f', servers_list=None, dbtype='seqdb', qtype='hmm', translate=False, trans_table=None, resume=False, no_file_comments=False, maxhits=0, report_no_hits=False, maxseqlen=None, cut_ga=True, clean_overlaps='hmmsearch_clans', evalue=1e-10, score=None, qcov=0, Z=40000000, temp_dir='/home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp', excluded_taxa=None)
# 500 1936.8278985023499 0.26 q/s (% mem usage: 1.30, % mem avail: 98.67)
# 1000 1937.1202931404114 0.52 q/s (% mem usage: 1.30, % mem avail: 98.67)
# 1500 1937.4332449436188 0.77 q/s (% mem usage: 1.30, % mem avail: 98.67)
# 2000 1937.7816615104675 1.03 q/s (% mem usage: 1.30, % mem avail: 98.69)
# 2500 1938.1603555679321 1.29 q/s (% mem usage: 1.30, % mem avail: 98.69)
# 3000 1938.4869232177734 1.55 q/s (% mem usage: 1.30, % mem avail: 98.69)
# 3500 1938.8515794277191 1.81 q/s (% mem usage: 1.30, % mem avail: 98.69)
# 4000 1939.1976218223572 2.06 q/s (% mem usage: 1.30, % mem avail: 98.69)
# 4500 1939.6094653606415 2.32 q/s (% mem usage: 1.30, % mem avail: 98.73)
# 5000 1940.007336139679 2.58 q/s (% mem usage: 1.30, % mem avail: 98.73)
# 5500 1940.4094214439392 2.83 q/s (% mem usage: 1.30, % mem avail: 98.73)
# 6000 1940.9920907020569 3.09 q/s (% mem usage: 1.30, % mem avail: 98.72)
# 6500 1941.5877594947815 3.35 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 7000 1942.0937850475311 3.60 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 7500 1942.9340589046478 3.86 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 8000 1943.4039647579193 4.12 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 8500 1944.4430389404297 4.37 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 9000 1945.4193875789642 4.63 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 9500 1946.374478816986 4.88 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 10000 1947.415036201477 5.14 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 10500 1949.2378854751587 5.39 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 11000 1951.0494921207428 5.64 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 11500 1954.1160044670105 5.89 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 12000 1960.1019520759583 6.12 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 12500 1962.2805461883545 6.37 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 13000 1964.5515179634094 6.62 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 13500 1967.7910039424896 6.86 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 14000 1970.8294546604156 7.10 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 14500 1973.189219713211 7.35 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 15000 1976.1312606334686 7.59 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 15500 1979.437059879303 7.83 q/s (% mem usage: 1.30, % mem avail: 98.74)
# 16000 1982.175112247467 8.07 q/s (% mem usage: 1.50, % mem avail: 98.51)
# 16500 1985.7839670181274 8.31 q/s (% mem usage: 1.30, % mem avail: 98.65)
# 17000 1990.1253292560577 8.54 q/s (% mem usage: 1.50, % mem avail: 98.54)
# 17500 1993.0858538150787 8.78 q/s (% mem usage: 1.40, % mem avail: 98.57)
# 18000 1997.106840133667 9.01 q/s (% mem usage: 1.40, % mem avail: 98.64)
# 18500 2001.1775419712067 9.24 q/s (% mem usage: 1.40, % mem avail: 98.60)
# 19000 2005.1405465602875 9.48 q/s (% mem usage: 1.50, % mem avail: 98.52)
# 19500 2009.56533741951 9.70 q/s (% mem usage: 1.50, % mem avail: 98.45)
# 20000 2014.4270269870758 9.93 q/s (% mem usage: 1.50, % mem avail: 98.47)
# 20500 2020.5024454593658 10.15 q/s (% mem usage: 1.50, % mem avail: 98.53)
# 21000 2024.232114315033 10.37 q/s (% mem usage: 1.50, % mem avail: 98.53)
# 21500 2028.0461330413818 10.60 q/s (% mem usage: 1.40, % mem avail: 98.60)
# 22000 2033.1968166828156 10.82 q/s (% mem usage: 1.50, % mem avail: 98.53)
# 22500 2036.4913942813873 11.05 q/s (% mem usage: 1.50, % mem avail: 98.49)
# 23000 2040.3639905452728 11.27 q/s (% mem usage: 1.60, % mem avail: 98.44)
# 23500 2044.679265499115 11.49 q/s (% mem usage: 1.50, % mem avail: 98.47)
# 24000 2048.442482471466 11.72 q/s (% mem usage: 1.50, % mem avail: 98.48)
# 24500 2053.2151458263397 11.93 q/s (% mem usage: 1.50, % mem avail: 98.46)
# 25000 2057.402370929718 12.15 q/s (% mem usage: 1.30, % mem avail: 98.67)
# 25500 2061.9055440425873 12.37 q/s (% mem usage: 1.50, % mem avail: 98.48)
# 26000 2065.9218170642853 12.59 q/s (% mem usage: 1.50, % mem avail: 98.53)
# 26500 2069.3568902015686 12.81 q/s (% mem usage: 1.50, % mem avail: 98.47)
# 27000 2073.465125799179 13.02 q/s (% mem usage: 1.60, % mem avail: 98.45)
# 27500 2077.6617999076843 13.24 q/s (% mem usage: 1.50, % mem avail: 98.52)
# 28000 2081.611243724823 13.45 q/s (% mem usage: 1.50, % mem avail: 98.48)
# 28500 2086.31462430954 13.66 q/s (% mem usage: 1.60, % mem avail: 98.41)
# 29000 2091.5698766708374 13.87 q/s (% mem usage: 1.50, % mem avail: 98.46)
# 29500 2096.6048517227173 14.07 q/s (% mem usage: 1.50, % mem avail: 98.48)
# 30000 2101.448240995407 14.28 q/s (% mem usage: 1.40, % mem avail: 98.62)
# 30500 2106.441559791565 14.48 q/s (% mem usage: 1.50, % mem avail: 98.50)
# 30804 2114.8667962551117 14.57 q/s (% mem usage: 1.40, % mem avail: 98.58)
# Done
#    /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.hits
#    /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.seed_orthologs
#    /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.annotations
#    /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.pfam
#    /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.orthologs
# ================================================================================
# CITATION:
# If you use this software, please cite:
# [1] eggNOG-mapper v2: functional annotation, orthology assignments, and domain
#       prediction at the metagenomic scale. Carlos P. Cantalapiedra,
#       Ana Hernandez-Plaza, Ivica Letunic, Peer Bork, Jaime Huerta-Cepas. 2021.
#       Molecular Biology and Evolution, msab293, https://doi.org/10.1093/molbev/msab293
# [2] eggNOG 5.0: a hierarchical, functionally and phylogenetically annotated
#       orthology resource based on 5090 organisms and 2502 viruses. Jaime
#       Huerta-Cepas, Damian Szklarczyk, Davide Heller, Ana Hernandez-Plaza,
#       Sofia K Forslund, Helen Cook, Daniel R Mende, Ivica Letunic, Thomas
#       Rattei, Lars J Jensen, Christian von Mering and Peer Bork. Nucleic Acids
#       Research, Volume 47, Issue D1, 8 January 2019, Pages D309-D314,
#       https://doi.org/10.1093/nar/gky1085
# [3] Sensitive protein alignments at tree-of-life scale using DIAMOND.
#        Buchfink B, Reuter K, Drost HG. 2021.
#        Nature Methods 18, 366–368 (2021). https://doi.org/10.1038/s41592-021-01101-x
# e.g. Functional annotation was performed using emapper-2.1.6 [1]
#  based on eggNOG orthology data [2]. Sequence searches were performed using [3].
# ================================================================================
# Total hits processed: 30804
# Total time: 5474 secs
# FINISHED
#Preaxostyla
ls /home/inf-47-2020/ThesisTrich/SortedEncodedData/Preaxostyla/*.fasta | while read file; do
  dir_file="${file##*/}"; #this line removes the path before the file name
  dir_name="${dir_file%.*}"; #this line removes the file extension
  mkdir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  cd /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/${dir_name}/;
  emapper.py -m diamond -i $file --report_orthologs --pfam_realign denovo -o ${dir_name}.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4;
done &
#A. lanta run
conda activate eggNOG-NEW-env
mkdir Anaeramoeba_lanta_160522_edit
emapper.py -m diamond -i /home/inf-47-2020/ThesisTrich/SortedEncodedData/Anaeramoebidae/Anaeramoeba_lanta_160522_edit.fasta --report_orthologs --pfam_realign denovo -o Anaeramoeba_lanta_160522_edit.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4 &>Alanta_log.txt &
# [1] 3577086
#done!

```

### InterProScan

Since this program was run on the SNIC Kebnekaise HPC, each species FASTA file was run with `sbatch`.

The model batch script looks like this: 

```bash
#!/bin/bash
# The name of the account you are running in, mandatory.
#SBATCH -A SNIC2021-22-658
# Name the job
#SBATCH -J iprScan_20220228

# NUMBER OF NODES
#SBATCH -N 1
# NUMER OF THREADS (PTHREADED)
#SBATCH -c 4
# Request  minutes of runtime for the job
#SBATCH --time=96:00:00
# Set the names for the error and output files
#SBATCH --error=job.%J.err
#SBATCH --output=job.%J.out

#SBATCH --mail-user=vi2505va-s@student.lu.se
# if you'd like to receive mails from the cluster :p
#SBATCH --mail-type=BEGIN
# you recieve a mail when the job has begun
#SBATCH --mail-type=END
# you recieve a mail when the job has completed


## TEMPLATE START

# Change directory to where you currently are
cd WORKING_DIRECTORY_FINDREPLACE


# Create directory for new files
mkdir INPUTFILE_DIRECTORY_FINDREPLACE
# Move into the newly created directory
#cd INPUTFILE_DIRECTORY_FINDREPLACE


# Java is needed for interprocan, 11.0.2 works for iprscan 5.52-86.0

module load Java/11.0.2

# Assign variables to file paths and options

interproscan=/proj/nobackup/trichocompare_2021/software/my_interproscan/interproscan-5.54-87.0/interproscan.sh
inputfile=INPUTFILE_FINDREPLACE
outputdirectory=WORKING_DIRECTORY_FINDREPLACE
tempdir=OUTPUTFILE_TEMPDIR_FINDREPLACE
options="--goterms --iprlookup --pathways --cpu 4"


$interproscan --input $inputfile --output-dir $outputdirectory --tempdir $tempdir $options

```

The unique submission shell scripts for each file were created using the following script: 

```bash
# sed script to create batch files for slurm
#specifically edited for the IPR_Scan runs

for file in /home/v/vivarga/ThesisTrich/Data/SplitStdData/*.fasta; do
  full_file="${file##*/}"; #this line removes the path before the file name
  file_base="${full_file%.*}"; #this line removes the file extension
  slurmsubfile=interproscan_${file_base}.sh;
  sed 's/WORKING_DIRECTORY_FINDREPLACE/\/proj\/nobackup\/trichocompare_2021\/Vi_storage\/IPRScan_Results_PROSITE\//g' iprScan_batch_12.sh > $slurmsubfile;
  sed -i "s|INPUTFILE_DIRECTORY_FINDREPLACE|${file_base}|g" $slurmsubfile;
  sed -i "s|INPUTFILE_FINDREPLACE|${file}|g" $slurmsubfile;
  sed -i 's/OUTPUTFILE_TEMPDIR_FINDREPLACE/\/proj\/nobackup\/trichocompare_2021\/Vi_storage\/IPR_temp\//g' $slurmsubfile;
done

```

Larger files were split into multiple parts, since they would not have completed in an acceptable amount of time. Batch scripts were submitted as follows, for example: 

```bash
sbatch interproscan_Trichomonas_foetus.PRJNA345179_edit_StandardAA.Part05.sh
```

### DeepLoc

Running the program:

```bash
#working in subdirectories within the /home/inf-47-2020/ThesisTrich/DeepLoc_Results/ directory
#activate the necessary conda environment
conda activate DeepLoc_usage
# running the code the way it should work:
#looping would be nicer, but then I can't rename the files
deeploc --fasta ../../EncodedData/BM_newprots_may21.anaeromoeba_edit.fasta --output BM_anaeromoeba_DL --attention
#when it finishes, get this printout to stdout:
#WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
#Input file processed. Starting prediction...
#Prediction finished. Generating output...
#Done!
tar -zcvf BM_anaeromoeba_DL.tar.gz BM_anaeromoeba_DL
rm -r BM_anaeromoeba_DL/
deeploc --fasta ../../EncodedData/BS_newprots_may21.anaeromoeba_edit.fasta --output BS_anaeromoeba_DL --attention
#Looping would be much, much simpler than doing this individually
#but given that I know //from experience// that DeepLoc can run for nearly //three (3) full days// at worst
#we're going to run all of these individually, with at most 3 running at a time
deeploc --fasta ../DataFiles/EncodedData/Carpediemonas_membranifera.PRJNA719540_edit.fasta --output Carpediemonas_membranifera_DL --attention
deeploc --fasta ../DataFiles/EncodedData/EP00701_Giardia_intestinalis_edit.fasta --output EP00701_Giardia_intestinalis_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00703_Trepomonas_sp_PC1_edit.fasta --output EP00703_Trepomonas_sp_PC1_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00708_Paratrimastix_pyriformis_edit.fasta --output EP00708_Paratrimastix_pyriformis_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00764_Aduncisulcus_paluster_edit.fasta --output EP00764_Aduncisulcus_paluster_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00766_Chilomastix_caulleryi_edit.fasta --output EP00766_Chilomastix_caulleryi_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00767_Chilomastix_cuspidata_edit.fasta --output EP00767_Chilomastix_cuspidata_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00768_Dysnectes_brevis_edit.fasta --output EP00768_Dysnectes_brevis_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00769_Ergobibamus_cyprinoides_edit.fasta --output EP00769_Ergobibamus_cyprinoides_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00770_Monocercomonoides_exilis_edit.fasta --output EP00770_Monocercomonoides_exilis_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/EP00771_Trimastix_marina_edit.fasta --output EP00771_Trimastix_marina_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/GiardiaDB_GintestinalisADH_edit.fasta --output GiardiaDB_GintestinalisADH_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/GiardiaDB_GintestinalisBGS_B_edit.fasta --output GiardiaDB_GintestinalisBGS_B_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/GiardiaDB_GintestinalisBGS_edit.fasta --output GiardiaDB_GintestinalisBGS_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/GiardiaDB_GintestinalisEP15_edit.fasta --output GiardiaDB_GintestinalisEP15_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/Giardia_intestinalis.PRJNA1439_edit.fasta --output Giardia_intestinalis.PRJNA1439_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/Giardia_muris.PRJNA524057_edit.fasta --output Giardia_muris.PRJNA524057_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/Histomonas_meleagridis.PRJNA594289_edit.fasta --output Histomonas_meleagridis.PRJNA594289_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/Kipferlia_bialata.PRJDB5223_edit.fasta --output Kipferlia_bialata.PRJDB5223_DL --attention
deeploc --fasta ../../DataFiles/EncodedData/Spironucleus_salmonicida.PRJNA60811_edit.fasta --output Spironucleus_salmonicida.PRJNA60811_DL --attention
#now, extract the summary file and compress the folders
cp */*_DL.txt .
find . -maxdepth 1 -mindepth 1 -type d -exec tar zcvf {}.tar.gz {} --remove-files \;
#finally, link the files from the original project
ln -s ~/Trich_Parab/DeepLocResults/*_DL.* .
#Done!
#later genome: Barthelona
deeploc --fasta ../../DataFiles/EncodedData/EP00792_Barthelona_sp_PAP020_edit.fasta --output EP00792_Barthelona_sp_PAP020_DL --attention
cp EP00792_Barthelona_sp_PAP020_DL.txt . #extract the summary file out to the DeepLoc folder
tar -czvf EP00792_Barthelona_sp_PAP020_DL.tar.gz EP00792_Barthelona_sp_PAP020_DL/ --remove-files
#A. lanta integration
nohup deeploc --fasta ../../DataFiles/EncodedData/Anaeramoeba_lanta_160522_edit.fasta --output Anaeramoeba_lanta_160522_DL --attention &> nohup_Alanta.out &
# [3] 2354080
cp Anaeramoeba_lanta_160522_DL/Anaeramoeba_lanta_160522_DL.txt .
tar -czvf Anaeramoeba_lanta_160522_DL.tar.gz Anaeramoeba_lanta_160522_DL/ --remove-files
#done!

```

### MitoFates

An issue was encountered wherein a number of these files raised a large number of errors due to non-standard amino acids in the protein sequences. In order to deal with this issue, the following Python script was written to remove those non-standard amino acids:

```python
#!/bin/python
"""

Title: remove_nonStandardAA.py
Date: 17.12.2021
Author: Virág Varga

Description:
	This program removes non-standard amino acids from protein FASTA files.
		X and * characters are removed entirely from FASTA sequences, while
		selenocysteine (recorded as U) is replaced with simple cysteine (C).

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys

Procedure:
	1. Loading required modules & assigning command line argument.
	2. Parsing the input FASTA file in order to replace non-standard amino acids
		in the sequences. X and * characters are removed entirely from FASTA
		sequences, while selenocysteine (recorded as U) is replaced with simple
		cysteine (C).
	3. Writing out the new FASTA file with only standard amino acids.


Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is pre-determined (not user-defined), based on
		the name of the input FASTA file.

Usage
	./remove_nonStandardAA.py input_fasta
	OR
	python remove_nonStandardAA.py input_fasta

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#import necessary modules
import sys #allows execution of script from command line


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "non_standard_test.seq.fasta"
output_fasta = ".".join(input_fasta.split('.')[:-1]) + '_StandardAA.fasta'


#write the program
with open(input_fasta, "r") as infile, open(output_fasta, "w") as outfile:
	#open the input and output files
	for line in infile:
		#iterate through the input file line by line
		if not line.startswith(">"):
			#identify the sequence lines
			sequence = line.strip()
			#remove the "\n" endline character from the end of the sequence lines
			sequence = sequence.upper()
			#make sure all characters are uppercase
			#this should already be the case, but just in case an X is lowercase
			standard_seq = sequence.replace("U", "C")
			#replace the selenocysteines (U) with cysteines (C)
			standard_seq = standard_seq.replace("*", "")
			#remove non-standard * characters
			standard_seq = standard_seq.replace("X", "")
			#remove non-standard X characters
			#now print the standardized amino acid sequence to the outfile
			outfile.write(standard_seq + "\n")
		else:
			#sequence lines are copied to the outfile without changes
			outfile.write(line)
```

Further errors lead to the decision to remove sequences that don't start with Methionine (M).

```python
#!/bin/python
"""

Title: remove_nonM.py
Date: 17.12.2021
Author: Virág Varga

Description:
	This program removes sequences from a protein FASTA file if the sequence does
		not start with Methionine (M).

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys

Procedure:
	1. Loading required modules & assigning command line argument.
	2. Parsing the input FASTA file in order to remove sequences that do not start
		with Methionine (M).
	3. Writing out the new FASTA file with only standard amino acids.


Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is pre-determined (not user-defined), based on
		the name of the input FASTA file.

Usage
	./remove_nonM.py input_fasta
	OR
	python remove_nonM.py input_fasta

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#import necessary modules
import sys #allows execution of script from command line


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "non_standard_test.seq_StandardAA.Bad_M.fasta"
output_fasta = ".".join(input_fasta.split('.')[:-1]) + '_nonM.fasta'


#write the program
with open(input_fasta, "r") as infile, open(output_fasta, "w") as outfile:
	#open the input and output files
	for line in infile:
		#iterate through the input file line by line
		if line.startswith(">"):
			#identify the header lines
			header = line.strip()
			#remove the "\n" endline character from the end of the header lines
		if line.startswith("M"):
			#identify sequence lines that start with "M"
			sequence = line.strip()
			#remove the "\n" endline character from the end of the sequence lines
			#now print the standardized amino acid sequence to the outfile
			outfile.write(header + "\n" + sequence + "\n")
```

The following script (replace_AA.py) was written to replace amino acids. It was used to remove "B" amino acids from the GiardiaDB_GintestinalisBGS files.

```python
#!/bin/python
"""

Title: replace_AA.py
Date: 21.12.2021
Author: Virág Varga

Description:
	This program replaces a selected non-standard amino acid in a protein FASTA
		with a chosen replacement.
	It is intended for use in instances of uncertain amino acids (ex.: "B", "Z",
		or "J"). In these instances, the user can replace the uncertain amino acid
		with their choice of replacement. If no replacement is given, the uncertain
		amino acid is simply removed.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys

Procedure:
	1. Loading required modules & assigning command line argument.
	2. Parsing the input FASTA file in order to replace selected non-standard
		amino acid in the sequences with chosen replacement.
	3. Writing out the new FASTA file with standardized amino acids.


Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is pre-determined (not user-defined), based on
		the name of the input FASTA file.

Usage
	./replace_AA.py input_fasta nonStandard_aa replacement_aa
	OR
	python replace_AA.py input_fasta nonStandard_aa replacement_aa

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#import necessary modules
import sys #allows execution of script from command line


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "non_standard_test.seq.fasta"
nonStandard_aa = sys.argv[2]
if len(sys.argv) == 4:
	replacement_aa = sys.argv[3]
else:
	replacement_aa = ""
#nonStandard_aa = "B"
#replacement_aa = "D"
output_fasta = ".".join(input_fasta.split('.')[:-1]) + '_replaceAA_' + replacement_aa + '.fasta'


#write the program
with open(input_fasta, "r") as infile, open(output_fasta, "w") as outfile:
	#open the input and output files
	for line in infile:
		#iterate through the input file line by line
		if not line.startswith(">"):
			#identify the sequence lines
			sequence = line.strip()
			#remove the "\n" endline character from the end of the sequence lines
			sequence = sequence.replace(nonStandard_aa, replacement_aa)
			#replace non-standard characters
			#now print the standardized amino acid sequence to the outfile
			outfile.write(sequence + "\n")
		else:
			#sequence lines are copied to the outfile without changes
			outfile.write(line)

```

Running all of this (hopefully) a final time:

```bash
#activate conda environment
conda activate MitoFates-env
#model to use:
perl MitoFates.pl example.fasta metazoa
#running the python script above on the files
#working in the ThesisTrich/DataFiles/StandardAA/ directory
ls ../EncodedData/*.fasta | while read file; do
  python ../remove_nonStandardAA.py $file;
done
#working in the DataFiles/ directory
mv EncodedData/*_StandardAA.fasta StandardAA/
#making the non-Methionine files
ls StandardAA/*_StandardAA.fasta | while read file; do
  python remove_nonM.py $file;
done
#going to file away the preliminary data files into their own directory
mkdir Standardized_runs
mv *_MFresults.txt Standardized_runs/
#running MitoFates again
ls ../DataFiles/StandardAA/*_nonM.fasta | while read file; do
  full_file="${file##*/}"; #this line removes the path before the file name
  file_base="${full_file%.*}"; #this line removes the file extension
  perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl $file metazoa > ${file_base}_MFresults.txt;
done &
#later Barthelona file
python ../remove_nonStandardAA.py EP00792_Barthelona_sp_PAP020_edit.fasta
#move over to StandardAA directory
python ../remove_nonM.py EP00792_Barthelona_sp_PAP020_edit_StandardAA.fasta
#A. lanta integration
python ../remove_nonStandardAA.py Anaeramoeba_lanta_160522_edit.fasta
#move over to StandardAA directory
python ../remove_nonM.py Anaeramoeba_lanta_160522_edit_StandardAA.fasta
###
#ran into some issues with the previous loop
#here is the workflow to deal with files with unique errors
#the GiardiaDB_GintestinalisBGS proteome file includes a 'B' amino acid
python replace_AA.py StandardAA/GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM.fasta B
#this removed B amino acids from the file
#replacement isn't particularly viable (though I can go back and do so if needed)
#because B denotes uncertaintly between asparagine and aspartic acid
#ref: https://www.ddbj.nig.ac.jp/ddbj/code-e.html
perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM_replaceAA_.fasta metazoa > GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM_replaceAA__MFresults.txt
#proteomes with sequences of length <30 amino acids
#displays the error code: "uninitialized value in multiplication (*)"
#working in the /home/inf-47-2020/ThesisTrich/DataFiles/StandardAA/ directory
seqkit seq -m 30 -g Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM.fasta -o Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa.fasta \
  > Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_start.fasta
awk '{if (NR==1 && NF==0) next};1' Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_start.fasta \
  > Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_fin.fasta
seqkit seq -m 30 -g Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM.fasta -o Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa.fasta \
  > Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_start.fasta
awk '{if (NR==1 && NF==0) next};1' Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_start.fasta \
  > Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_fin.fasta
seqkit seq -m 30 -g Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM.fasta -o Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa.fasta \
  > Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_start.fasta
awk '{if (NR==1 && NF==0) next};1' Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_start.fasta \
  > Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_fin.fasta
seqkit seq -m 30 -g Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM.fasta -o Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa.fasta \
  > Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_start.fasta
awk '{if (NR==1 && NF==0) next};1' Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_start.fasta \
  > Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_fin.fasta
#now running the program
perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_fin.fasta metazoa > Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_fin_MFresults.txt
perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_fin.fasta metazoa > Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_fin_MFresults.txt
perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_fin.fasta metazoa > Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_fin_MFresults.txt
perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_fin.fasta metazoa > Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_fin_MFresults.txt
###
#A. lanta integration
nohup perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/Anaeramoeba_lanta_160522_edit_StandardAA_nonM.fasta metazoa > Anaeramoeba_lanta_160522_edit_StandardAA_nonM_MFresults.txt &> nohup_Alanta.out &
# [1] 2362556
#ran into the uninitialized value error again, so here's hoping this fixes it again: 
seqkit seq -m 30 -g Anaeramoeba_lanta_160522_edit_StandardAA_nonM.fasta -o Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa.fasta
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' \
  < Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa.fasta \
  > Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_start.fasta
awk '{if (NR==1 && NF==0) next};1' Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_start.fasta \
  > Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_fin.fasta
nohup perl /home/inf-47-2020/bin/MitoFates/MitoFates.pl ../DataFiles/StandardAA/Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_fin.fasta metazoa > Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_fin_MFresults.txt &> nohup_Alanta30.out &
# [1] 2411352
#this saved the output to the nohup_Alanta30.out file, so let's fix that real fast
tail -n+2 nohup_Alanta30.out > Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_fin_MFresults.txt
#done! 

```

### SignalP

```bash
#obtain the help menu with the following
signalp -h
#running the program
ls /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta | while read file; do
  dir_file="${file##*/}"; #this line removes the path before the file name
  dir_name="${dir_file%.*}"; #this line removes the file extension
  mkdir /home/inf-47-2020/ThesisTrich/SignalP_Results/${dir_name}/;
  cd /home/inf-47-2020/ThesisTrich/SignalP_Results/${dir_name}/;
  signalp -format 'long' -org 'euk' -plot 'none' -fasta $file;
done
#copying out the summary files
cp */*.signalp5 .
#compressing the rest of the data
find . -maxdepth 1 -mindepth 1 -type d -exec tar zcvf {}.tar.gz {} --remove-files \;
#linking other files in new PrelimProjData/ directory
ln -s /home/inf-47-2020/Trich_Parab/SignalPResults/* .
###
#genomes added later
signalp -format 'long' -org 'euk' -plot 'none' -fasta ../../DataFiles/EncodedData/EP00703_Trepomonas_sp_PC1_edit.fasta
signalp -format 'long' -org 'euk' -plot 'none' -fasta ../../DataFiles/EncodedData/EP00792_Barthelona_sp_PAP020_edit.fasta
#A. lanta integration
nohup signalp -format 'long' -org 'euk' -plot 'none' -fasta ../../DataFiles/EncodedData/Anaeramoeba_lanta_160522_edit.fasta &> nohup_Alanta.out &
# [1] 2352706
tar -zcvf Anaeramoeba_lanta_160522_edit.tar.gz Anaeramoeba_lanta_160522_edit/ --remove-files

```

### TargetP

```bash
#obtain the help menu with the following
targetp -h
#running the program
ls /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta | while read file; do
  dir_file="${file##*/}"; #this line removes the path before the file name
  dir_name="${dir_file%.*}"; #this line removes the file extension
  mkdir /home/inf-47-2020/ThesisTrich/TargetP_Results/${dir_name}/;
  cd /home/inf-47-2020/ThesisTrich/TargetP_Results/${dir_name}/;
  targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta $file;
  #the `org 'non-pl'` is different from signalp
  #where signalp requires specification of eukaryotic vs non organisms
  #targetp asks to specify plant or non-plant
  #use `-batch 10000` to make the batch size same as SignalP
done
#it turns out the T. gallinarum file is too large for TargetP to analyze
#I'm going to divide the FASTA file into 2 parts,
#and run them separately
head -n 67510 Tetratrichomonas_gallinarum.5730.aa_edit.fasta > Tetratrichomonas_gallinarum.5730.aa_edit.1st_half.fasta
tail -n +67511 Tetratrichomonas_gallinarum.5730.aa_edit.fasta > Tetratrichomonas_gallinarum.5730.aa_edit.2nd_half.fasta
#now have separate folders for 1st half & 2nd half of file
#in T. gallinarum TargetP directory
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../Tetratrichomonas_gallinarum.5730.aa_edit.1st_half.fasta
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../Tetratrichomonas_gallinarum.5730.aa_edit.2nd_half.fasta
#we can `cat` the summary files together later
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../../DataFiles/EncodedData/Trichomonas_foetus.PRJNA345179_edit.fasta
#the file length issue occured with the T. vaginalis genomes, too,
#so I did the same thing as before, splitting them into two files
#GenBank:
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../../DataFiles/EncodedData/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta
#instead of the above:
head -n 59680 Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta > Trichomonas_vaginalis_GenBank.PRJNA16084_edit.1st_half.fasta
tail -n +59681 Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta > Trichomonas_vaginalis_GenBank.PRJNA16084_edit.2nd_half.fasta
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../Trichomonas_vaginalis_GenBank.PRJNA16084_edit.1st_half.fasta &
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../Trichomonas_vaginalis_GenBank.PRJNA16084_edit.2nd_half.fasta &
#RefSeq:
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../../DataFiles/EncodedData/Trichomonas_vaginalis_RefSeq.G3_edit.fasta
#these files are really large, though, so removal isn't as simple as
#`rm *` and then `rm -r directory/`
#instead, have to do:
tar -zcvf Trichomonas_vaginalis_RefSeq.G3_edit.tar.gz Trichomonas_vaginalis_RefSeq.G3_edit/ --remove-files
rm Trichomonas_vaginalis_RefSeq.G3_edit.tar.gz
#instead of the above:
head -n 59680 Trichomonas_vaginalis_RefSeq.G3_edit.fasta > Trichomonas_vaginalis_RefSeq.G3_edit.1st_half.fasta
tail -n +59681 Trichomonas_vaginalis_RefSeq.G3_edit.fasta > Trichomonas_vaginalis_RefSeq.G3_edit.2nd_half.fasta
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../Trichomonas_vaginalis_RefSeq.G3_edit.1st_half.fasta &
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../Trichomonas_vaginalis_RefSeq.G3_edit.2nd_half.fasta &
###
#new genomes
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../../DataFiles/EncodedData/EP00703_Trepomonas_sp_PC1_edit.fasta
###
#finishing up - compression & file deletion
find . -maxdepth 1 -mindepth 1 -type d -exec tar zcvf {}.tar.gz {} --remove-files \;
#Barthelona file
targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../../DataFiles/EncodedData/EP00792_Barthelona_sp_PAP020_edit.fasta
#A. lanta integration
nohup targetp -format 'long' -org 'non-pl' -plot 'none' -batch 10000 -fasta ../../DataFiles/EncodedData/Anaeramoeba_lanta_160522_edit.fasta &> nohup_Alanta.out &
# [2] 2353362

```

### YLoc

```bash
docker run --mount type=bind,source=/home/inf-47-2020/ThesisTrich/DataFiles/InProgressEncoding,target=/YLoc/Vi_hostDataNEW,readonly --mount type=bind,source=/home/inf-47-2020/Trich_Parab/Data_Files/DataEncoding,target=/YLoc/Vi_hostDataOG,readonly -it f8ff9f3b1898 /bin/bash
mkdir Vi_results
mv *_YL.txt Vi_results
docker ps #to extract the container IDs
#fortunately, the PuTTY terminal has the container ID at the top of the window; also visible on command line after root@
docker cp <containerID>:/YLoc/Vi_results /home/inf-47-2020/ThesisTrich/YLoc_Results
docker ps
# CONTAINER ID   IMAGE          COMMAND                  CREATED        STATUS        PORTS     NAMES
# 4e8be9bd0d6a   f8ff9f3b1898   "/bin/bash"              35 hours ago   Up 35 hours   80/tcp    gifted_swirles
# 02215441176b   f8ff9f3b1898   "/bin/bash"              36 hours ago   Up 36 hours   80/tcp    upbeat_bassi
# b80bcb53e981   f8ff9f3b1898   "/bin/bash"              36 hours ago   Up 36 hours   80/tcp    recursing_raman
# b0d70f2c4e2e   f8ff9f3b1898   "/bin/bash"              36 hours ago   Up 36 hours   80/tcp    tender_volhard
# cc549d853b49   f8ff9f3b1898   "sh /webservice/yloc…"   5 days ago     Up 5 days     80/tcp    musing_gauss
#files can be copied from Docker to the local computer using the following models: 
docker cp 4e8be9bd0d6a:/YLoc /home/inf-47-2020/ThesisTrich/YLoc_Results
docker cp 02215441176b:/YLoc /home/inf-47-2020/ThesisTrich/YLoc_Results
docker cp b0d70f2c4e2e:/YLoc/Vi_results  /home/inf-47-2020/ThesisTrich/YLoc_Results
docker cp b80bcb53e981:/YLoc/Vi_results /home/inf-47-2020/ThesisTrich/YLoc_Results
#below, the commands for the preliminary project files
python yloc.py /YLoc/Vi_hostDataOG/BM_newprots_may21.anaeromoeba_edit.fasta "YLoc+* Animals" BM_newprots_may21.anaeromoeba_edit_YL y > BM_newprots_may21.anaeromoeba_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/BS_newprots_may21.anaeromoeba_edit.fasta "YLoc+* Animals" BS_newprots_may21.anaeromoeba_edit_YL y > BS_newprots_may21.anaeromoeba_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Dientamoeba_fragilis.43352.aa_edit.fasta "YLoc+* Animals" Dientamoeba_fragilis.43352.aa_edit_YL y > Dientamoeba_fragilis.43352.aa_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Histomonas_meleagridis.135588.aa_edit.fasta "YLoc+* Animals" Histomonas_meleagridis.135588.aa_edit_YL y > Histomonas_meleagridis.135588.aa_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Pentatrichomonas_hominis.5728.aa_edit.fasta "YLoc+* Animals" Pentatrichomonas_hominis.5728.aa_edit_YL y > Pentatrichomonas_hominis.5728.aa_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/SC_newprots_may21.anaeromoeba_edit.fasta "YLoc+* Animals" SC_newprots_may21.anaeromoeba_edit_YL y > SC_newprots_may21.anaeromoeba_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Tetratrichomonas_gallinarum.5730.aa_edit.fasta "YLoc+* Animals" Tetratrichomonas_gallinarum.5730.aa_edit_YL y > Tetratrichomonas_gallinarum.5730.aa_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Trichomonas_foetus.PRJNA345179_edit.fasta "YLoc+* Animals" Trichomonas_foetus.PRJNA345179_edit_YL y > Trichomonas_foetus.PRJNA345179_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta "YLoc+* Animals" Trichomonas_vaginalis_GenBank.PRJNA16084_edit_YL y > Trichomonas_vaginalis_GenBank.PRJNA16084_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataOG/Trichomonas_vaginalis_RefSeq.G3_edit.fasta "YLoc+* Animals" Trichomonas_vaginalis_RefSeq.G3_edit_YL y > Trichomonas_vaginalis_RefSeq.G3_edit_YL.txt

#new files
python yloc.py /YLoc/Vi_hostDataNEW/Carpediemonas_membranifera.PRJNA719540_edit.fasta "YLoc+* Animals" Carpediemonas_membranifera.PRJNA719540_edit_YL y > Carpediemonas_membranifera.PRJNA719540_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00701_Giardia_intestinalis_edit.fasta "YLoc+* Animals" EP00701_Giardia_intestinalis_edit_YL y > EP00701_Giardia_intestinalis_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00703_Trepomonas_sp_PC1_edit.fasta "YLoc+* Animals" EP00703_Trepomonas_sp_PC1_edit_YL y > EP00703_Trepomonas_sp_PC1_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00708_Paratrimastix_pyriformis_edit.fasta "YLoc+* Animals" EP00708_Paratrimastix_pyriformis_edit_YL y > EP00708_Paratrimastix_pyriformis_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00764_Aduncisulcus_paluster.fasta "YLoc+* Animals" EP00764_Aduncisulcus_paluster_YL y > EP00764_Aduncisulcus_paluster_YL.txt
python yloc.py /YLoc/Vi_hostDataNEWEP00766_Chilomastix_caulleryi_edit.fasta "YLoc+* Animals" EP00766_Chilomastix_caulleryi_edit_YL y > EP00766_Chilomastix_caulleryi_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00767_Chilomastix_cuspidata_edit.fasta "YLoc+* Animals" EP00767_Chilomastix_cuspidata_edit_YL y > EP00767_Chilomastix_cuspidata_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00768_Dysnectes_brevis_edit.fasta "YLoc+* Animals" EP00768_Dysnectes_brevis_edit_YL y > EP00768_Dysnectes_brevis_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00769_Ergobibamus_cyprinoides_edit.fasta "YLoc+* Animals" EP00769_Ergobibamus_cyprinoides_edit_YL y > EP00769_Ergobibamus_cyprinoides_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00770_Monocercomonoides_exilis_edit.fasta "YLoc+* Animals" EP00770_Monocercomonoides_exilis_edit_YL y > EP00770_Monocercomonoides_exilis_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00771_Trimastix_marina_edit.fasta "YLoc+* Animals" EP00771_Trimastix_marina_edit_YL y > EP00771_Trimastix_marina_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/EP00792_Barthelona_sp_PAP020_edit.fasta "YLoc+* Animals" EP00792_Barthelona_sp_PAP020_edit_YL y > EP00792_Barthelona_sp_PAP020_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/GiardiaDB_GintestinalisADH_edit.fasta "YLoc+* Animals" GiardiaDB_GintestinalisADH_edit_YL y > GiardiaDB_GintestinalisADH_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/GiardiaDB_GintestinalisBGS_B_edit.fasta "YLoc+* Animals" GiardiaDB_GintestinalisBGS_B_edit_YL y > GiardiaDB_GintestinalisBGS_B_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/GiardiaDB_GintestinalisBGS_edit.fasta "YLoc+* Animals" GiardiaDB_GintestinalisBGS_edit_YL y > GiardiaDB_GintestinalisBGS_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/GiardiaDB_GintestinalisEP15_edit.fasta "YLoc+* Animals" GiardiaDB_GintestinalisEP15_edit_YL y > GiardiaDB_GintestinalisEP15_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/Giardia_intestinalis.PRJNA1439_edit.fasta "YLoc+* Animals" Giardia_intestinalis.PRJNA1439_edit_YL y > Giardia_intestinalis.PRJNA1439_edit_YL.txt


#done earlier, because it's a smaller file
python yloc.py /YLoc/Vi_hostDataNEW/Giardia_muris.PRJNA524057_edit.fasta "YLoc+* Animals" Giardia_muris.PRJNA524057_edit_YL y > Giardia_muris.PRJNA524057_edit_YL.txt


python yloc.py /YLoc/Vi_hostDataNEW/Histomonas_meleagridis.PRJNA594289_edit.fasta "YLoc+* Animals" Histomonas_meleagridis.PRJNA594289_edit_YL y > Histomonas_meleagridis.PRJNA594289_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/Kipferlia_bialata.PRJDB5223_edit.fasta "YLoc+* Animals" Kipferlia_bialata.PRJDB5223_edit_YL y > Kipferlia_bialata.PRJDB5223_edit_YL.txt
python yloc.py /YLoc/Vi_hostDataNEW/Spironucleus_salmonicida.PRJNA60811_edit.fasta "YLoc+* Animals" Spironucleus_salmonicida.PRJNA60811_edit_YL y > Spironucleus_salmonicida.PRJNA60811_edit_YL.txt


###
#A. lanta integration
#I need to mount a new container for this
docker run --mount type=bind,source=/home/vi_varga/Thesis_Data,target=/YLoc/Vi_hostData,readonly -it fce2dc66b359 /bin/bash
cd YLoc
mkdir Vi_results
#start the container manually via the Docker window, then do:
docker container attach practical_wright
#run on Asus computer
python yloc.py /YLoc/Vi_hostData/Anaeramoeba_lanta_160522_edit.fasta "YLoc+* Animals" Anaeramoeba_lanta_160522_edit_YL y > Anaeramoeba_lanta_160522_edit_YL.txt
#currently running...
docker cp a324cdd2cc24:/YLoc/Vi_results /mnt/c/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/Data/YLoc_Ubuntu
#done! 

```

### BUSCO

```bash
conda activate env-BUSCO
#usage:
proteinortho -h
# *****************************************************************
# Proteinortho with PoFF version 6.0.33 - An orthology detection tool
# *****************************************************************
#      |
#     / \
#    /\ /\
#   / / \ \
# Usage: proteinortho6.pl [OPTIONS] FASTA1 FASTA2 [FASTA...] (one for each species, at least 2)
# Options:
#          [General options]
#          -project=    prefix for all result file names [default: myproject]
#          -cpus=       number of processors to use [default: auto]
#          -ram=        maximal used ram threshold for LAPACK and the input graph in MB [default: 75% of the free memory]
#          -silent      sets verbose to 0
#          -temp=       path for temporary files [default: working directory]
#          -keep        stores temporary blast results for reuse (same -project=... name is mandatory)
#          -force       forces the recalculation of the blast results in any case in step=2. Also forces the recreation of the database generation in step=1
#          -clean       remove all unnecessary files after processing
#          -step=       1 -> generate indices
#                       2 -> run blast (and ff-adj, if -synteny is set)
#                       3 -> clustering
#                       0 -> all (default)
#          -isoform=    Enables the isoform merging:
#                       ncbi -> if the word 'isoform' is found
#                       uniprot -> 'Isoform of XYZ' (You need to add the *_additional.fasta files to the analysis)
#                       trinity -> using '_iX' suffix
#          [Search options]
#          -p=          blast program [default: diamond]
#                       {autoblast|blastp|blastn|tblastx|blastp_legacy|blastn_legacy|tblastx_legacy|diamond|usearch|ublast|lastp|lastn|rapsearch|topaz|blatp|blatn|mmseqsp|mmseqsn}
#                       The suffix 'p' or 'n' indicates aminoacid fasta files (p) or nucleotide fasta files (n).
#                       The suffix '_legacy' indicates legacy blastall (otherwise blast+ is used).
#          -checkfasta  Checks if the given fasta files are compatible with the algorithm of -p
#          -e=          E-value for blast [default: 1e-05]
#          -sim=        min. reciprocal similarity for additional hits (0..1) [default: 0.95]
#                       1 : only the best reciprocal hits are reported
#                       0 : all possible reciprocal blast matches (within the -evalue) are reported
#          [Synteny options]
#          -synteny     activate PoFF extension to separate similar sequences print
#                       by contextual adjacencies (requires .gff for each .fasta)
#          -alpha=      PoFF: weight of adjacencies vs. sequence similarity
#                       (default: 0.5)
#          [Clustering options]
#          -singles     report singleton genes without any hit
#          -conn=       min. algebraic connectivity [default: 0.1]
#          -xml         produces an OrthoXML formatted file of the *.proteinortho.
#          (...) show more with --help
# For more information see the man page: 'proteinortho -man' or online: https://gitlab.com/paulklemm_PHD/proteinortho
# Or you can use the GUI proteinorthoHelper.html (available at http://lechnerlab.de/proteinortho/)
# Do you have suggestions or need more help: write a mail to lechner@staff.uni-marburg.de.
busco --list-datasets
# 2022-07-01 16:27:20 INFO:       Downloading information on latest versions of BUSCO data...

# ################################################

# Datasets available to be used with BUSCO v4 and v5:

#  bacteria_odb10
#      - acidobacteria_odb10
#      - actinobacteria_phylum_odb10
#          - actinobacteria_class_odb10
#              - corynebacteriales_odb10
#              - micrococcales_odb10
#              - propionibacteriales_odb10
#              - streptomycetales_odb10
#              - streptosporangiales_odb10
#          - coriobacteriia_odb10
#              - coriobacteriales_odb10
#      - aquificae_odb10
#      - bacteroidetes-chlorobi_group_odb10
#          - bacteroidetes_odb10
#              - bacteroidia_odb10
#                  - bacteroidales_odb10
#              - cytophagia_odb10
#                  - cytophagales_odb10
#              - flavobacteriia_odb10
#                  - flavobacteriales_odb10
#              - sphingobacteriia_odb10
#          - chlorobi_odb10
#      - chlamydiae_odb10
#      - chloroflexi_odb10
#      - cyanobacteria_odb10
#          - chroococcales_odb10
#          - nostocales_odb10
#          - oscillatoriales_odb10
#          - synechococcales_odb10
#      - firmicutes_odb10
#          - bacilli_odb10
#              - bacillales_odb10
#              - lactobacillales_odb10
#          - clostridia_odb10
#              - clostridiales_odb10
#              - thermoanaerobacterales_odb10
#          - selenomonadales_odb10
#          - tissierellia_odb10
#              - tissierellales_odb10
#      - fusobacteria_odb10
#          - fusobacteriales_odb10
#      - planctomycetes_odb10
#      - proteobacteria_odb10
#          - alphaproteobacteria_odb10
#              - rhizobiales_odb10
#                  - rhizobium-agrobacterium_group_odb10
#              - rhodobacterales_odb10
#              - rhodospirillales_odb10
#              - rickettsiales_odb10
#              - sphingomonadales_odb10
#          - betaproteobacteria_odb10
#              - burkholderiales_odb10
#              - neisseriales_odb10
#              - nitrosomonadales_odb10
#          - delta-epsilon-subdivisions_odb10
#              - deltaproteobacteria_odb10
#                  - desulfobacterales_odb10
#                  - desulfovibrionales_odb10
#                  - desulfuromonadales_odb10
#              - epsilonproteobacteria_odb10
#                  - campylobacterales_odb10
#          - gammaproteobacteria_odb10
#              - alteromonadales_odb10
#              - cellvibrionales_odb10
#              - chromatiales_odb10
#              - enterobacterales_odb10
#              - legionellales_odb10
#              - oceanospirillales_odb10
#              - pasteurellales_odb10
#              - pseudomonadales_odb10
#              - thiotrichales_odb10
#              - vibrionales_odb10
#              - xanthomonadales_odb10
#      - spirochaetes_odb10
#          - spirochaetia_odb10
#              - spirochaetales_odb10
#      - synergistetes_odb10
#      - tenericutes_odb10
#          - mollicutes_odb10
#              - entomoplasmatales_odb10
#              - mycoplasmatales_odb10
#      - thermotogae_odb10
#      - verrucomicrobia_odb10
#  archaea_odb10
#      - thaumarchaeota_odb10
#      - thermoprotei_odb10
#          - thermoproteales_odb10
#          - sulfolobales_odb10
#          - desulfurococcales_odb10
#      - euryarchaeota_odb10
#          - thermoplasmata_odb10
#          - methanococcales_odb10
#          - methanobacteria_odb10
#          - methanomicrobia_odb10
#              - methanomicrobiales_odb10
#          - halobacteria_odb10
#              - halobacteriales_odb10
#              - natrialbales_odb10
#              - haloferacales_odb10
#  eukaryota_odb10
#      - alveolata_odb10
#          - apicomplexa_odb10
#              - aconoidasida_odb10
#                  - plasmodium_odb10
#              - coccidia_odb10
#      - euglenozoa_odb10
#      - fungi_odb10
#          - ascomycota_odb10
#              - dothideomycetes_odb10
#                  - capnodiales_odb10
#                  - pleosporales_odb10
#              - eurotiomycetes_odb10
#                  - chaetothyriales_odb10
#                  - eurotiales_odb10
#                  - onygenales_odb10
#              - leotiomycetes_odb10
#                  - helotiales_odb10
#              - saccharomycetes_odb10
#              - sordariomycetes_odb10
#                  - glomerellales_odb10
#                  - hypocreales_odb10
#          - basidiomycota_odb10
#              - agaricomycetes_odb10
#                  - agaricales_odb10
#                  - boletales_odb10
#                  - polyporales_odb10
#              - tremellomycetes_odb10
#          - microsporidia_odb10
#          - mucoromycota_odb10
#              - mucorales_odb10
#      - metazoa_odb10
#          - arthropoda_odb10
#              - arachnida_odb10
#              - insecta_odb10
#                  - endopterygota_odb10
#                      - diptera_odb10
#                      - hymenoptera_odb10
#                      - lepidoptera_odb10
#                  - hemiptera_odb10
#          - mollusca_odb10
#          - nematoda_odb10
#          - vertebrata_odb10
#              - actinopterygii_odb10
#                  - cyprinodontiformes_odb10
#              - tetrapoda_odb10
#                  - mammalia_odb10
#                      - eutheria_odb10
#                          - euarchontoglires_odb10
#                              - glires_odb10
#                              - primates_odb10
#                          - laurasiatheria_odb10
#                              - carnivora_odb10
#                              - cetartiodactyla_odb10
#                  - sauropsida_odb10
#                      - aves_odb10
#                          - passeriformes_odb10
#      - stramenopiles_odb10
#      - viridiplantae_odb10
#          - chlorophyta_odb10
#          - embryophyta_odb10
#              - liliopsida_odb10
#                  - poales_odb10
#              - eudicots_odb10
#                  - brassicales_odb10
#                  - fabales_odb10
#                  - solanales_odb10
#  viruses (no root dataset)
#      - alphaherpesvirinae_odb10
#      - baculoviridae_odb10
#      - rudiviridae_odb10
#      - betaherpesvirinae_odb10
#      - herpesviridae_odb10
#      - poxviridae_odb10
#      - tevenvirinae_odb10
#      - aviadenovirus_odb10
#      - enquatrovirus_odb10
#      - teseptimavirus_odb10
#      - bclasvirinae_odb10
#      - fromanvirus_odb10
#      - skunavirus_odb10
#      - betabaculovirus_odb10
#      - pahexavirus_odb10
#      - alphabaculovirus_odb10
#      - tunavirinae_odb10
#      - simplexvirus_odb10
#      - gammaherpesvirinae_odb10
#      - varicellovirus_odb10
#      - cheoctovirus_odb10
#      - guernseyvirinae_odb10
#      - tequatrovirus_odb10
#      - chordopoxvirinae_odb10
#      - peduovirus_odb10
#      - iridoviridae_odb10
#      - spounavirinae_odb10
#first the larger database
nohup busco --in /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/ --out trichLantaEuk --mode proteins --lineage_dataset eukaryota_odb10 --cpu 20 --evalue 0.001 &> nohup_Lanta_euk.out &
# [1] 3240136
#then the smaller database
nohup busco --in /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/ --out trichLantaEugl --mode proteins --lineage_dataset euglenozoa_odb10 --cpu 20 --evalue 0.001 &> nohup_Lanta_eugl2.out &
# [1] 3368679

```


## Orthologous Clustering

### Broccoli

```bash
#program use
#based on manual, here: https://github.com/rderelle/Broccoli/blob/master/manual_Broccoli_v1.2.pdf
conda activate env-broccoli
nohup python /home/inf-47-2020/bin/Broccoli/broccoli.py -dir ../../DataFiles/EncodedData -phylogenies ml -threads 20 &> nohup_Alanta.out &
# [2] 2411376
#the following is copied from the nohup record file: 
#             Broccoli v1.1
#  --- STEP 1: kmer clustering
#  # parameters
#  input dir     : ../../DataFiles/EncodedData
#  kmer size     : 100
#  kmer nb aa    : 15
#  # check input files
#  32 input files
#  556047 sequences
#  # kmer clustering
#  32 proteomes on 20 threads
#  -> 457323 proteins saved for the next step
#  --- STEP 2: phylomes
#  # parameters
#  e_value     : 0.001
#  nb_hits     : 6
#  gaps        : 0.7
#  phylogenies : maximum likelihood
#  threads     : 20
#  # check input files
#  32 input fasta files
#  457323 sequences
#  # build phylomes ... be patient
#  done
#  --- STEP 3: network analysis
#  ## parameters
#  species overlap  : 0.5
#  min edge weight  : 0.1
#  min nb hits      : 2
#  chimeric edges   : 0.5
#  chimeric species : 3
#  threads          : 20
#  ## get ortho and para
#  extract ortho from similarity
#  extract ortho from trees
#  remove ortho found only once
#  extract para from trees
#  ## network analysis
#  build network:
#       _ 364861 nodes
#       _ 11277930 edges
#  load similarity search outputs
#  compute lcc for each node
#  apply LPA and corrections:
#       _ 39273 connected components
#       _ 49817 communities
#       _ 153 chimeric proteins
#       _ 9466 spurious hits removed
#  --- STEP 4: orthologous pairs
#  ## parameters
#  ratio ortho  : 0.5
#  not same sp  : False
#  threads      : 20
#  ## load data
#  load NO tree results
#  load tree results
#  load OGs
#  ## analyse 39715 orthologous groups 1 by 1
#  done

```

### OrthoFinder

```bash
#program usage
#run from the ThesisTrich/ directory
#working in PuTTY terminal
orthofinder -f DataFiles/EncodedData/ -a 15 -o OrthoFinder_Results3/
# OrthoFinder version 2.5.4 Copyright (C) 2014 David Emms
# 2022-05-16 17:11:51 : Starting OrthoFinder 2.5.4
# 256 thread(s) for highly parallel tasks (BLAST searches etc.)
# 15 thread(s) for OrthoFinder algorithm
# Checking required programs are installed
# ----------------------------------------
# Test can run "mcl -h" - ok
# Test can run "fastme -i OrthoFinder_Results3/Results_May16/WorkingDirectory/SimpleTest.phy -o OrthoFinder_Results3/Results_May16/WorkingDirectory/SimpleTest.tre" - ok
# Dividing up work for BLAST for parallel processing
# --------------------------------------------------
# 2022-05-16 17:11:53 : Creating diamond database 1 of 32
# 2022-05-16 17:11:53 : Creating diamond database 2 of 32
# 2022-05-16 17:11:53 : Creating diamond database 3 of 32
# 2022-05-16 17:11:54 : Creating diamond database 4 of 32
# 2022-05-16 17:11:54 : Creating diamond database 5 of 32
# 2022-05-16 17:11:54 : Creating diamond database 6 of 32
# 2022-05-16 17:11:54 : Creating diamond database 7 of 32
# 2022-05-16 17:11:54 : Creating diamond database 8 of 32
# 2022-05-16 17:11:54 : Creating diamond database 9 of 32
# 2022-05-16 17:11:54 : Creating diamond database 10 of 32
# 2022-05-16 17:11:54 : Creating diamond database 11 of 32
# 2022-05-16 17:11:54 : Creating diamond database 12 of 32
# 2022-05-16 17:11:54 : Creating diamond database 13 of 32
# 2022-05-16 17:11:54 : Creating diamond database 14 of 32
# 2022-05-16 17:11:55 : Creating diamond database 15 of 32
# 2022-05-16 17:11:55 : Creating diamond database 16 of 32
# 2022-05-16 17:11:55 : Creating diamond database 17 of 32
# 2022-05-16 17:11:55 : Creating diamond database 18 of 32
# 2022-05-16 17:11:55 : Creating diamond database 19 of 32
# 2022-05-16 17:11:55 : Creating diamond database 20 of 32
# 2022-05-16 17:11:55 : Creating diamond database 21 of 32
# 2022-05-16 17:11:55 : Creating diamond database 22 of 32
# 2022-05-16 17:11:55 : Creating diamond database 23 of 32
# 2022-05-16 17:11:56 : Creating diamond database 24 of 32
# 2022-05-16 17:11:56 : Creating diamond database 25 of 32
# 2022-05-16 17:11:56 : Creating diamond database 26 of 32
# 2022-05-16 17:11:56 : Creating diamond database 27 of 32
# 2022-05-16 17:11:56 : Creating diamond database 28 of 32
# 2022-05-16 17:11:56 : Creating diamond database 29 of 32
# 2022-05-16 17:11:56 : Creating diamond database 30 of 32
# 2022-05-16 17:11:57 : Creating diamond database 31 of 32
# 2022-05-16 17:11:57 : Creating diamond database 32 of 32
# Running diamond all-versus-all
# ------------------------------
# Using 256 thread(s)
# 2022-05-16 17:11:57 : This may take some time....
# 2022-05-16 17:11:58 : Done 0 of 1024
# 2022-05-16 17:13:22 : Done 100 of 1024
# 2022-05-16 17:14:11 : Done 200 of 1024
# 2022-05-16 17:14:42 : Done 300 of 1024
# 2022-05-16 17:15:12 : Done 400 of 1024
# 2022-05-16 17:15:33 : Done 500 of 1024
# 2022-05-16 17:15:48 : Done 600 of 1024
# 2022-05-16 17:16:02 : Done 700 of 1024
# 2022-05-16 18:23:18 : Done all-versus-all sequence search
# Running OrthoFinder algorithm
# -----------------------------
# 2022-05-16 18:23:18 : Initial processing of each species
# 2022-05-16 18:23:24 : Initial processing of species 14 complete
# 2022-05-16 18:23:25 : Initial processing of species 12 complete
# 2022-05-16 18:23:26 : Initial processing of species 9 complete
# 2022-05-16 18:23:26 : Initial processing of species 7 complete
# 2022-05-16 18:23:27 : Initial processing of species 5 complete
# 2022-05-16 18:23:27 : Initial processing of species 10 complete
# 2022-05-16 18:23:28 : Initial processing of species 3 complete
# 2022-05-16 18:23:28 : Initial processing of species 11 complete
# 2022-05-16 18:23:29 : Initial processing of species 6 complete
# 2022-05-16 18:23:29 : Initial processing of species 4 complete
# 2022-05-16 18:23:31 : Initial processing of species 8 complete
# 2022-05-16 18:23:32 : Initial processing of species 16 complete
# 2022-05-16 18:23:32 : Initial processing of species 18 complete
# 2022-05-16 18:23:34 : Initial processing of species 19 complete
# 2022-05-16 18:23:34 : Initial processing of species 20 complete
# 2022-05-16 18:23:34 : Initial processing of species 17 complete
# 2022-05-16 18:23:35 : Initial processing of species 21 complete
# 2022-05-16 18:23:37 : Initial processing of species 13 complete
# 2022-05-16 18:23:38 : Initial processing of species 0 complete
# 2022-05-16 18:23:39 : Initial processing of species 22 complete
# 2022-05-16 18:23:42 : Initial processing of species 27 complete
# 2022-05-16 18:23:42 : Initial processing of species 1 complete
# 2022-05-16 18:23:45 : Initial processing of species 24 complete
# 2022-05-16 18:23:46 : Initial processing of species 15 complete
# 2022-05-16 18:24:08 : Initial processing of species 2 complete
# 2022-05-16 18:24:17 : Initial processing of species 23 complete
# 2022-05-16 18:24:18 : Initial processing of species 29 complete
# 2022-05-16 18:24:21 : Initial processing of species 26 complete
# 2022-05-16 18:24:26 : Initial processing of species 25 complete
# 2022-05-16 18:24:43 : Initial processing of species 30 complete
# 2022-05-16 18:24:44 : Initial processing of species 31 complete
# 2022-05-16 18:24:47 : Initial processing of species 28 complete
# 2022-05-16 18:25:02 : Connected putative homologues
# 2022-05-16 18:25:03 : Written final scores for species 14 to graph file
# 2022-05-16 18:25:04 : Written final scores for species 12 to graph file
# 2022-05-16 18:25:04 : Written final scores for species 5 to graph file
# 2022-05-16 18:25:05 : Written final scores for species 9 to graph file
# 2022-05-16 18:25:05 : Written final scores for species 4 to graph file
# 2022-05-16 18:25:05 : Written final scores for species 7 to graph file
# 2022-05-16 18:25:05 : Written final scores for species 6 to graph file
# 2022-05-16 18:25:05 : Written final scores for species 3 to graph file
# 2022-05-16 18:25:05 : Written final scores for species 10 to graph file
# 2022-05-16 18:25:06 : Written final scores for species 11 to graph file
# 2022-05-16 18:25:06 : Written final scores for species 16 to graph file
# 2022-05-16 18:25:06 : Written final scores for species 18 to graph file
# 2022-05-16 18:25:07 : Written final scores for species 19 to graph file
# 2022-05-16 18:25:07 : Written final scores for species 20 to graph file
# 2022-05-16 18:25:07 : Written final scores for species 21 to graph file
# 2022-05-16 18:25:07 : Written final scores for species 17 to graph file
# 2022-05-16 18:25:08 : Written final scores for species 8 to graph file
# 2022-05-16 18:25:08 : Written final scores for species 0 to graph file
# 2022-05-16 18:25:08 : Written final scores for species 1 to graph file
# 2022-05-16 18:25:08 : Written final scores for species 22 to graph file
# 2022-05-16 18:25:09 : Written final scores for species 13 to graph file
# 2022-05-16 18:25:10 : Written final scores for species 27 to graph file
# 2022-05-16 18:25:13 : Written final scores for species 24 to graph file
# 2022-05-16 18:25:13 : Written final scores for species 15 to graph file
# 2022-05-16 18:25:15 : Written final scores for species 2 to graph file
# 2022-05-16 18:25:16 : Written final scores for species 23 to graph file
# 2022-05-16 18:25:18 : Written final scores for species 29 to graph file
# 2022-05-16 18:25:21 : Written final scores for species 26 to graph file
# 2022-05-16 18:25:25 : Written final scores for species 25 to graph file
# 2022-05-16 18:25:32 : Written final scores for species 30 to graph file
# 2022-05-16 18:25:32 : Written final scores for species 31 to graph file
# 2022-05-16 18:25:38 : Written final scores for species 28 to graph file
# WARNING: program called by OrthoFinder produced output to stderr
# Command: mcl OrthoFinder_Results3/Results_May16/WorkingDirectory/OrthoFinder_graph.txt -I 1.5 -o OrthoFinder_Results3/Results_May16/WorkingDirectory/clusters_OrthoFinder_I1.5.txt -te 15 -V all
# stdout
# ------
# b''
# stderr
# ------
# b'[mcl] cut <20> instances of overlap\n'
# 2022-05-16 18:31:31 : Ran MCL
# Writing orthogroups to file
# ---------------------------
# OrthoFinder assigned 481336 genes (86.6% of total) to 78982 orthogroups. Fifty percent of all genes were in orthogroups with 9 or more genes (G50 was 9) and were contained in the largest 8611 orthogroups (O50 was 8611). There were 161 orthogroups with all species present and 0 of these consisted entirely of single-copy genes.
# 2022-05-16 18:31:52 : Done orthogroups
# Analysing Orthogroups
# =====================
# Calculating gene distances
# --------------------------
# 2022-05-16 18:34:45 : Done
# Inferring gene and species trees
# --------------------------------
# 2022-05-16 18:34:49 : Done 0 of 25038
# 2022-05-16 18:34:50 : Done 1000 of 25038
# 2022-05-16 18:34:50 : Done 2000 of 25038
# 2022-05-16 18:34:51 : Done 3000 of 25038
# 2022-05-16 18:34:51 : Done 4000 of 25038
# 2022-05-16 18:34:52 : Done 5000 of 25038
# 2022-05-16 18:34:52 : Done 6000 of 25038
# 2022-05-16 18:34:52 : Done 7000 of 25038
# 2022-05-16 18:34:53 : Done 8000 of 25038
# 2022-05-16 18:34:53 : Done 9000 of 25038
# 2022-05-16 18:34:54 : Done 10000 of 25038
# 2022-05-16 18:34:54 : Done 11000 of 25038
# 2022-05-16 18:34:54 : Done 12000 of 25038
# 2022-05-16 18:34:55 : Done 13000 of 25038
# 2022-05-16 18:34:55 : Done 14000 of 25038
# 2022-05-16 18:34:56 : Done 15000 of 25038
# 2022-05-16 18:34:56 : Done 16000 of 25038
# 2022-05-16 18:34:56 : Done 17000 of 25038
# 2022-05-16 18:34:57 : Done 18000 of 25038
# 2022-05-16 18:34:57 : Done 19000 of 25038
# 2022-05-16 18:34:58 : Done 20000 of 25038
# 2022-05-16 18:34:58 : Done 21000 of 25038
# 2022-05-16 18:34:58 : Done 22000 of 25038
# 2022-05-16 18:34:59 : Done 23000 of 25038
# 2022-05-16 18:34:59 : Done 24000 of 25038
# 161 trees had all species present and will be used by STAG to infer the species tree
# Best outgroup(s) for species tree
# ---------------------------------
# 2022-05-16 18:37:02 : Starting STRIDE
# 2022-05-16 18:37:23 : Done STRIDE
# Observed 7216 well-supported, non-terminal duplications. 7216 support the best roots and 0 contradict them.
# Best outgroups for species tree:
#   Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, Kipferlia_bialata.PRJDB5223_edit, EP00767_Chilomastix_cuspidata_edit, EP00764_Aduncisulcus_paluster_edit, EP00766_Chilomastix_caulleryi_edit, EP00769_Ergobibamus_cyprinoides_edit, EP00768_Dysnectes_brevis_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit, Carpediemonas_membranifera.PRJNA719540_edit
#   EP00769_Ergobibamus_cyprinoides_edit, Carpediemonas_membranifera.PRJNA719540_edit
#   EP00792_Barthelona_sp_PAP020_edit
#   EP00769_Ergobibamus_cyprinoides_edit
#   Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, EP00768_Dysnectes_brevis_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit
#   Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, Kipferlia_bialata.PRJDB5223_edit, EP00767_Chilomastix_cuspidata_edit, EP00766_Chilomastix_caulleryi_edit, EP00768_Dysnectes_brevis_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit
#   EP00771_Trimastix_marina_edit, EP00708_Paratrimastix_pyriformis_edit
#   EP00770_Monocercomonoides_exilis_edit
#   EP00768_Dysnectes_brevis_edit
#   BS_newprots_may21.anaeromoeba_edit, SC_newprots_may21.anaeromoeba_edit, BM_newprots_may21.anaeromoeba_edit, Anaeramoeba_lanta_160522_edit
#   EP00767_Chilomastix_cuspidata_edit
#   EP00766_Chilomastix_caulleryi_edit
#   EP00764_Aduncisulcus_paluster_edit
#   Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit
#   EP00771_Trimastix_marina_edit, EP00770_Monocercomonoides_exilis_edit, EP00708_Paratrimastix_pyriformis_edit
#   Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, Kipferlia_bialata.PRJDB5223_edit, EP00767_Chilomastix_cuspidata_edit, EP00764_Aduncisulcus_paluster_edit, EP00766_Chilomastix_caulleryi_edit, EP00768_Dysnectes_brevis_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit
#   Kipferlia_bialata.PRJDB5223_edit
#   Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, Kipferlia_bialata.PRJDB5223_edit, EP00768_Dysnectes_brevis_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit
#   Carpediemonas_membranifera.PRJNA719540_edit
#   EP00767_Chilomastix_cuspidata_edit, EP00766_Chilomastix_caulleryi_edit
#   Trichomonas_foetus.PRJNA345179_edit, Anaeramoeba_lanta_160522_edit, EP00771_Trimastix_marina_edit, Trichomonas_vaginalis_RefSeq.G3_edit, SC_newprots_may21.anaeromoeba_edit, Histomonas_meleagridis.PRJNA594289_edit, BS_newprots_may21.anaeromoeba_edit, Histomonas_meleagridis.135588.aa_edit, Dientamoeba_fragilis.43352.aa_edit, Trichomonas_vaginalis_GenBank.PRJNA16084_edit, EP00770_Monocercomonoides_exilis_edit, BM_newprots_may21.anaeromoeba_edit, Pentatrichomonas_hominis.5728.aa_edit, EP00708_Paratrimastix_pyriformis_edit, Tetratrichomonas_gallinarum.5730.aa_edit
#   Trichomonas_foetus.PRJNA345179_edit, Trichomonas_vaginalis_RefSeq.G3_edit, Histomonas_meleagridis.PRJNA594289_edit, Histomonas_meleagridis.135588.aa_edit, Dientamoeba_fragilis.43352.aa_edit, Trichomonas_vaginalis_GenBank.PRJNA16084_edit, Pentatrichomonas_hominis.5728.aa_edit, Tetratrichomonas_gallinarum.5730.aa_edit
#   Trichomonas_foetus.PRJNA345179_edit, Anaeramoeba_lanta_160522_edit, Trichomonas_vaginalis_RefSeq.G3_edit, SC_newprots_may21.anaeromoeba_edit, Histomonas_meleagridis.PRJNA594289_edit, BS_newprots_may21.anaeromoeba_edit, Histomonas_meleagridis.135588.aa_edit, Dientamoeba_fragilis.43352.aa_edit, Trichomonas_vaginalis_GenBank.PRJNA16084_edit, BM_newprots_may21.anaeromoeba_edit, Pentatrichomonas_hominis.5728.aa_edit, Tetratrichomonas_gallinarum.5730.aa_edit
# WARNING: Multiple potential species tree roots were identified, only one will be analyed.
# Reconciling gene trees and species tree
# ---------------------------------------
# Outgroup: Giardia_intestinalis.PRJNA1439_edit, GiardiaDB_GintestinalisBGS_B_edit, GiardiaDB_GintestinalisEP15_edit, Giardia_muris.PRJNA524057_edit, Spironucleus_salmonicida.PRJNA60811_edit, Kipferlia_bialata.PRJDB5223_edit, EP00767_Chilomastix_cuspidata_edit, EP00764_Aduncisulcus_paluster_edit, EP00766_Chilomastix_caulleryi_edit, EP00769_Ergobibamus_cyprinoides_edit, EP00768_Dysnectes_brevis_edit, EP00701_Giardia_intestinalis_edit, GiardiaDB_GintestinalisBGS_edit, GiardiaDB_GintestinalisADH_edit, EP00703_Trepomonas_sp_PC1_edit, Carpediemonas_membranifera.PRJNA719540_edit
# 2022-05-16 18:37:23 : Starting Recon and orthologues
# 2022-05-16 18:37:23 : Starting OF Orthologues
# 2022-05-16 18:37:26 : Done 0 of 25038
# 2022-05-16 18:37:28 : Done 1000 of 25038
# 2022-05-16 18:37:29 : Done 2000 of 25038
# 2022-05-16 18:37:29 : Done 3000 of 25038
# 2022-05-16 18:37:29 : Done 4000 of 25038
# 2022-05-16 18:37:30 : Done 5000 of 25038
# 2022-05-16 18:37:30 : Done 6000 of 25038
# 2022-05-16 18:37:30 : Done 7000 of 25038
# 2022-05-16 18:37:30 : Done 8000 of 25038
# 2022-05-16 18:37:31 : Done 9000 of 25038
# 2022-05-16 18:37:31 : Done 10000 of 25038
# 2022-05-16 18:37:31 : Done 11000 of 25038
# 2022-05-16 18:37:31 : Done 12000 of 25038
# 2022-05-16 18:37:31 : Done 13000 of 25038
# 2022-05-16 18:37:31 : Done 14000 of 25038
# 2022-05-16 18:37:32 : Done 15000 of 25038
# 2022-05-16 18:37:32 : Done 16000 of 25038
# 2022-05-16 18:37:32 : Done 17000 of 25038
# 2022-05-16 18:37:32 : Done 18000 of 25038
# 2022-05-16 18:37:32 : Done 19000 of 25038
# 2022-05-16 18:37:32 : Done 20000 of 25038
# 2022-05-16 18:37:32 : Done 21000 of 25038
# 2022-05-16 18:37:33 : Done 22000 of 25038
# 2022-05-16 18:37:33 : Done 23000 of 25038
# 2022-05-16 18:37:33 : Done 24000 of 25038
# 2022-05-16 18:37:33 : Done 25000 of 25038
# 2022-05-16 18:37:34 : Done OF Orthologues
# Writing results files
# =====================
# 2022-05-16 18:37:45 : Done orthologues
# Results:
#     OrthoFinder_Results3/Results_May16/
# CITATION:
#  When publishing work that uses OrthoFinder please cite:
#  Emms D.M. & Kelly S. (2019), Genome Biology 20:238
#  If you use the species tree in your work then please also cite:
#  Emms D.M. & Kelly S. (2017), MBE 34(12): 3267-3278
#  Emms D.M. & Kelly S. (2018), bioRxiv https://doi.org/10.1101/267914
#done!

```

### ProteinOrtho

```bash
#installation with conda
conda create -n env-protOrtho
conda activate env-protOrtho
conda install proteinortho
###
#usage:
proteinortho -h
# *****************************************************************
# Proteinortho with PoFF version 6.0.33 - An orthology detection tool
# *****************************************************************
#      |
#     / \
#    /\ /\
#   / / \ \
# Usage: proteinortho6.pl [OPTIONS] FASTA1 FASTA2 [FASTA...] (one for each species, at least 2)
# Options:
#          [General options]
#          -project=    prefix for all result file names [default: myproject]
#          -cpus=       number of processors to use [default: auto]
#          -ram=        maximal used ram threshold for LAPACK and the input graph in MB [default: 75% of the free memory]
#          -silent      sets verbose to 0
#          -temp=       path for temporary files [default: working directory]
#          -keep        stores temporary blast results for reuse (same -project=... name is mandatory)
#          -force       forces the recalculation of the blast results in any case in step=2. Also forces the recreation of the database generation in step=1
#          -clean       remove all unnecessary files after processing
#          -step=       1 -> generate indices
#                       2 -> run blast (and ff-adj, if -synteny is set)
#                       3 -> clustering
#                       0 -> all (default)
#          -isoform=    Enables the isoform merging:
#                       ncbi -> if the word 'isoform' is found
#                       uniprot -> 'Isoform of XYZ' (You need to add the *_additional.fasta files to the analysis)
#                       trinity -> using '_iX' suffix
#          [Search options]
#          -p=          blast program [default: diamond]
#                       {autoblast|blastp|blastn|tblastx|blastp_legacy|blastn_legacy|tblastx_legacy|diamond|usearch|ublast|lastp|lastn|rapsearch|topaz|blatp|blatn|mmseqsp|mmseqsn}
#                       The suffix 'p' or 'n' indicates aminoacid fasta files (p) or nucleotide fasta files (n).
#                       The suffix '_legacy' indicates legacy blastall (otherwise blast+ is used).
#          -checkfasta  Checks if the given fasta files are compatible with the algorithm of -p
#          -e=          E-value for blast [default: 1e-05]
#          -sim=        min. reciprocal similarity for additional hits (0..1) [default: 0.95]
#                       1 : only the best reciprocal hits are reported
#                       0 : all possible reciprocal blast matches (within the -evalue) are reported
#          [Synteny options]
#          -synteny     activate PoFF extension to separate similar sequences print
#                       by contextual adjacencies (requires .gff for each .fasta)
#          -alpha=      PoFF: weight of adjacencies vs. sequence similarity
#                       (default: 0.5)
#          [Clustering options]
#          -singles     report singleton genes without any hit
#          -conn=       min. algebraic connectivity [default: 0.1]
#          -xml         produces an OrthoXML formatted file of the *.proteinortho.
#          (...) show more with --help
# For more information see the man page: 'proteinortho -man' or online: https://gitlab.com/paulklemm_PHD/proteinortho
# Or you can use the GUI proteinorthoHelper.html (available at http://lechnerlab.de/proteinortho/)
# Do you have suggestions or need more help: write a mail to lechner@staff.uni-marburg.de.
#in the ProteinOrtho_Results/ directory
nohup proteinortho -project=trichAlanta -cpus=20 -p=diamond -e=0.001 /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta &> nohup_Alanta.out &
# [1] 2859469
mkdir DiamondFiles/
mv /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.diamond.dmnd /home/inf-47-2020/ThesisTrich/ProteinOrtho_Results/Alanta_Run/DiamondFiles/
#copying out the contents of the nohup file
# *****************************************************************
# Proteinortho with PoFF version 6.0.33 - An orthology detection tool
# *****************************************************************
# Using 20 CPU threads, Detected 'diamond' version 2.0.14
# Checking input files.
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Anaeramoeba_lanta_160522_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/BM_newprots_may21.anaeromoeba_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/BS_newprots_may21.anaeromoeba_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Carpediemonas_membranifera.PRJNA719540_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Dientamoeba_fragilis.43352.aa_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00701_Giardia_intestinalis_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00703_Trepomonas_sp_PC1_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00708_Paratrimastix_pyriformis_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00764_Aduncisulcus_paluster_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00766_Chilomastix_caulleryi_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00767_Chilomastix_cuspidata_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00768_Dysnectes_brevis_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00769_Ergobibamus_cyprinoides_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00770_Monocercomonoides_exilis_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00771_Trimastix_marina_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00792_Barthelona_sp_PAP020_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisADH_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisBGS_B_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisBGS_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisEP15_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Giardia_intestinalis.PRJNA1439_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Giardia_muris.PRJNA524057_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Histomonas_meleagridis.135588.aa_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Histomonas_meleagridis.PRJNA594289_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Kipferlia_bialata.PRJDB5223_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Pentatrichomonas_hominis.5728.aa_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/SC_newprots_may21.anaeromoeba_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Spironucleus_salmonicida.PRJNA60811_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Tetratrichomonas_gallinarum.5730.aa_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Trichomonas_foetus.PRJNA345179_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta... ok
# Checking /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Trichomonas_vaginalis_RefSeq.G3_edit.fasta... ok
# **Step 1**
# Generating indices.
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Tetratrichomonas_gallinarum.5730.aa_edit.fasta'       (67510 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta'  (59681 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Trichomonas_vaginalis_RefSeq.G3_edit.fasta'   (59679 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Pentatrichomonas_hominis.5728.aa_edit.fasta'  (44819 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/SC_newprots_may21.anaeromoeba_edit.fasta'     (29853 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/BS_newprots_may21.anaeromoeba_edit.fasta'     (29791 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Trichomonas_foetus.PRJNA345179_edit.fasta'    (25030 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Histomonas_meleagridis.PRJNA594289_edit.fasta'        (22256 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00792_Barthelona_sp_PAP020_edit.fasta'      (21618 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Kipferlia_bialata.PRJDB5223_edit.fasta'       (17269 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00770_Monocercomonoides_exilis_edit.fasta'  (16767 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00764_Aduncisulcus_paluster_edit.fasta'     (15474 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Anaeramoeba_lanta_160522_edit.fasta'  (14842 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/BM_newprots_may21.anaeromoeba_edit.fasta'     (14817 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00768_Dysnectes_brevis_edit.fasta'  (9689 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Spironucleus_salmonicida.PRJNA60811_edit.fasta'       (8667 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00767_Chilomastix_cuspidata_edit.fasta'     (8607 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Carpediemonas_membranifera.PRJNA719540_edit.fasta'    (8272 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00703_Trepomonas_sp_PC1_edit.fasta' (7980 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00708_Paratrimastix_pyriformis_edit.fasta'  (7108 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Histomonas_meleagridis.135588.aa_edit.fasta'  (6982 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00766_Chilomastix_caulleryi_edit.fasta'     (6869 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Dientamoeba_fragilis.43352.aa_edit.fasta'     (6558 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00701_Giardia_intestinalis_edit.fasta'      (6502 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisBGS_B_edit.fasta'      (6098 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisADH_edit.fasta'        (5147 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisEP15_edit.fasta'       (5007 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Giardia_intestinalis.PRJNA1439_edit.fasta'    (4965 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00769_Ergobibamus_cyprinoides_edit.fasta'   (4809 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/Giardia_muris.PRJNA524057_edit.fasta' (4660 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/GiardiaDB_GintestinalisBGS_edit.fasta'        (4470 sequences)
# Building database for '/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/EP00771_Trimastix_marina_edit.fasta'  (4251 sequences)
# **Step 2** using diamond
# Running blast analysis: 100% (496/496)
# [OUTPUT] -> written to trichAlanta.blast-graph
# **Step 3**
# Clustering by similarity (Proteinortho mode) and 20 cpu core(s).
# [OUTPUT] -> Orthologous groups are written to trichAlanta.proteinortho.tsv
# [OUTPUT] -> Orthologous pairs are written to trichAlanta.proteinortho-graph
# [OUTPUT] -> Summary is written to trichAlanta.proteinortho-graph.summary
# [OUTPUT] -> You can extract a html version of the output using :
# proteinortho2html.pl trichAlanta.proteinortho.tsv [PLACE FASTA FILES HERE] >trichAlanta.proteinortho.html
# [OUTPUT] -> You can extract a xml version of the output using :
# proteinortho2xml.pl trichAlanta.proteinortho.tsv >trichAlanta.proteinortho.tsv.xml
# All finished.

```

### SonicParanoid

```bash
#program usage
conda activate sonicparanoid
sonicparanoid -i /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData -o ./A_lanta_Run -p Trich_Thesis3 -t 15 --aln-tool diamond -m most-sensitive -ca -op &>log_Alanta_detach.txt &
# [1] 3342162
#this time, successful completion! 
#copy of printout from log file (log_Alanta_detach.txt): 
# INFO: at present DB indexing with Diamond is not supported in SonicParanoid.
# The --no-indexing parameter was set to True.
# Run START:      Thu May 19 10:43:31 2022
# SonicParanoid 1.3.8 will be executed with the following parameters:
# Run ID: Trich_Thesis3
# Run directory: /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/runs/Trich_Thesis3
# Input directory: /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData
# Input proteomes:        32
# Output directory: /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run
# Alignment tool: diamond
# Run mode:       most-sensitive (Diamond [--ultra-sensitive])
# Minimum bitscore:       40
# Alignments directory: /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/alignments/
# Pairwise tables directory: /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/runs/Trich_Thesis3/pairwise_orthologs/
# Directory with ortholog groups: /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/runs/Trich_Thesis3/ortholog_groups/
# Pairwise tables database directory: /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/orthologs_db
# Update run:     False
# Create pre-filter indexes:      False
# Complete overwrite:     False
# Re-create ortholog tables:      False
# MCL inflation:  1.50
# Threads:        15
# Memory per thread (Gigabytes):  133.60
# Minimum memory per thread (Gigabytes):  1.00
# Compress alignments:    True
# Compression level:      5
# SonicParanoid installation directory: /home/inf-47-2020/miniconda3/envs/sonicparanoid/lib/python3.8/site-packages/sonicparanoid
# Python version: 3.8.12 | packaged by conda-forge | (default, Oct 12 2021, 21:57:06)
# [GCC 9.4.0]
# Python executables: /home/inf-47-2020/miniconda3/envs/sonicparanoid/bin/python
# For the 32 input species 496 combinations are possible.
# 1024 Diamond alignments will be performed...
# Creating 32 Diamond databases...
# Diamond databases creation elapsed time (seconds):      1.39
# Performing 1024 alignment using Diamond...
# Elapsed time for all-vs-all alignments using Diamond (seconds): 14525.792
# Predicting 496 ortholog tables...
# Ortholog tables creation elapsed time (seconds):        34.017
# Creating ortholog groups using MCL clustering...
# Creating orthology matrixes...
# Ortholog matrixes creation elapsed time (seconds):      8.766
# Merging inparalog matrixes...
# Inparalogs merging elapsed time (seconds):      12.192
# Creating input matrix for MCL...
# MCL graph creation elapsed time (seconds):      17.206
# Running MCL...
# MCL execution elapsed time (seconds):   310.373
# Generating final output files...
# Elapsed time for the creation of final output (seconds):        11.087
# Ortholog groups creation elapsed time (seconds):        359.662
# Ortholog groups:        66802
# Single-copy ortholog groups:    52332
# The ortholog groups and related statistics are stored in the directory:
# /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/runs/Trich_Thesis3/ortholog_groups
# Creating file with homolog pairs...
# Total orthologous relations     3547544
# Ortholog-pairs (benchmark file) creation time (seconds):        6.233
# Total elapsed time (seconds):   14928.737
# CITATION INFO:
# Cosentino Salvatore & Wataru Iwasaki. (2019),
# Volume 35, Issue 1, 01 January 2019, Pages 149–151, https://doi.org/10.1093/bioinformatics/bty631
# INSTRUCTIONS:
# Find the user manual for SonicParanoid at
# http://iwasakilab.k.u-tokyo.ac.jp/sonicparanoid/
# CONTACT:
# Salvatore Cosentino:    salvo981@gmail.com  or  salvocos@bs.s.u-tokyo.ac.jp
# Wataru Iwasaki:         iwasaki@k.u-tokyo.ac.jp

```

## Quality Checking

All of the programs that have been run need some basic quality/sanity checks done, to ensure that the files were created properly.

```bash
#reference data
grep -c ">" *.fasta
# BM_newprots_may21.anaeromoeba_edit.fasta:14817
# BS_newprots_may21.anaeromoeba_edit.fasta:29791
# Carpediemonas_membranifera.PRJNA719540_edit.fasta:8272
# Dientamoeba_fragilis.43352.aa_edit.fasta:6558
# EP00701_Giardia_intestinalis_edit.fasta:6502
# EP00703_Trepomonas_sp_PC1_edit.fasta:7980
# EP00708_Paratrimastix_pyriformis_edit.fasta:7108
# EP00764_Aduncisulcus_paluster_edit.fasta:15474
# EP00766_Chilomastix_caulleryi_edit.fasta:6869
# EP00767_Chilomastix_cuspidata_edit.fasta:8607
# EP00768_Dysnectes_brevis_edit.fasta:9689
# EP00769_Ergobibamus_cyprinoides_edit.fasta:4809
# EP00770_Monocercomonoides_exilis_edit.fasta:16767
# EP00771_Trimastix_marina_edit.fasta:4251
# EP00792_Barthelona_sp_PAP020_edit.fasta:21618
# GiardiaDB_GintestinalisADH_edit.fasta:5147
# GiardiaDB_GintestinalisBGS_B_edit.fasta:6098
# GiardiaDB_GintestinalisBGS_edit.fasta:4470
# GiardiaDB_GintestinalisEP15_edit.fasta:5007
# Giardia_intestinalis.PRJNA1439_edit.fasta:4965
# Giardia_muris.PRJNA524057_edit.fasta:4660
# Histomonas_meleagridis.135588.aa_edit.fasta:6982
# Histomonas_meleagridis.PRJNA594289_edit.fasta:22256
# Kipferlia_bialata.PRJDB5223_edit.fasta:17269
# Pentatrichomonas_hominis.5728.aa_edit.fasta:44819
# SC_newprots_may21.anaeromoeba_edit.fasta:29853
# Spironucleus_salmonicida.PRJNA60811_edit.fasta:8667
# Tetratrichomonas_gallinarum.5730.aa_edit.fasta:67510
# Trichomonas_foetus.PRJNA345179_edit.fasta:25030
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta:59681
# Trichomonas_vaginalis_RefSeq.G3_edit.fasta:59679
#checking the programs
#SignalP
ls *.signalp5 | while read file; do
    echo $file;
    grep -v "#" $file | wc -l;
done
# BM_newprots_may21.anaeromoeba_edit_summary.signalp5
# 14817
# BS_newprots_may21.anaeromoeba_edit_summary.signalp5
# 29791
# Carpediemonas_membranifera.PRJNA719540_edit_summary.signalp5
# 8272
# Dientamoeba_fragilis.43352.aa_edit_summary.signalp5
# 6558
# EP00701_Giardia_intestinalis_edit_summary.signalp5
# 6502
# EP00703_Trepomonas_sp_PC1_edit_summary.signalp5
# 7980
# EP00708_Paratrimastix_pyriformis_edit_summary.signalp5
# 7108
# EP00764_Aduncisulcus_paluster_edit_summary.signalp5
# 15474
# EP00766_Chilomastix_caulleryi_edit_summary.signalp5
# 6869
# EP00767_Chilomastix_cuspidata_edit_summary.signalp5
# 8607
# EP00768_Dysnectes_brevis_edit_summary.signalp5
# 9689
# EP00769_Ergobibamus_cyprinoides_edit_summary.signalp5
# 4809
# EP00770_Monocercomonoides_exilis_edit_summary.signalp5
# 16767
# EP00771_Trimastix_marina_edit_summary.signalp5
# 4251
# EP00792_Barthelona_sp_PAP020_edit_summary.signalp5
# 21618
# GiardiaDB_GintestinalisADH_edit_summary.signalp5
# 5147
# GiardiaDB_GintestinalisBGS_B_edit_summary.signalp5
# 6098
# GiardiaDB_GintestinalisBGS_edit_summary.signalp5
# 4470
# GiardiaDB_GintestinalisEP15_edit_summary.signalp5
# 5007
# Giardia_intestinalis.PRJNA1439_edit_summary.signalp5
# 4965
# Giardia_muris.PRJNA524057_edit_summary.signalp5
# 4660
# Histomonas_meleagridis.135588.aa_edit_summary.signalp5
# 6982
# Histomonas_meleagridis.PRJNA594289_edit_summary.signalp5
# 22256
# Kipferlia_bialata.PRJDB5223_edit_summary.signalp5
# 17269
# Pentatrichomonas_hominis.5728.aa_edit_summary.signalp5
# 44819
# SC_newprots_may21.anaeromoeba_edit_summary.signalp5
# 29853
# Spironucleus_salmonicida.PRJNA60811_edit_summary.signalp5
# 8667
# Tetratrichomonas_gallinarum.5730.aa_edit_summary.signalp5
# 67510
# Trichomonas_foetus.PRJNA345179_edit_summary.signalp5
# 25030
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_summary.signalp5
# 59681
# Trichomonas_vaginalis_RefSeq.G3_edit_summary.signalp5
# 59679
ls *.signalp5 | while read file; do
    seq_num="$(grep -v "#" $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_newprots_may21.anaeromoeba_edit_summary.signalp5 : 14817
# BS_newprots_may21.anaeromoeba_edit_summary.signalp5 : 29791
# Carpediemonas_membranifera.PRJNA719540_edit_summary.signalp5 : 8272
# Dientamoeba_fragilis.43352.aa_edit_summary.signalp5 : 6558
# EP00701_Giardia_intestinalis_edit_summary.signalp5 : 6502
# EP00703_Trepomonas_sp_PC1_edit_summary.signalp5 : 7980
# EP00708_Paratrimastix_pyriformis_edit_summary.signalp5 : 7108
# EP00764_Aduncisulcus_paluster_edit_summary.signalp5 : 15474
# EP00766_Chilomastix_caulleryi_edit_summary.signalp5 : 6869
# EP00767_Chilomastix_cuspidata_edit_summary.signalp5 : 8607
# EP00768_Dysnectes_brevis_edit_summary.signalp5 : 9689
# EP00769_Ergobibamus_cyprinoides_edit_summary.signalp5 : 4809
# EP00770_Monocercomonoides_exilis_edit_summary.signalp5 : 16767
# EP00771_Trimastix_marina_edit_summary.signalp5 : 4251
# EP00792_Barthelona_sp_PAP020_edit_summary.signalp5 : 21618
# GiardiaDB_GintestinalisADH_edit_summary.signalp5 : 5147
# GiardiaDB_GintestinalisBGS_B_edit_summary.signalp5 : 6098
# GiardiaDB_GintestinalisBGS_edit_summary.signalp5 : 4470
# GiardiaDB_GintestinalisEP15_edit_summary.signalp5 : 5007
# Giardia_intestinalis.PRJNA1439_edit_summary.signalp5 : 4965
# Giardia_muris.PRJNA524057_edit_summary.signalp5 : 4660
# Histomonas_meleagridis.135588.aa_edit_summary.signalp5 : 6982
# Histomonas_meleagridis.PRJNA594289_edit_summary.signalp5 : 22256
# Kipferlia_bialata.PRJDB5223_edit_summary.signalp5 : 17269
# Pentatrichomonas_hominis.5728.aa_edit_summary.signalp5 : 44819
# SC_newprots_may21.anaeromoeba_edit_summary.signalp5 : 29853
# Spironucleus_salmonicida.PRJNA60811_edit_summary.signalp5 : 8667
# Tetratrichomonas_gallinarum.5730.aa_edit_summary.signalp5 : 67510
# Trichomonas_foetus.PRJNA345179_edit_summary.signalp5 : 25030
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_summary.signalp5 : 59681
# Trichomonas_vaginalis_RefSeq.G3_edit_summary.signalp5 : 59679
#yup, these are a match
#TargetP
ls *.targetp2 | while read file; do
    seq_num="$(grep -v "#" $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_newprots_may21.anaeromoeba_edit_summary.targetp2 : 14817
# BS_newprots_may21.anaeromoeba_edit_summary.targetp2 : 29791
# Carpediemonas_membranifera.PRJNA719540_edit_summary.targetp2 : 8272
# Dientamoeba_fragilis.43352.aa_edit_summary.targetp2 : 6558
# EP00701_Giardia_intestinalis_edit_summary.targetp2 : 6502
# EP00703_Trepomonas_sp_PC1_edit_summary.targetp2 : 7980
# EP00708_Paratrimastix_pyriformis_edit_summary.targetp2 : 7108
# EP00764_Aduncisulcus_paluster_edit_summary.targetp2 : 15474
# EP00766_Chilomastix_caulleryi_edit_summary.targetp2 : 6869
# EP00767_Chilomastix_cuspidata_edit_summary.targetp2 : 8607
# EP00768_Dysnectes_brevis_edit_summary.targetp2 : 9689
# EP00769_Ergobibamus_cyprinoides_edit_summary.targetp2 : 4809
# EP00770_Monocercomonoides_exilis_edit_summary.targetp2 : 16767
# EP00771_Trimastix_marina_edit_summary.targetp2 : 4251
# EP00792_Barthelona_sp_PAP020_edit_summary.targetp2 : 21618
# GiardiaDB_GintestinalisADH_edit_summary.targetp2 : 5147
# GiardiaDB_GintestinalisBGS_B_edit_summary.targetp2 : 6098
# GiardiaDB_GintestinalisBGS_edit_summary.targetp2 : 4470
# GiardiaDB_GintestinalisEP15_edit_summary.targetp2 : 5007
# Giardia_intestinalis.PRJNA1439_edit_summary.targetp2 : 4965
# Giardia_muris.PRJNA524057_edit_summary.targetp2 : 4660
# Histomonas_meleagridis.135588.aa_edit_summary.targetp2 : 6982
# Histomonas_meleagridis.PRJNA594289_edit_summary.targetp2 : 22256
# Kipferlia_bialata.PRJDB5223_edit_summary.targetp2 : 17269
# Pentatrichomonas_hominis.5728.aa_edit_summary.targetp2 : 44819
# SC_newprots_may21.anaeromoeba_edit_summary.targetp2 : 29853
# Spironucleus_salmonicida.PRJNA60811_edit_summary.targetp2 : 8667
# Tetratrichomonas_gallinarum.5730.aa_edit.1st_half_summary.targetp2 : 33755
# Tetratrichomonas_gallinarum.5730.aa_edit.2nd_half_summary.targetp2 : 33755
#total 67510
# Trichomonas_foetus.PRJNA345179_edit_summary.targetp2 : 25030
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit.1st_half_summary.targetp2 : 29840
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit.2nd_half_summary.targetp2 : 29841
#total 59681
# Trichomonas_vaginalis_RefSeq.G3_edit.1st_half_summary.targetp2 : 29840
# Trichomonas_vaginalis_RefSeq.G3_edit.2nd_half_summary.targetp2 : 29839
#total 29679
#yup these are good
#EggNOG
#annotations
ls */*.annotations | while read file; do
    seq_num="$(grep -v "#" $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_newprots_may21.anaeromoeba_edit/BM_newprots_may21.anaeromoeba_edit.emap.emapper.annotations : 8791
# BS_newprots_may21.anaeromoeba_edit/BS_newprots_may21.anaeromoeba_edit.emap.emapper.annotations : 18032
# Carpediemonas_membranifera.PRJNA719540_edit/Carpediemonas_membranifera.PRJNA719540_edit.emap.emapper.annotations : 3757
# Dientamoeba_fragilis.43352.aa_edit/Dientamoeba_fragilis.43352.aa_edit.emap.emapper.annotations : 3875
# EP00701_Giardia_intestinalis_edit/EP00701_Giardia_intestinalis_edit.emap.emapper.annotations : 2432
# EP00703_Trepomonas_sp_PC1_edit/EP00703_Trepomonas_sp_PC1_edit.emap.emapper.annotations : 2860
# EP00708_Paratrimastix_pyriformis_edit/EP00708_Paratrimastix_pyriformis_edit.emap.emapper.annotations : 3767
# EP00764_Aduncisulcus_paluster_edit/EP00764_Aduncisulcus_paluster_edit.emap.emapper.annotations : 8190
# EP00766_Chilomastix_caulleryi_edit/EP00766_Chilomastix_caulleryi_edit.emap.emapper.annotations : 2293
# EP00767_Chilomastix_cuspidata_edit/EP00767_Chilomastix_cuspidata_edit.emap.emapper.annotations : 3432
# EP00768_Dysnectes_brevis_edit/EP00768_Dysnectes_brevis_edit.emap.emapper.annotations : 3788
# EP00769_Ergobibamus_cyprinoides_edit/EP00769_Ergobibamus_cyprinoides_edit.emap.emapper.annotations : 2814
# EP00770_Monocercomonoides_exilis_edit/EP00770_Monocercomonoides_exilis_edit.emap.emapper.annotations : 5084
# EP00771_Trimastix_marina_edit/EP00771_Trimastix_marina_edit.emap.emapper.annotations : 2886
# GiardiaDB_GintestinalisADH_edit/GiardiaDB_GintestinalisADH_edit.emap.emapper.annotations : 2414
# GiardiaDB_GintestinalisBGS_B_edit/GiardiaDB_GintestinalisBGS_B_edit.emap.emapper.annotations : 2932
# GiardiaDB_GintestinalisBGS_edit/GiardiaDB_GintestinalisBGS_edit.emap.emapper.annotations : 2086
# GiardiaDB_GintestinalisEP15_edit/GiardiaDB_GintestinalisEP15_edit.emap.emapper.annotations : 2363
# Giardia_intestinalis.PRJNA1439_edit/Giardia_intestinalis.PRJNA1439_edit.emap.emapper.annotations : 2342
# Giardia_muris.PRJNA524057_edit/Giardia_muris.PRJNA524057_edit.emap.emapper.annotations : 2293
# Histomonas_meleagridis.135588.aa_edit/Histomonas_meleagridis.135588.aa_edit.emap.emapper.annotations : 3276
# Histomonas_meleagridis.PRJNA594289_edit/Histomonas_meleagridis.PRJNA594289_edit.emap.emapper.annotations : 13735
# Kipferlia_bialata.PRJDB5223_edit/Kipferlia_bialata.PRJDB5223_edit.emap.emapper.annotations : 4320
# Pentatrichomonas_hominis.5728.aa_edit/Pentatrichomonas_hominis.5728.aa_edit.emap.emapper.annotations : 11399
# SC_newprots_may21.anaeromoeba_edit/SC_newprots_may21.anaeromoeba_edit.emap.emapper.annotations : 18180
# Spironucleus_salmonicida.PRJNA60811_edit/Spironucleus_salmonicida.PRJNA60811_edit.emap.emapper.annotations : 2159
# Tetratrichomonas_gallinarum.5730.aa_edit/Tetratrichomonas_gallinarum.5730.aa_edit.emap.emapper.annotations : 20034
# Trichomonas_foetus.PRJNA345179_edit/Trichomonas_foetus.PRJNA345179_edit.emap.emapper.annotations : 11881
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.emap.emapper.annotations : 25322
# Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.annotations : 25322
#PFam
ls */*.pfam | while read file; do
    seq_num="$(grep -v "#" $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_newprots_may21.anaeromoeba_edit/BM_newprots_may21.anaeromoeba_edit.emap.emapper.pfam : 13674
# BS_newprots_may21.anaeromoeba_edit/BS_newprots_may21.anaeromoeba_edit.emap.emapper.pfam : 26240
# Carpediemonas_membranifera.PRJNA719540_edit/Carpediemonas_membranifera.PRJNA719540_edit.emap.emapper.pfam : 5101
# Dientamoeba_fragilis.43352.aa_edit/Dientamoeba_fragilis.43352.aa_edit.emap.emapper.pfam : 4028
# EP00701_Giardia_intestinalis_edit/EP00701_Giardia_intestinalis_edit.emap.emapper.pfam : 4127
# EP00703_Trepomonas_sp_PC1_edit/EP00703_Trepomonas_sp_PC1_edit.emap.emapper.pfam : 3248
# EP00708_Paratrimastix_pyriformis_edit/EP00708_Paratrimastix_pyriformis_edit.emap.emapper.pfam : 4287
# EP00764_Aduncisulcus_paluster_edit/EP00764_Aduncisulcus_paluster_edit.emap.emapper.pfam : 8237
# EP00766_Chilomastix_caulleryi_edit/EP00766_Chilomastix_caulleryi_edit.emap.emapper.pfam : 2158
# EP00767_Chilomastix_cuspidata_edit/EP00767_Chilomastix_cuspidata_edit.emap.emapper.pfam : 3961
# EP00768_Dysnectes_brevis_edit/EP00768_Dysnectes_brevis_edit.emap.emapper.pfam : 4633
# EP00769_Ergobibamus_cyprinoides_edit/EP00769_Ergobibamus_cyprinoides_edit.emap.emapper.pfam : 3102
# EP00770_Monocercomonoides_exilis_edit/EP00770_Monocercomonoides_exilis_edit.emap.emapper.pfam : 6996
# EP00771_Trimastix_marina_edit/EP00771_Trimastix_marina_edit.emap.emapper.pfam : 3918
# GiardiaDB_GintestinalisADH_edit/GiardiaDB_GintestinalisADH_edit.emap.emapper.pfam : 3970
# GiardiaDB_GintestinalisBGS_B_edit/GiardiaDB_GintestinalisBGS_B_edit.emap.emapper.pfam : 4814
# GiardiaDB_GintestinalisBGS_edit/GiardiaDB_GintestinalisBGS_edit.emap.emapper.pfam : 3447
# GiardiaDB_GintestinalisEP15_edit/GiardiaDB_GintestinalisEP15_edit.emap.emapper.pfam : 4107
# Giardia_intestinalis.PRJNA1439_edit/Giardia_intestinalis.PRJNA1439_edit.emap.emapper.pfam : 4130
# Giardia_muris.PRJNA524057_edit/Giardia_muris.PRJNA524057_edit.emap.emapper.pfam : 3772
# Histomonas_meleagridis.135588.aa_edit/Histomonas_meleagridis.135588.aa_edit.emap.emapper.pfam : 3532
# Histomonas_meleagridis.PRJNA594289_edit/Histomonas_meleagridis.PRJNA594289_edit.emap.emapper.pfam : 18000
# Kipferlia_bialata.PRJDB5223_edit/Kipferlia_bialata.PRJDB5223_edit.emap.emapper.pfam : 4730
# Pentatrichomonas_hominis.5728.aa_edit/Pentatrichomonas_hominis.5728.aa_edit.emap.emapper.pfam : 12880
# SC_newprots_may21.anaeromoeba_edit/SC_newprots_may21.anaeromoeba_edit.emap.emapper.pfam : 26340
# Spironucleus_salmonicida.PRJNA60811_edit/Spironucleus_salmonicida.PRJNA60811_edit.emap.emapper.pfam : 2658
# Tetratrichomonas_gallinarum.5730.aa_edit/Tetratrichomonas_gallinarum.5730.aa_edit.emap.emapper.pfam : 20829
# Trichomonas_foetus.PRJNA345179_edit/Trichomonas_foetus.PRJNA345179_edit.emap.emapper.pfam : 15991
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.emap.emapper.pfam : 32030
# Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.pfam : 31990
#so as it turns out, there's no way to check these
#EggNOG does not include the queries w/o matching orthologs in its results files
#so with organisms like ours... the annotations are gonna be WAY below the number of sequences
#ref: https://github.com/eggnogdb/eggnog-mapper/issues/317
#not to mention, there are sometimes multiple lines per proetin sequence
#so these results can't be counted so easily
#DeepLoc
ls *.txt | while read file; do
    seq_num="$(grep -v "ID\t" $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_anaeromoeba_DL.txt : 14818
# BS_anaeromoeba_DL.txt : 29792
# Carpediemonas_membranifera_DL.txt : 8273
# D_fragilis_DL.txt : 6559
# EP00701_Giardia_intestinalis_DL.txt : 6502
# EP00703_Trepomonas_sp_PC1_DL.txt : 7980
# EP00708_Paratrimastix_pyriformis_DL.txt : 7109
# EP00764_Aduncisulcus_paluster_DL.txt : 15475
# EP00766_Chilomastix_caulleryi_DL.txt : 6870
# EP00767_Chilomastix_cuspidata_DL.txt : 8606
# EP00768_Dysnectes_brevis_DL.txt : 9690
# EP00769_Ergobibamus_cyprinoides_DL.txt : 4810
# EP00770_Monocercomonoides_exilis_DL.txt : 16766
# EP00771_Trimastix_marina_DL.txt : 4252
# EP00792_Barthelona_sp_PAP020_DL.txt : 21617
# GiardiaDB_GintestinalisADH_DL.txt : 5148
# GiardiaDB_GintestinalisBGS_B_DL.txt : 6099
# GiardiaDB_GintestinalisBGS_DL.txt : 4471
# GiardiaDB_GintestinalisEP15_DL.txt : 5008
# Giardia_intestinalis.PRJNA1439_DL.txt : 4966
# Giardia_muris.PRJNA524057_DL.txt : 4660
# Histomonas_meleagridis.PRJNA594289_DL.txt : 22256
# H_meleagridis_DL.txt : 6982
# Kipferlia_bialata.PRJDB5223_DL.txt : 17269
# P_hominis_DL.txt : 44819
# SC_anaeromoeba_DL.txt : 29853
# Spironucleus_salmonicida.PRJNA60811_DL.txt : 8667
# T_foetus_DL.txt : 25030
# T_gallinarum_DL.txt : 67508
# T_vaginalis_GB_DL.txt : 59678
# T_vaginalis_RS_DL.txt : 59676
#that gives me 1 more than I wanted, so the removeal of header line isn't working
tail -n +<N+1> <filename>
#ref: https://stackoverflow.com/questions/604864/print-a-file-skipping-the-first-x-lines-in-bash
ls *.txt | while read file; do
    seq_num="$(tail -n +2 $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_anaeromoeba_DL.txt : 14817
# BS_anaeromoeba_DL.txt : 29791
# Carpediemonas_membranifera_DL.txt : 8272
# D_fragilis_DL.txt : 6558
# EP00701_Giardia_intestinalis_DL.txt : 6502
# EP00703_Trepomonas_sp_PC1_DL.txt : 7980
# EP00708_Paratrimastix_pyriformis_DL.txt : 7108
# EP00764_Aduncisulcus_paluster_DL.txt : 15474
# EP00766_Chilomastix_caulleryi_DL.txt : 6869
# EP00767_Chilomastix_cuspidata_DL.txt : 8607
# EP00768_Dysnectes_brevis_DL.txt : 9689
# EP00769_Ergobibamus_cyprinoides_DL.txt : 4809
# EP00770_Monocercomonoides_exilis_DL.txt : 16767
# EP00771_Trimastix_marina_DL.txt : 4251
# EP00792_Barthelona_sp_PAP020_DL.txt : 21618
# GiardiaDB_GintestinalisADH_DL.txt : 5147
# GiardiaDB_GintestinalisBGS_B_DL.txt : 6098
# GiardiaDB_GintestinalisBGS_DL.txt : 4470
# GiardiaDB_GintestinalisEP15_DL.txt : 5007
# Giardia_intestinalis.PRJNA1439_DL.txt : 4965
# Giardia_muris.PRJNA524057_DL.txt : 4660
# Histomonas_meleagridis.PRJNA594289_DL.txt : 22256
# H_meleagridis_DL.txt : 6982
# Kipferlia_bialata.PRJDB5223_DL.txt : 17269
# P_hominis_DL.txt : 44819
# SC_anaeromoeba_DL.txt : 29853
# Spironucleus_salmonicida.PRJNA60811_DL.txt : 8667
# T_foetus_DL.txt : 25030
# T_gallinarum_DL.txt : 67510
# T_vaginalis_GB_DL.txt : 59681
# T_vaginalis_RS_DL.txt : 59679
#this one looks good
#the old file name format was different, so I thought I missed some
#but I figured it out, we're fine
#MitoFates
ls *_MFresults.txt | while read file; do
    seq_num="$(tail -n +2 $file | wc -l)";
    echo $file ":" $seq_num;
done
# BM_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults.txt : 14817
# BS_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults.txt : 29788
# Carpediemonas_membranifera.PRJNA719540_edit_StandardAA_nonM_MFresults.txt : 8270
# Dientamoeba_fragilis.43352.aa_edit_StandardAA_nonM_MFresults.txt : 1358
# EP00701_Giardia_intestinalis_edit_StandardAA_nonM_MFresults.txt : 6501
# EP00703_Trepomonas_sp_PC1_edit_StandardAA_nonM_MFresults.txt : 1293
# EP00708_Paratrimastix_pyriformis_edit_StandardAA_nonM_MFresults.txt : 2017
# EP00764_Aduncisulcus_paluster_edit_StandardAA_nonM_MFresults.txt : 4282
# EP00766_Chilomastix_caulleryi_edit_StandardAA_nonM_MFresults.txt : 3267
# EP00767_Chilomastix_cuspidata_edit_StandardAA_nonM_MFresults.txt : 3893
# EP00768_Dysnectes_brevis_edit_StandardAA_nonM_MFresults.txt : 4521
# EP00769_Ergobibamus_cyprinoides_edit_StandardAA_nonM_MFresults.txt : 1433
# EP00770_Monocercomonoides_exilis_edit_StandardAA_nonM_MFresults.txt : 16589
# EP00771_Trimastix_marina_edit_StandardAA_nonM_MFresults.txt : 2451
# EP00792_Barthelona_sp_PAP020_edit_StandardAA_nonM_MFresults.txt : 7727
# GiardiaDB_GintestinalisADH_edit_StandardAA_nonM_MFresults.txt : 4357
# GiardiaDB_GintestinalisBGS_B_edit_StandardAA_nonM_MFresults.txt : 4904
# GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM_replaceAA__MFresults.txt : 4470
# GiardiaDB_GintestinalisEP15_edit_StandardAA_nonM_MFresults.txt : 4980
# Giardia_intestinalis.PRJNA1439_edit_StandardAA_nonM_MFresults.txt : 4963
# Giardia_muris.PRJNA524057_edit_StandardAA_nonM_MFresults.txt : 4656
# Histomonas_meleagridis.135588.aa_edit_StandardAA_nonM_MFresults.txt : 2399
# Histomonas_meleagridis.PRJNA594289_edit_StandardAA_nonM_MFresults.txt : 22256
# Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_fin_MFresults.txt : 11136
# Pentatrichomonas_hominis.5728.aa_edit_StandardAA_nonM_MFresults.txt : 13451
# SC_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults.txt : 29852
# Spironucleus_salmonicida.PRJNA60811_edit_StandardAA_nonM_MFresults.txt : 8662
# Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA_nonM_MFresults.txt : 15582
# Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_fin_MFresults.txt : 25029
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_fin_MFresults.txt : 57671
# Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_fin_MFresults.txt : 57669
#which reminds me... these source files are different from the regulars, actually
grep -c ">" *.fasta
# BM_newprots_may21.anaeromoeba_edit_StandardAA.fasta:14817
# BM_newprots_may21.anaeromoeba_edit_StandardAA_nonM.fasta:14817
# BS_newprots_may21.anaeromoeba_edit_StandardAA.fasta:29791
# BS_newprots_may21.anaeromoeba_edit_StandardAA_nonM.fasta:29788
# Carpediemonas_membranifera.PRJNA719540_edit_StandardAA.fasta:8272
# Carpediemonas_membranifera.PRJNA719540_edit_StandardAA_nonM.fasta:8270
# Dientamoeba_fragilis.43352.aa_edit_StandardAA.fasta:6558
# Dientamoeba_fragilis.43352.aa_edit_StandardAA_nonM.fasta:1358
# EP00701_Giardia_intestinalis_edit_StandardAA.fasta:6502
# EP00701_Giardia_intestinalis_edit_StandardAA_nonM.fasta:6501
# EP00703_Trepomonas_sp_PC1_edit_StandardAA.fasta:7980
# EP00703_Trepomonas_sp_PC1_edit_StandardAA_nonM.fasta:1293
# EP00708_Paratrimastix_pyriformis_edit_StandardAA.fasta:7108
# EP00708_Paratrimastix_pyriformis_edit_StandardAA_nonM.fasta:2017
# EP00764_Aduncisulcus_paluster_edit_StandardAA.fasta:15474
# EP00764_Aduncisulcus_paluster_edit_StandardAA_nonM.fasta:4282
# EP00766_Chilomastix_caulleryi_edit_StandardAA.fasta:6869
# EP00766_Chilomastix_caulleryi_edit_StandardAA_nonM.fasta:3267
# EP00767_Chilomastix_cuspidata_edit_StandardAA.fasta:8607
# EP00767_Chilomastix_cuspidata_edit_StandardAA_nonM.fasta:3893
# EP00768_Dysnectes_brevis_edit_StandardAA.fasta:9689
# EP00768_Dysnectes_brevis_edit_StandardAA_nonM.fasta:4521
# EP00769_Ergobibamus_cyprinoides_edit_StandardAA.fasta:4809
# EP00769_Ergobibamus_cyprinoides_edit_StandardAA_nonM.fasta:1433
# EP00770_Monocercomonoides_exilis_edit_StandardAA.fasta:16767
# EP00770_Monocercomonoides_exilis_edit_StandardAA_nonM.fasta:16589
# EP00771_Trimastix_marina_edit_StandardAA.fasta:4251
# EP00771_Trimastix_marina_edit_StandardAA_nonM.fasta:2451
# EP00792_Barthelona_sp_PAP020_edit_StandardAA.fasta:21618
# EP00792_Barthelona_sp_PAP020_edit_StandardAA_nonM.fasta:7727
# GiardiaDB_GintestinalisADH_edit_StandardAA.fasta:5147
# GiardiaDB_GintestinalisADH_edit_StandardAA_nonM.fasta:4357
# GiardiaDB_GintestinalisBGS_B_edit_StandardAA.fasta:6098
# GiardiaDB_GintestinalisBGS_B_edit_StandardAA_nonM.fasta:4904
# GiardiaDB_GintestinalisBGS_edit_StandardAA.fasta:4470
# GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM.fasta:4470
# GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM_replaceAA_.fasta:4470
# GiardiaDB_GintestinalisEP15_edit_StandardAA.fasta:5007
# GiardiaDB_GintestinalisEP15_edit_StandardAA_nonM.fasta:4980
# Giardia_intestinalis.PRJNA1439_edit_StandardAA.fasta:4965
# Giardia_intestinalis.PRJNA1439_edit_StandardAA_nonM.fasta:4963
# Giardia_muris.PRJNA524057_edit_StandardAA.fasta:4660
# Giardia_muris.PRJNA524057_edit_StandardAA_nonM.fasta:4656
# Histomonas_meleagridis.135588.aa_edit_StandardAA.fasta:6982
# Histomonas_meleagridis.135588.aa_edit_StandardAA_nonM.fasta:2399
# Histomonas_meleagridis.PRJNA594289_edit_StandardAA.fasta:22256
# Histomonas_meleagridis.PRJNA594289_edit_StandardAA_nonM.fasta:22256
# Kipferlia_bialata.PRJDB5223_edit_StandardAA.fasta:17269
# Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa.fasta:11136
# Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_fin.fasta:11136
# Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_start.fasta:11136
# Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM.fasta:11404
# Pentatrichomonas_hominis.5728.aa_edit_StandardAA.fasta:44819
# Pentatrichomonas_hominis.5728.aa_edit_StandardAA_nonM.fasta:13451
# SC_newprots_may21.anaeromoeba_edit_StandardAA.fasta:29853
# SC_newprots_may21.anaeromoeba_edit_StandardAA_nonM.fasta:29852
# Spironucleus_salmonicida.PRJNA60811_edit_StandardAA.fasta:8667
# Spironucleus_salmonicida.PRJNA60811_edit_StandardAA_nonM.fasta:8662
# Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA.fasta:67510
# Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA_nonM.fasta:15582
# Trichomonas_foetus.PRJNA345179_edit_StandardAA.fasta:25030
# Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa.fasta:25029
# Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_fin.fasta:25029
# Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_start.fasta:25029
# Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM.fasta:25030
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA.fasta:59681
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa.fasta:57671
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_fin.fasta:57671
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_start.fasta:57671
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM.fasta:57689
# Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA.fasta:59679
# Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa.fasta:57669
# Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_fin.fasta:57669
# Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_start.fasta:57669
# Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM.fasta:57687
#looks good!
#YLoc
ls *_YL.txt | while read file; do
    seq_num="$(grep -c "=== Prediction for sequence" $file)";
    echo $file ":" $seq_num;
done
# BM_newprots_may21.anaeromoeba_edit_YL.txt : 14817
# BS_newprots_may21.anaeromoeba_edit_YL.txt : 29791
# Carpediemonas_membranifera.PRJNA719540_edit_YL.txt : 8272
# Dientamoeba_fragilis.43352.aa_edit_YL.txt : 6558
# EP00701_Giardia_intestinalis_edit_YL.txt : 6502
# EP00703_Trepomonas_sp_PC1_edit_YL.txt : 7980
# EP00708_Paratrimastix_pyriformis_edit_YL.txt : 7108
# EP00764_Aduncisulcus_paluster_edit_YL.txt : 15474
# EP00766_Chilomastix_caulleryi_edit_YL.txt : 6869
# EP00767_Chilomastix_cuspidata_edit_YL.txt : 8607
# EP00768_Dysnectes_brevis_edit_YL.txt : 9689
# EP00769_Ergobibamus_cyprinoides_edit_YL.txt : 4809
# EP00770_Monocercomonoides_exilis_edit_YL.txt : 16767
# EP00771_Trimastix_marina_edit_YL.txt : 4251
# EP00792_Barthelona_sp_PAP020_edit_YL.txt : 21618
# GiardiaDB_GintestinalisADH_edit_YL.txt : 5147
# GiardiaDB_GintestinalisBGS_B_edit_YL.txt : 6098
# GiardiaDB_GintestinalisBGS_edit_YL.txt : 4470
# GiardiaDB_GintestinalisEP15_edit_YL.txt : 5007
# Giardia_intestinalis.PRJNA1439_edit_YL.txt : 4965
# Giardia_muris.PRJNA524057_edit_YL.txt : 4660
# Histomonas_meleagridis.135588.aa_edit_YL.txt : 6982
# Histomonas_meleagridis.PRJNA594289_edit_YL.txt : 22256
# Kipferlia_bialata.PRJDB5223_edit_YL.txt : 17269
# Pentatrichomonas_hominis.5728.aa_edit.1st_half_YL.txt : 22409
# Pentatrichomonas_hominis.5728.aa_edit.2nd_half_YL.txt : 22410
#total 44819
# SC_newprots_may21.anaeromoeba_edit_YL.txt : 29853
# Spironucleus_salmonicida.PRJNA60811_edit_YL.txt : 8667
# Tetratrichomonas_gallinarum.5730.aa_edit.1st_half_YL.txt : 33755
# Tetratrichomonas_gallinarum.5730.aa_edit.2nd_half_YL.txt : 33755
#total 67510
# Trichomonas_foetus.PRJNA345179_edit_YL.txt : 25030
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit.1st_half_YL.txt : 28844
# Trichomonas_vaginalis_GenBank.PRJNA16084_edit.2nd_half_YL.txt : 30837
#total 59681
# Trichomonas_vaginalis_RefSeq.G3_edit.1st_half_YL.txt : 28844
# Trichomonas_vaginalis_RefSeq.G3_edit.2nd_half_YL.txt : 30835
#total 59679
#good to go!

```


## Parsing Program Results

### EggNOG *de novo* PFam search: annotations file

Parser for the eggNOG proper results files (saved in eggNOG_dn_Parser__v2.py):
  - Should use *.annotations file, and keep pythonic indexes: 0-4, 8-9, 11-12, 14, 20
  - header/column names: '#query'[0], 'seed_ortholog'[1], 'evalue'[2], 'score'[3], 'eggNOG_OGs'[4], 'max_annot_lvl'[5], 'COG_category'[6], 'Description'[7], 'Preferred_name'[8], 'GOs'[9], 'EC'[10], 'KEGG_ko'[11], 'KEGG_Pathway'[12], 'KEGG_Module'[13], 'KEGG_Reaction'[14], 'KEGG_rclass'[15], 'BRITE'[16], 'KEGG_TC'[17], 'CAZy'[18], 'BiGG_Reaction'[19], 'PFAMs[20]'
  - Don't need to accommodate for null values, because program uses "-". But *will* have to accommodate for this when porting into the large table - will need to convert to "null". 

```python
#!/bin/python
"""
Title: eggNOG_dn_Parser__v2.py
Date: 2022-03-01
Authors: Virág Varga

Description:
	This program parses the .annotations results file produced by the eggNOG program
		when doing de novo PFam searches and creates an output text file containing
		selected categories of information for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Creating a dictionary containing the pertinent results of the eggNOG
		annotations file.
	3. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This eggNOG results parser is made specifically to suit the formatting
		of the eggNOG annotations output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.

Usage
	./eggNOG_Parser__v2.py input_file
	OR
	python eggNOG_Parser__v2.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows the easy manipulation of dataframes in Python

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit.emap.emapper.annotations"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_eggNOG.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_edit.emap.emapper_eggNOG.txt"


#Part 2: Importing the dataframe into Pandas

#read in the input text file, assigning the first row as a header row
eggNOG_df = pd.read_csv(input_file, sep='\t', header=4, index_col=(False), skipfooter=3, engine='python')
#the header is in row 5 (pythonic index 4)
#the `skipfooter=3` argument is included because the last 3 rows are informational/summary lines
#the `engine='python'` is needed because otherwise `skipfooter` raises an error
#rename the first column header to 'Query' in order to match other files
eggNOG_df.rename(columns={'#query': 'Query'}, inplace=True)


#Part 3: Filter the database to include only desired columns, then write out results file

#select the relevant columns and copy them to a new dataframe
filt_eggNOG_df = eggNOG_df[['Query', 'seed_ortholog', 'evalue', 'score', 'eggNOG_OGs', 'Preferred_name', 'GOs', 'KEGG_ko', 'KEGG_Pathway', 'KEGG_Reaction', 'PFAMs']].copy()
#write out the results to a tab-separated text file
filt_eggNOG_df.to_csv(output_file, sep='\t', index=False)

```

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../EggNOG_dn_Results/*_edit/*.annotations | while read file; do
  python ../../Scripts/eggNOG_dn_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt EggNOG_dn/
#then transfer back to the home computer
###
#A. lanta
python ../../Scripts/eggNOG_dn_Parser__v2.py ../../EggNOG_dn_Results/Anaeramoeba_lanta_160522_edit/Anaeramoeba_lanta_160522_edit.emap.emapper.annotations

```

### EggNOG *de novo* PFam search: PFAM file

Parser for the eggNOG proper results files (saved in eggNOG_dn_PFam_Parser__v2.py):
  - Groups the PFam hits, e-values, and sum scores into comma-separated strings, with one row per protein query

```python
#!/bin/python
"""
Title: eggNOG_dn_PFam_Parser__v2.py
Date: 2022-03-01
Authors: Virág Varga

Description:
	This program parses the de novo PFam search results produced by the eggNOG program
		and creates an output text file containing selected categories of information
		for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Importing the dataframe into Pandas.
	3. Filter the database to include only desired columns, correctly formatted.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This eggNOG PFam results parser is made specifically to suit the formatting
		of the eggNOG PFam de novo search output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.

Usage
	./eggNOG_dn_PFam_Parser__v2.py input_file
	OR
	python eggNOG_dn_PFam_Parser__v2.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows the easy manipulation of dataframes in Python

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit.emap.emapper.pfam"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_PFam.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_edit.emap.emapper_PFam.txt"


#Part 2: Importing the dataframe into Pandas

#read in the input text file, assigning the first row as a header row
pfam_df = pd.read_csv(input_file, sep='\t', header=4, index_col=(False), skipfooter=3, engine='python')
#the header is in row 5 (pythonic index 4)
#the `skipfooter=3` argument is included because the last 3 rows are informational/summary lines
#the `engine='python'` is needed because otherwise `skipfooter` raises an error
#rename the first column header to 'Query' in order to match other files
pfam_df.rename(columns={'# query_name': 'Query'}, inplace=True)


#Part 3: Filter the database to include only desired columns

#select the relevant columns and copy them to a new dataframe
filt_pfam_df = pfam_df[['Query', 'hit', 'evalue', 'sum_score']].copy()

#group the results into 1 row per protein
filt_pfam_df = filt_pfam_df.groupby('Query')[['hit', 'evalue', 'sum_score']].agg(list).reset_index()
#the above creates lists in the columns, so need to remove the brackets by converting to strings
filt_pfam_df['hit'] = filt_pfam_df['hit'].apply(lambda x: ', '.join(map(str, x)))
filt_pfam_df['evalue'] = filt_pfam_df['evalue'].apply(lambda x: ', '.join(map(str, x)))
filt_pfam_df['sum_score'] = filt_pfam_df['sum_score'].apply(lambda x: ', '.join(map(str, x)))


#Part 4: Write out the results to a file

#write out the results to a tab-separated text file
filt_pfam_df.to_csv(output_file, sep='\t', index=False)

```

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../EggNOG_dn_Results/*_edit/*.pfam | while read file; do
  python ../../Scripts/eggNOG_dn_PFam_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt EggNOG_dn_PFam/
#then transfer back to the home computer
###
#through some strange computational error (that I have succeeded in replicating multiple times),
#certain PFam results files have the contents of the 1st & 2nd columns switched, without the column headers changing
#below, the "correct" output for the BM Anaeramoeba, and the "incorrect" output for the BS Anaeramoeba
head */*.pfam
# ==> BM_newprots_may21.anaeromoeba_edit/BM_newprots_may21.anaeromoeba_edit.emap.emapper.pfam <==
# ## Tue Feb  8 01:30:30 2022
# ## emapper-2.1.6
# ## /resources/binp28/Programs/eggNOG/newest/emapper.py -m diamond -i /home/inf-47-2020/ThesisTrich/SortedEncodedData/Anaeramoebidae/BM_newprots_may21.anaeromoeba_edit.fasta --report_orthologs --pfam_realign denovo -o BM_newprots_may21.anaeromoeba_edit.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4
# ##
# # query_name    hit     evalue  sum_score       query_length    hmmfrom hmmto   seqfrom seqto   query_coverage
# SQZctlq3NQok8j20        Actin   6.5e-64 212.8   419     5       395     2       415     0.9856801909307876
# TpGiwxfbfGko4QQd        DUF547  2.6e-26 91.9    1270    2       124     1065    1190    0.0984251968503937
# TpGiwxfbfGko4QQd        RGS     2.5e-19 69.6    1270    2       116     852     967     0.09055118110236221
# kWunoOfjWzEoy6ev        Peptidase_M20   4.7e-35 120.7   397     1       204     72      389     0.7984886649874056
# kWunoOfjWzEoy6ev        M20_dimer       2.1e-11 42.7    397     2       104     184     296     0.28211586901763225
# ==> BS_newprots_may21.anaeromoeba_edit/BS_newprots_may21.anaeromoeba_edit.emap.emapper.pfam <==
# ## Tue Feb  8 02:53:47 2022
# ## emapper-2.1.6
# ## /resources/binp28/Programs/eggNOG/newest/emapper.py -m diamond -i /home/inf-47-2020/ThesisTrich/SortedEncodedData/Anaeramoebidae/BS_newprots_may21.anaeromoeba_edit.fasta --report_orthologs --pfam_realign denovo -o BS_newprots_may21.anaeromoeba_edit.emap --temp_dir /home/inf-47-2020/ThesisTrich/EggNOG_dn_Results/OG_comp_tmp --cpu 4
# ##
# # query_name    hit     evalue  sum_score       query_length    hmmfrom hmmto   seqfrom seqto   query_coverage
# AhpC-TSA        EFumlKAx51jHMFyo        3.3e-33 124.7   124     1       123     20      153     1.0725806451612903
# 1-cysPrx_C      EFumlKAx51jHMFyo        2.7e-05 33.7    40      1       23      174     196     0.55
# AhpC-TSA        u88P6d9AKc4fnor0        7.1e-25 95.6    124     1       123     20      142     0.9838709677419355
# 1-cysPrx_C      u88P6d9AKc4fnor0        2.8e-05 33.7    40      1       24      163     186     0.575
# AhpC-TSA        I5i0LUhpi5toMTv5        1.1e-33 126.2   124     1       123     20      153     1.0725806451612903
#it turns out 7 files have an issue where the contents of the query and hit columns are reversed
#so I'll rerun the parsing for those with a new script:
#model: 
python eggNOG_dn_PFam_Parser_bad.py input_file
#adapting it
#BS_newprots_may21.anaeromoeba_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/BS_newprots_may21.anaeromoeba_edit/BS_newprots_may21.anaeromoeba_edit.emap.emapper.pfam
#Histomonas_meleagridis.PRJNA594289_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/Histomonas_meleagridis.PRJNA594289_edit/Histomonas_meleagridis.PRJNA594289_edit.emap.emapper.pfam
#Pentatrichomonas_hominis.5728.aa_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/Pentatrichomonas_hominis.5728.aa_edit/Pentatrichomonas_hominis.5728.aa_edit.emap.emapper.pfam
#SC_newprots_may21.anaeromoeba_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/SC_newprots_may21.anaeromoeba_edit/SC_newprots_may21.anaeromoeba_edit.emap.emapper.pfam
#Tetratrichomonas_gallinarum.5730.aa_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/Tetratrichomonas_gallinarum.5730.aa_edit/Tetratrichomonas_gallinarum.5730.aa_edit.emap.emapper.pfam
#Trichomonas_vaginalis_GenBank.PRJNA16084_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/Trichomonas_vaginalis_GenBank.PRJNA16084_edit/Trichomonas_vaginalis_GenBank.PRJNA16084_edit.emap.emapper.pfam
#Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.pfam
python ../../Scripts/eggNOG_dn_PFam_Parser_bad.py ../../EggNOG_dn_Results/Trichomonas_vaginalis_RefSeq.G3_edit/Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper.pfam
#hopefully we're good now -_-
###
#A. lanta
python ../../Scripts/eggNOG_dn_PFam_Parser__v2.py ../../EggNOG_dn_Results/Anaeramoeba_lanta_160522_edit/Anaeramoeba_lanta_160522_edit.emap.emapper.pfam
#checked to make sure the file was the correct format (query IDs in column 1)

```

The python script for the .pfam files with the column issue (contents of query and PFam hit columns reversed) (script saved to eggNOG_dn_PFam_Parser_bad.py)

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""
Title: eggNOG_dn_PFam_Parser_bad.py
Date: 2022-04-15
Authors: Virág Varga

Description:
	This program parses the de novo PFam search results produced by the eggNOG program
		and creates an output text file containing selected categories of information
		for each query sequence.
	This version of the EggNOG PFam parser is specifically intended for the "bad" output
		files, wherein the contents of the query and PFam hit have been reversed.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Importing the dataframe into Pandas.
	3. Filter the database to include only desired columns, correctly formatted.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This eggNOG PFam results parser is made specifically to suit the formatting
		of the eggNOG PFam de novo search output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This script is a modified version of the eggNOG_dn_PFam_Parser__v2.py program,
		which is intended to be run on .pfam files with incorrect column contents
		(swapped content of query and PFam hit columns).

Usage
	./eggNOG_dn_PFam_Parser_bad.py input_file
	OR
	python eggNOG_dn_PFam_Parser_bad.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows the easy manipulation of dataframes in Python

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/BS_newprots_may21.anaeromoeba_edit.emap.emapper.pfam"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_PFam.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_edit.emap.emapper_PFam.txt"


#Part 2: Importing the dataframe into Pandas

#read in the input text file, assigning the first row as a header row
pfam_df = pd.read_csv(input_file, sep='\t', header=4, index_col=(False), skipfooter=3, engine='python')
#the header is in row 5 (pythonic index 4)
#the `skipfooter=3` argument is included because the last 3 rows are informational/summary lines
#the `engine='python'` is needed because otherwise `skipfooter` raises an error
#rename the first column header to 'Query' in order to match other files
pfam_df.rename(columns={'# query_name': 'hit', 'hit': 'Query'}, inplace=True)


#Part 3: Filter the database to include only desired columns

#select the relevant columns and copy them to a new dataframe
filt_pfam_df = pfam_df[['Query', 'hit', 'evalue', 'sum_score']].copy()

#group the results into 1 row per protein
filt_pfam_df = filt_pfam_df.groupby('Query')[['hit', 'evalue', 'sum_score']].agg(list).reset_index()
#the above creates lists in the columns, so need to remove the brackets by converting to strings
filt_pfam_df['hit'] = filt_pfam_df['hit'].apply(lambda x: ', '.join(map(str, x)))
filt_pfam_df['evalue'] = filt_pfam_df['evalue'].apply(lambda x: ', '.join(map(str, x)))
filt_pfam_df['sum_score'] = filt_pfam_df['sum_score'].apply(lambda x: ', '.join(map(str, x)))


#Part 4: Write out the results to a file

#write out the results to a tab-separated text file
filt_pfam_df.to_csv(output_file, sep='\t', index=False)

```

### SignalP

Parser for the SignalP results files (saved in signalP_Parser__v2.py):
  - Categories (listed in column 2 (pythonic index [1])) (as per website, here: https://services.healthtech.dtu.dk/service.php?SignalP-5.0) are:
    - Sec/SPI: "standard" secretory signal peptides transported by the Sec translocon and cleaved by Signal Peptidase I (Lep)
    - Sec/SPII: lipoprotein signal peptides transported by the Sec translocon and cleaved by Signal Peptidase II (Lsp)
    - Tat/SPI: Tat signal peptides transported by the Tat translocon and cleaved by Signal Peptidase I (Lep)
    - OTHER
  - Want to keep everything except the "other" category (since "other" is non-secreted proteins)
  - Want to keep pythonic indices: 0-2
    - where [0] is the ID
    - where [1] is the PREDICTION
    - where [2] is the likelihood that the prediction is correct

```python
#!/bin/python
"""
Title: signalP_Parser__v2.py
Date: 2022-03-01
Authors: Virág Varga

Description:
	This program parses the SignalP summary search results and creates an output
		text file containing selected categories of information	for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Creating a dictionary containing the pertinent results of the SignalP
		search file.
	3. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This SignalP results parser is made specifically to suit the formatting
		of the SignalP search output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.
	More significant alterations were made to the bulk of the program - efficency of
		use was improved by the use of pandas.

Usage
	./signalP_Parser__v2.py input_file
	OR
	python signalP_Parser__v2.py input_file

This script was written for Python 3.8.10, in Spyder 5.0.5.

"""


#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows manipulation of dataframes

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit_summary.signalp5"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_SignalP.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_SignalP.txt"


#Part 2: Import the data into a Pandas dataframe

#read in the input text file, assigning the second row as a header row
#this means using Pythonic index 1
signal_df = pd.read_csv(input_file, sep='\t', header=1)
#remove the # and space from the name of the first column, and rename it to 'Query'
#this is done to match other files
signal_df.rename(columns={'# ID': 'Query', 'SP(Sec/SPI)': 'SP_Probability'}, inplace=True)
#set the column containing the query IDs as the index
signal_df.set_index('Query')


#Part 3: Filter the Pandas dataframe

#remove query sequences without predictions
filt_signal_df = signal_df[signal_df.Prediction != "OTHER"].copy()


#Part 4: Create output with prediction probabilities

#create a new dataframe containing only the desired columns
pred_signal_df = filt_signal_df[['Query', 'Prediction', 'SP_Probability']].copy()
#write out the results to a tab-separated file
pred_signal_df.to_csv(output_file, sep='\t', index=False)

```

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../SignalP_Results/*.signalp5 | while read file; do
  python ../../Scripts/signalP_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt SignalP/
#then transfer back to the home computer
###
#A. lanta integration
python ../../Scripts/signalP_Parser__v2.py ../../SignalP_Results/Anaeramoeba_lanta_160522_edit_summary.signalp5

```

### DeepLoc

Parser for the DeepLoc results files (saved in deepLoc_Parser__v2.py):
  - Want "Extracellular" in column 2 (pythonic index [1])
    - "Cell_membrane" might also be interesting (% possibility in [7])
  - Also want contents of column 6 (pythonic index [5]) which contains the probability of the target being extracellular

```python
#!/bin/python
"""
Title: deepLoc_Parser__v2.py
Date: 2022-03-02
Authors: Virág Varga

Description:
	This program parses the DeepLoc search results and creates an output
		text file containing selected categories of information	for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas
	numpy

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Importing the data into a Pandas dataframe.
	3. Writing out a version of the dataframe containing only the query IDs, the
		predicted localization, and the probability of said localization.

Known bugs and limitations:
	- This DeepLoc results parser is made specifically to suit the formatting
		of the DeepLoc search output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.
	More significant alterations were made to the bulk of the program - efficency of
		use was improved by the use of pandas. In addition, more columns were extracted
		for the more significant breadth of the Thesis project vs. the preliminary
		exploratory project.

Usage
	./deepLoc_Parser__v2.py input_file
	OR
	python deepLoc_Parser__v2.py input_file

This script was written for Python 3.8.10, in Spyder 5.0.5.

"""


#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows manipulation of dataframes
import numpy as np #allows numerical manipulations, empty dataframe columns

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_DL.txt"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_DeepLocP.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_DeepLocP.txt"


#Part 2: Import the data into a Pandas dataframe

#read in the input text file, assigning the first row as a header row
#this means using Pythonic index 0
deeploc_df = pd.read_csv(input_file, sep='\t', header=0)
#rename the first column to 'Query' to match the other files
deeploc_df.rename(columns={'ID': 'Query'}, inplace=True)
#set the column containing the query IDs as the index
#deeploc_df.set_index('Query')

#create a list of columns
pred_list = list(deeploc_df)
#filter the list down to only the prediction category headers
pred_list = pred_list[2:]


#Part 3: Create output with prediction probabilities

#create an empty column at the end filled with NaN
deeploc_df['Probability'] = np.nan

for pred in pred_list:
	#iterate over the list of possible prediction locations
	deeploc_df.loc[deeploc_df['Location']==pred, 'Probability'] = deeploc_df[pred]
	#when the predicted location of the protein query matches a protein location in the list
	#copy the prediction probability for that location from the appropriate column
	#into the Probability column at the end

#create a new dataframe containing only the desired columns
final_deeploc_df = deeploc_df[['Query', 'Location', 'Probability']].copy()
#write out the results to a tab-separated file
final_deeploc_df.to_csv(output_file, sep='\t', index=False)

```

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../DeepLoc_Results/*_DL.txt | while read file; do
  python ../../Scripts/deepLoc_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt DeepLoc/
#then transfer back to the home computer
###
#A. lanta integration
python ../../Scripts/deepLoc_Parser__v2.py ../../DeepLoc_Results/Anaeramoeba_lanta_160522_DL.txt

```

### TargetP

Parser for the TargetP results files (saved in targetP_Parser__v2.py):
  - Categories (listed in column 2 (pythonic index [1])) (as per website, here: https://services.healthtech.dtu.dk/service.php?TargetP-2.0) are:
    - "SP" for signal peptide
    - "MT" for mitochondrial transit peptide (mTP)
    - "CH" for chloroplast transit peptide (cTP)
    - "TH" for thylakoidal lumen composite transit peptide (lTP)
    - "Other" for no targeting peptide (in this case, the length is given as 0) - this category appears in the results file with the label "noTP"
  - Only keeping the mTP and SP results, which requires some reorganization of the data table into a format containing only the Query IDs, the Prediction (ie. mTP or SP), and the Probability of that prediction

```python
#!/bin/python
"""
Title: targetP_Parser__v2.py
Date: 2022-03-01
Authors: Virág Varga

Description:
	This program parses the TargetP summary search results and creates an output
		text file containing selected categories of information	for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas
	numpy

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Importing data into a Pandas dataframe.
	3. Filtering the Pandas dataframe to exclude queries with no predicted target.
	4. Creating a new dataframe with the desired data columns, and writing out
		the results to a tab-separated text file.

Known bugs and limitations:
	- This TargetP results parser is made specifically to suit the formatting
		of the TargetP search output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.

Usage
	./targetP_Parser__v2.py input_file
	OR
	python targetP_Parser__v2.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows manipulation of dataframes
import numpy as np #allows numerical manipulations, empty dataframe columns

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit_summary.targetp2"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_TargetP.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_TargetP.txt"


#Part 2: Import the data into a Pandas dataframe

#read in the input tsv file, assigning the first row as a header row
target_df = pd.read_csv(input_file, sep='\t', header=1)
#remove the # and space from the name of the first column, and rename it to 'Query'
#this is done to match other files
target_df.rename(columns={'# ID': 'Query'}, inplace=True)
#set the column containing the query IDs as the index
target_df.set_index('Query')


#Part 3: Filter the Pandas dataframe

#remove query sequences without predictions
filt_target_df = target_df[target_df.Prediction != "noTP"].copy()
#create an empty column at the end filled with NaN
filt_target_df['Probability'] = np.nan


#Part 4: Create output with prediction probabilities

#copy signal peptide probabilities to 'Probability' column
filt_target_df.loc[filt_target_df['Prediction']=='SP', 'Probability'] = filt_target_df['SP']
#copy mitochondrial transit peptide probabilities to 'Probability' column
filt_target_df.loc[filt_target_df['Prediction']=='mTP', 'Probability'] = filt_target_df['mTP']
#create a new dataframe containing only the desired columns
pred_target_df = filt_target_df[['Query', 'Prediction', 'Probability']].copy()
#write out the results to a tab-separated file
pred_target_df.to_csv(output_file, sep='\t', index=False)

```

Running it:

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../TargetP_Results/*.targetp2 | while read file; do
  python ../../Scripts/targetP_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt TargetP/
#then transfer back to the home computer
###
#A. lanta integration
python ../../Scripts/targetP_Parser__v2.py ../../TargetP_Results/Anaeramoeba_lanta_160522_edit_summary.targetp2

```

### YLoc

Parser for the YLoc results files (saved in yLoc_Parser__v2.py):
  - Formatting of the output files more resembles BLAST than any of these other parsers, in that the results are not simply in one large data table. Instead, the results for each query are in their own section, so extraction of pertinent data will be a bit more complex.
  - Want to keep pythonic indices: 0-2
    - where [0] is the Query
    - where [1] is the positive mitochondrial prediction
    - where [2] is the likelihood that the prediction is correct

```python
#!/bin/python
"""
Title: yLoc_Parser__v2.py
Date: 2022-03-01
Authors: Virág Varga

Description:
	This program parses the YLoc+* Animals search results and creates an output
		text file containing selected categories of information	for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	re

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Preparing necessary elements for character removal using regex.
	3. Parsing through the file to extract desired information.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This YLoc results parser is made specifically to suit the formatting
		of the YLoc+* Animals search output files. It has not been tested on the
		results files of other YLoc models.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.

Usage
	./yLoc_Parser__v2.py input_file
	OR
	python yLoc_Parser__v2.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import re #enables regex pattern matching


#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit_YL.txt"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_YLparsed.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_edit_YL_YLparsed.txt"


#Part 2: Prepare necessary elements for character removal using regex

#create list of characters to remove with regex
characters_to_remove = "() =%"
#use string concatenation of add the [] characters so that the pattern isn't read in order,
#but instead all of these characters are individually removed
pattern = "[" + characters_to_remove + "]"


#Part 3: Parse through the file to extract desired information
#Part 4: Write out the results to the output file

with open(input_file, "r") as infile, open(output_file, "w") as outfile:
	#open the input YLoc result file for reading
	#open the output file for writing
	outfile.write("Query" + "\t" + "Prediction" + "\t" + "Probability" + "\n")
	#the header line is prepared and written out to the output file
	for line in infile:
		#read through the file line by line
		line = line.strip()
		#remove the '\n' endline character from the line
		seq_list = [None] * 2
		#empty list seq_list will hold the alignment data associated with each query-target match
		#with each iteration of the loop, this list is overwritten
		if line.startswith('=== Prediction for sequence:'):
			#identify lines that start with "=== Prediction for sequence:"
			#this is the first line with information on the next query
			query_id_line = line.split('\t')
			#the line with the query id is 'split' - separated into a list based on the locations of spaces
			query_id = query_id_line[1]
			#save the second element of the list, which contains the actual query id
			query_id = re.sub(pattern, "", query_id)
			#remove the unnecessary characters and save the query id to variable query_id
			next_line = next(infile).strip()
			#save the next line, where the query protein prediction will be, without endline character
			query_results = next_line.split('\t')
			#the line with the query id is 'split' - separated into a list based on the locations of tabs
			query_results = query_results[1]
			#the item with index 1 in the query_results list is the query sequence prediction
			#so we save only that
			query_results = query_results.split("(")
			#split the results-containing string into the result and its probability
			query_results[1] = re.sub(pattern, "", query_results[1])
			#remove the unnecessary characters from the probability field
			query_results[1] = float(query_results[1])/100
			#turn the probability percentage into a float
			outfile.write('{}\t{}\t{}\n'.format(query_id, query_results[0], query_results[1]))
			#write out the results to the output file

```

Running it:

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../YLoc_Results/*_YL.txt | while read file; do
  python ../../Scripts/yLoc_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt YLoc/
#then transfer back to the home computer
###
#A. lanta integration
python ../../Scripts/yLoc_Parser__v2.py ../../YLoc_Results/Anaeramoeba_lanta_160522_edit_YL.txt

```

### MitoFates

Parser for the MitoFates results files (saved in mitoFates_Parser__v2.py):
  - Want to keep only the queries that are predicted to be targeted to the mitochondrion.
  - Want to keep pythonic indices: 0-2
    - where [0] is the Query
    - where [1] is the positive mitochondrial prediction
    - where [2] is the likelihood that the prediction is correct

```python
#!/bin/python
"""
Title: mitoFates_Parser__v2.py
Date: 2022-03-01
Authors: Virág Varga

Description:
	This program parses the MitoFates summary search results and creates an output
		text file containing selected categories of information	for each query sequence.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Assigning command line arguments and output file name; loading modules.
	2. Importing the data into a Pandas dataframe.
	3. Filtering the dataframe to only include the desired columns of rows with
		proteins predicted to be targeted to the mitochonrion.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This MitoFates results parser is made specifically to suit the formatting
		of the MitoFates search output files.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Version:
	This is version 2.0 of this program. Slight alterations were made to the naming
		convention used, in order to keep the entire basename (minus the file extension),
		to eliminate issues related to files from the same species being overwritten
		when the program was used in a loop.

Usage
	./mitoFates_Parser__v2.py input_file
	OR
	python MitoFates_Parser__v2.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows the easy manipulation of dataframes in Python

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit_StandardAA_nonM_MFresults.txt"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_MFparsed.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_edit_StandardAA_nonM_MFresults_MFparsed.txt"


#Part 2: Importing the dataframe into Pandas

#read in the input text file, assigning the first row as a header row
mito_df = pd.read_csv(input_file, sep='\t', header=0, index_col=(False))
#replace spaces (' ') in column headers with underscores ('_')
mito_df.columns = mito_df.columns.str.replace(' ', '_')
#rename the first column header to 'Query' in order to match other files
mito_df.rename(columns={'Sequence_ID': 'Query'}, inplace=True)


#Part 3: Filter the database to include only desired rows and columns, reformatted

#filter out the results without a mitochondrial presequence
filt_mito_df = mito_df[mito_df.Prediction != "No mitochondrial presequence"].copy()
#remove all but first 3 columns
filt_mito_df  = filt_mito_df.iloc[: , :3]
#reorder columns
filt_mito_df = filt_mito_df[['Query', 'Prediction', 'Probability_of_presequence']]


#Part 4: Write out the results to a tab-separated file

#write out the results to a tab-separated text file
filt_mito_df.to_csv(output_file, sep='\t', index=False)

```

Running it:

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../MitoFates_Results/*_MFresults.txt | while read file; do
  python ../../Scripts/mitoFates_Parser__v2.py $file;
done
#then move all of these into a new directory
mv *.txt MitoFates/
#then transfer back to the home computer
###
#A. lanta integration
python ../../Scripts/mitoFates_Parser__v2.py ../../MitoFates_Results/Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_fin_MFresults.txt

```

### InterProScan

Parsing the .tsv results file output by the InterProScan program, when it is run with the following options: `--goterms --iprlookup --pathways`. Script saved to: iprScan_Parser.py

```python
#!/bin/python
"""
Title: iprScan_Parser.py
Date: 2022-03-04
Authors: Virág Varga

Description:
	This program parses the .tsv results file produced by the InterProScan program
		when it has been run with the command line options of
			`--goterms --iprlookup --pathways`
		and creates an output tab-separated text file containing selected categories
		of information for each query sequence.

List of functions:
	drop_nan_groupby
		(Based on: https://stackoverflow.com/questions/47376077/how-to-drop-nan-elements-in-a-groupby-on-a-pandas-dataframe)

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Assigning command line arguments and output file name, loading modules,
		defining function used in script.
	2. Importing the dataframe into Pandas.
	3. Filtering the database to include only desired columns, and removing rows
		containing unique queries.
	4. Grouping the contents of the table based on the protein query IDs, then
		performing necessary edits to clean up the formatting of cell contents
		in the dataframe.
	5. Adding the unique query rows back in, and writing out the results to a
		tab-separated text file.

Known bugs and limitations:
	- This InterProScan results parser is made specifically to suit the formatting
		of the InterProScan tsv-format output files, for runs with the command-line
		options of:
			`--goterms --iprlookup --pathways`
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.

Usage
	./iprScan_Parser.py input_file
	OR
	python iprScan_Parser.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#Part 1: Setup

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #allows the easy manipulation of dataframes in Python

#function to drop null values during grouping
#based on: https://stackoverflow.com/questions/47376077/how-to-drop-nan-elements-in-a-groupby-on-a-pandas-dataframe
def drop_nan_groupby(x):
	#drop the null (nan) values from the input
    y = x.dropna()
	#if the cells are empty return "-", otherwise return y
    return ["-"] if y.empty else y

#assign command line argument
input_file = sys.argv[1]
#input_file = "ParserTestData/EP00771_Trimastix_marina_edit_StandardAA.fasta.tsv"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
#remove the ".fasta" at the end of the file name - this gives a list
out_list = out_full.split(".")[:-1]
#concatenate the list back into a string
outname = '.'.join(out_list)
output_file = outname + "_IPRScan.txt"
#output_file = "ParserTestData/EP00771_Trimastix_marina_edit_StandardAA.fasta.txt"


#Part 2: Importing the dataframe into Pandas

#from the documentation, can find the column meanings
#ref: https://interproscan-docs.readthedocs.io/en/latest/OutputFormats.html
#replacing 'Protein accession' with 'Query' to match other files
ipr_columns = ['Query', 'Sequence_MD5_digest', 'Sequence_length', 'Analysis-Pfam-PRINTS-Gene3D',
			   'Signature_accession', 'Signature_description', 'Start_location', 'Stop_location',
			   'Score', 'Status_of_match', 'Run_date', 'InterPro_annotations-accession',
			   'InterPro_annotations-description', 'GO_annotations', 'Pathways_annotations']

#read in the input text file, assigning the first row as a header row
iprScan_df = pd.read_csv(input_file, sep='\t', names=ipr_columns, index_col=(False))
#there is no header, which means column "names" will be numeric indices
#instead of the usual `header=None`, set the column names immediately
#this helped prevent a pandas error I was having where pandas was expecting 13 columns instead of 15
#the error in question:
# pandas.errors.ParserError: Error tokenizing data. C error: Expected 13 fields in line 2, saw 15
#ref: https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data/54098017#54098017


#Part 3: Filter the database to include only desired columns

#select the relevant columns and copy them to a new dataframe
filt_iprScan_df = iprScan_df[['Query', 'Signature_accession', 'Signature_description', 'Score',
							  'InterPro_annotations-accession', 'InterPro_annotations-description',
							  'GO_annotations', 'Pathways_annotations']].copy()

#protein queries that occur only once in the dataframe don't do well with the .agg(list)
#so extract them into a small dataframe now, prior to grouping
#and add them back in at the end
non_duplicate_df = filt_iprScan_df.drop_duplicates(subset='Query', keep=False).copy()
#set the 'Query' column as index to match the grouped database that will be built after this
non_duplicate_df.set_index('Query', inplace=True)

#then drop those unique IDs from the primary filtered database
filt_iprScan_df = filt_iprScan_df[filt_iprScan_df.duplicated(subset=['Query'], keep=False)]


#Part 4: Group the contents of the table based on the protein query IDs, then perform necessary edits

#group the results into 1 row per protein
filt_iprScan_df = filt_iprScan_df.groupby('Query', dropna=True)[['Signature_accession', 'Signature_description',
																 'Score', 'InterPro_annotations-accession',
																 'InterPro_annotations-description', 'GO_annotations',
																 'Pathways_annotations']].agg(drop_nan_groupby).agg(list)
#the drop_nan_groupby function drops nan cells and inserts "-" characters into empty cells
#the .agg(list) groups the contents of the columns into lists on the basis of the query ID
#groupby automatically sets the Query columns as a new index

#convert lists currently in columns into strings
for column in filt_iprScan_df:
	#iterate through the dataframe row by row
	#join all lists into comma- and space-separated strings
	filt_iprScan_df[column] = filt_iprScan_df[column].apply(lambda x: ', '.join(map(str, x)))

#this process leaves a lot of extraneous, useless characters, so now need to remove those
for column in filt_iprScan_df:
	#iterate through the dataframe row by row
	#in the following rows, replace unnecessary characters, including blanks in the form of "-" characters
	filt_iprScan_df[column] = filt_iprScan_df[column].str.replace(" -,", "")
	filt_iprScan_df[column] = filt_iprScan_df[column].str.replace(", -", "")
	filt_iprScan_df[column] = filt_iprScan_df[column].str.replace("\|", ", ", regex=True)
	# FutureWarning: The default value of regex will change from True to False in a future version.
	# In addition, single character regular expressions will *not* be treated as literal strings when regex=True.
	# filt_iprScan_df[column] = filt_iprScan_df[column].str.replace("|", ", ")
	#use backslash "\" to break out of special character, and allow regex=True to deal with the error above
	filt_iprScan_df[column] = filt_iprScan_df[column].str.replace("-, ", "")


#Part 5: Recombine the dataframes and write out results file

#concatenating the two dataframes
filt_iprScan_df = pd.concat([filt_iprScan_df, non_duplicate_df], axis=0)
#use `axis=0` to concatenate based on index - this adds one dataframe below the other

#write out the results to a tab-separated text file
filt_iprScan_df.to_csv(output_file, sep='\t', index=True)
#since the query IDs are now in the index, need to write out the index, too

```

```bash
#running the program
#in the /home/inf-47-2020/ThesisTrich/PARSED_Results/ directory
ls ../../InterProScan_Results/IPRScan_Results/IPRScan_Results_PROSITE/*.tsv | while read file; do
  python ../../Scripts/iprScan_Parser.py $file;
done
#then move all of these into a new directory
mv *.txt IPRScan/
#then transfer back to the home computer
###
#A. lanta
python ../../Scripts/iprScan_Parser.py ../../InterProScan_Results/IPRScan_Results_PROSITE/Anaeramoeba_lanta_160522_edit_StandardAA.fasta.tsv

```

### OrthoFinder

Parsing the primary results file of OrthoFinder, Orthogroups.tsv. Script saved to: orthoFinder_Parser.py

```python
#!/bin/python
"""

Title: orthoFinder_Parser.py
Date: 2022-02-11
Author: Virág Varga

Description:
	This program pivots the Orthogroups.tsv file's data, creating an output file that
		contains query proteins in individual columns and the OG associated with them
		in a cell in the same row.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	itertools

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the Orthogroups.tsv file into a
		dataframe.
	3. Condensing the contents of all species columns into 1 large column, then
		separating out the protein ids into unique cells (instead of comma-separated
		strings in each cell), and then flipping the column order so that the query
		protein IDs are in the first column.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of the OrthoFinder results file, Orthogroups.tsv;
		or a file with identical formatting.

Version:
	This script can be considered a "Version 2" of the pivot_OGs.py script, with minor
		changes made due to a better understanding of the sorts of output files needed.

Usage
	./orthoFinder_Parser.py input_db output_db
	OR
	python orthoFinder_Parser.py input_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
from  itertools import product


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Orthogroups.tsv"
output_db = sys.argv[2]
#output_db = "OF_OGs_parsed.txt"


#import input database into a Pandas dataframe
ortho_df = pd.read_csv(input_db, sep = '\t', header = 0)
#rename the first column header for ease of manipulation
ortho_df.rename(columns={'Orthogroup': 'OrthoFinder_OG'}, inplace=True)


#first, condense the contents of the species columns,
#since the species information is unnecessary (already in large database)
ortho_df = ortho_df.melt(id_vars="OrthoFinder_OG", var_name="Species", value_name="Query")
#melting the database like this creates 3 columns


#pull apart the parts of the database that are comma-separated
#ie. flatten the database: each protein gets its own cell
#ref: https://stackoverflow.com/questions/50789834/parse-a-dataframe-column-by-comma-and-pivot-python
df1 = ortho_df.applymap(lambda x: x.split(', ') if isinstance (x, str) else [x])
#split apart the comma-separated lists of protein ids in each cell
df2 = pd.DataFrame([j for i in df1.values for j in product(*i)], columns=ortho_df.columns)
#give each protein ID its own cell, while still associated with the species ID and OG


#flip the column order, putting the proteins in the first column
df3 = df2.iloc[:, ::-1]

#remove rows where ""Query"" column contains a nan as the null indicator
final_df = df3[df3['Query'].notna()]


#now write out the results to a tab-separated text file
final_df.to_csv(output_db, sep='\t', index=False)

```

Using it:

```bash
python orthoFinder_Parser.py Orthogroups.tsv OF_OGs_parsed.txt
###
#A. lanta run
python /home/inf-47-2020/ThesisTrich/Scripts/orthoFinder_Parser.py /home/inf-47-2020/ThesisTrich/OrthoFinder_Results/Results_May16/Orthogroups/Orthogroups.tsv OF_OGs_parsed_Alanta.txt

```

### Broccoli

Parsing the primary results file of OrthoFinder, orthologous_groups.txt. Script saved to: broccoli_Parser.py

```python
#!/bin/python
"""

Title: broccoli_Parser.py
Date: 2022-02-11
Author: Virág Varga

Description:
	This program pivots the orthologous_groups.txt file, creating an output file that
		contains query proteins in individual columns and the OG associated with them
		in a cell in the same row.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	itertools

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the orthologou_groups.txt file into a
		dataframe.
	3. Reorganizing the dataframe so that each protein query is in its own row.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of the Broccoli results file, orthologous_groups.txt;
		or a file with identical formatting.

Usage
	./broccoli_Parser.py input_db output_db
	OR
	python broccoli_Parser.py input_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
from  itertools import product


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "orthologous_groups.txt"
output_db = sys.argv[2]
#output_db = "Broccoli_OGs_parsed.txt"


#import input database into a Pandas dataframe
ortho_df = pd.read_csv(input_db, sep = '\t', header = 0)
#rename the first column header for ease of manipulation in Python
ortho_df.rename(columns={'#OG_name': 'Broccoli_OG', 'protein_names': 'Query'}, inplace=True)


#pull apart the parts of the database that are space-separated
#ie. flatten the database: each protein gets its own cell
#ref: https://stackoverflow.com/questions/50789834/parse-a-dataframe-column-by-comma-and-pivot-python
df1 = ortho_df.applymap(lambda x: x.split(' ') if isinstance (x, str) else [x])
#split apart the comma-separated lists of protein ids in each cell
df2 = pd.DataFrame([j for i in df1.values for j in product(*i)], columns=ortho_df.columns)
#give each protein ID its own cell, while still associated with the species ID and OG


#flip the column order, putting the proteins in the first column
final_df = df2.iloc[:, ::-1]


#now write out the results to a tab-separated text file
final_df.to_csv(output_db, sep='\t', index=False)

```

Using it:

```bash
python broccoli_Parser.py orthologous_groups.txt Broccoli_OGs_parsed.txt
###
#A. lanta integration
python ../../../Scripts/broccoli_Parser.py ../../../Broccoli_Results/AlantaRun/dir_step3/orthologous_groups.txt Broccoli_OGs_parsed_Alanta.txt

```

### SonicParanoid

Parsing the primary results file of SonicParanoid, ortholog_groups.tsv. Script saved to: sonicParanoid_Parser.py

```python
#!/bin/python
"""

Title: sonicParanoid_Parser.py
Date: 2022-02-11
Author: Virág Varga

Description:
	This program pivots the ortholog_groups.tsv file, creating an output file that
		contains query proteins in individual columns and the OG associated with them
		in a cell in the same row.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	itertools

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the ortholog_groups.tsv file into a
		dataframe.
	3. Condensing the contents of all species columns into 1 large column, then
		separating out the protein ids into unique cells (instead of comma-separated
		strings in each cell), and then flip the column order so that the query
		protein IDs are in the first column.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of the SonicParanoid results file, ortholog_groups.tsv;
		or a file with identical formatting.

Usage
	./sonicParanoid_Parser.py input_db output_db
	OR
	python sonicParanoid_Parser.py input_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
from  itertools import product


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "ortholog_groups.tsv"
output_db = sys.argv[2]
#output_db = "SP_OGs_parsed.txt"


#import input database into a Pandas dataframe
ortho_df = pd.read_csv(input_db, sep = '\t', header = 0)
#filter the dataframe by removing unnecessary columns
ortho_df.drop(['group_size', 'sp_in_grp', 'seed_ortholog_cnt'], axis=1, inplace=True)
#rename the first column header for ease of manipulation
ortho_df.rename(columns={'group_id': 'SonicParanoid_OG'}, inplace=True)


#first, condense the contents of the species columns,
#since the species information is unnecessary (already in large database)
ortho_df = ortho_df.melt(id_vars="SonicParanoid_OG", var_name="Species", value_name="Query")
#melting the database like this creates 3 columns
#'Orthogroup' stays as is; the species headers go in column 2 'Species';
#and proteins go in column 3 'Query'


#pull apart the parts of the database that are comma-separated
#ie. flatten the database: each protein gets its own cell
#ref: https://stackoverflow.com/questions/50789834/parse-a-dataframe-column-by-comma-and-pivot-python
df1 = ortho_df.applymap(lambda x: x.split(',') if isinstance (x, str) else [x])
#split apart the comma-separated lists of protein ids in each cell
df2 = pd.DataFrame([j for i in df1.values for j in product(*i)], columns=ortho_df.columns)
#give each protein ID its own cell, while still associated with the species ID and OG

#flip the column order, putting the proteins in the first column
final_df = df2.iloc[:, ::-1]


#now write out the results to a tab-separated text file
final_df.to_csv(output_db, sep='\t', index=False)

```

Using it:

```bash
python sonicParanoid_Parser.py ortholog_groups.tsv SP_OGs_parsed.txt
###
#A. lanta integration
python /home/inf-47-2020/ThesisTrich/Scripts/sonicParanoid_Parser.py /home/inf-47-2020/ThesisTrich/SonicParanoid_Results/A_lanta_Run/runs/Trich_Thesis3/ortholog_groups/ortholog_groups.tsv SP_OGs_parsed_Alanta.txt

```

### ProteinOrtho

Parsing the primary results file of ProteinOrtho, {project_name}.proteinortho.tsv. Script saved to: proteinOrtho_Parser.py

```python
#!/bin/python
"""

Title: proteinOrtho_Parser.py
Date: 2022-02-11
Author: Virág Varga

Description:
	This program pivots the {project_name}.proteinortho.tsv file, creating an output
		file thatcontains query proteins in individual columns and the OG associated
		with them in a cell in the same row.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	itertools

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the ortholog_groups.tsv file into a
		dataframe.
	3. Condensing the contents of all species columns into 1 large column, then
		separating out the protein ids into unique cells (instead of comma-separated
		strings in each cell), and then flip the column order so that the query
		protein IDs are in the first column.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of the ProteinOrtho results file,
		{project_name}.proteinortho.tsv; or a file with identical formatting.

Usage
	./proteinOrtho_Parser.py input_db output_db
	OR
	python proteinOrtho_Parser.py input_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
from  itertools import product


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "trichoCompare.proteinortho.tsv"
output_db = sys.argv[2]
#output_db = "PO_OGs_parsed.txt"


#import input database into a Pandas dataframe
ortho_df = pd.read_csv(input_db, sep = '\t', header = 0)
#filter the dataframe by removing unnecessary columns
ortho_df.drop(['# Species', 'Genes', 'Alg.-Conn.'], axis=1, inplace=True)
#ProteinOrtho doesn't assign OG names, so we need to make a column to do that
#get number of rows in dataframe
df_length = len(ortho_df.index)
#create list of assigned OG "names"
assigned_OGs = [f'OG_{i}' for i in range(0, df_length)]
#insert the new column at the start of the row
ortho_df.insert(0, 'ProteinOrtho_OG', assigned_OGs)


#first, condense the contents of the species columns,
#since the species information is unnecessary (already in large database)
ortho_df = ortho_df.melt(id_vars="ProteinOrtho_OG", var_name="Species", value_name="Query")
#melting the database like this creates 3 columns
#'Orthogroup' stays as is; the species headers go in column 2 'Species';
#and proteins go in column 3 'Query'


#pull apart the parts of the database that are comma-separated
#ie. flatten the database: each protein gets its own cell
#ref: https://stackoverflow.com/questions/50789834/parse-a-dataframe-column-by-comma-and-pivot-python
df1 = ortho_df.applymap(lambda x: x.split(',') if isinstance (x, str) else [x])
#split apart the comma-separated lists of protein ids in each cell
df2 = pd.DataFrame([j for i in df1.values for j in product(*i)], columns=ortho_df.columns)
#give each protein ID its own cell, while still associated with the species ID and OG

#flip the column order, putting the proteins in the first column
df3 = df2.iloc[:, ::-1]

#remove rows where ""Query"" column contains an asterisk ("*") as the null indicator
final_df = df3[~df3.Query.str.contains("\*")]


#now write out the results to a tab-separated text file
final_df.to_csv(output_db, sep='\t', index=False)

```

Using it:

```bash
python proteinOrtho_Parser.py trichoCompare.proteinortho.tsv PO_OGs_parsed.txt
###
#A. lanta run
python ../../../Scripts/proteinOrtho_Parser.py ../../../ProteinOrtho_Results/Alanta_Run/trichAlanta.proteinortho.tsv PO_OGs_parsed_Alanta.txt

```

### Combined protein parser

This program is a compilation of the parsers above, for ease of use (saved to predictionParser__v2.py).

```python
#!/bin/python
"""
Title: predictionParser__v2.py
Date: 2022-03-05
Author: Virág Varga

Description:
	This program takes an input results file from a protein localization prediction software,
		and performs pre-determined data restructuring and extraction processes on the file.
		The resulting files can be combined into a large protein database.
	The prediction software whose results files can be used as input are:
		- DeepLoc
		- SignalP
		- TargetP
		- EggNOG
		- PFam via EggNOG
		- YLoc
		- MitoFates
		- InterProScan (with options: `--goterms --iprlookup --pathways`)

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	argparse
	os
	deepLoc_Parser__v2.py
	signalP_Parser__v2.py
	targetP_Parser__v2.py
	eggNOG_dn_Parser__v2.py
	eggNOG_dn_PFam_Parser__v2.py
	mitoFates_Parser__v2.py
	yLoc_Parser__v2.py
	iprScan_Parser.py

Procedure:
	1. Assignment of command-line arguments.
	2. The called argument is executed to parse the input file.
	3. Outputs from functions are written out to a results file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The program cannot accept multiple input files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Version:
	This is version 2.0 of this program. Modifications were made to accomadate new versions
		of the imported module scripts, and a new command line argument was added for the
		InterProScan results parser.

Usage:
	./predictionParser__v2.py [-h] [-dl] [-sp] [-tp] [-en] [-pfen] [-mf] [-yl] [-ipr] [-v] input_file
	OR
	python predictionParser__v2.py [-h] [-dl] [-sp] [-tp] [-en] [-pfen] [-mf] [-yl] [-ipr] [-v] input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""

#################################   ARGPARSE   #######################################
import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


parser = argparse.ArgumentParser(description =
								 'This program takes an input results file from a protein localization prediction software,\
								 and performs pre-determined data restructuring and extraction processes on the file. \
								 The resulting files can be combined into a large protein database. \
								 The prediction software whose results files can be used as input are: \
								 DeepLoc, SignalP, TargetP, eggNOG, PFam via eggNOG, MitoFates & YLoc.')
#The most general description of what this program can do is defined here


#adding the arguments that the program can use
parser.add_argument(
	'-dl', '--DeepLoc',
	action='store_true',
	help = 'This argument will parse the results of the DeepLoc program.'
	)
	#the '-dl' flag will call the deepLoc_Parser__v2.py program to parse the input file
parser.add_argument(
	'-sp', '--SignalP',
	action='store_true',
	help = 'This argument will parse the results of the SignalP program.'
	)
	#the '-sp' flag will call the signalP_Parser__v2.py program to parse the input file
parser.add_argument(
	'-tp', '--TargetP',
	action='store_true',
	help = 'This argument will parse the results of the TargetP program.'
	)
	#the '-tp' flag will call the targetP_Parser__v2.py program to parse the input file
parser.add_argument(
	'-en', '--EggNOG',
	action='store_true',
	help = 'This argument will parse the results of the eggNOG program.'
	)
	#the '-en' flag will call the eggNOG_dn_Parser__v2.py program to parse the input file
parser.add_argument(
	'-pfen', '--PFamEggNOG',
	action='store_true',
	help = 'This argument will parse the results of a PFam matching run of the eggNOG program.'
	)
	#the '-pfen' flag will call the eggNOG_dn_PFam_Parser__v2.py program to parse the input file
parser.add_argument(
	'-mf', '--MitoFates',
	action='store_true',
	help = 'This argument will parse the results of the MitoFates program.'
	)
	#the '-mf' flag will call the mitoFates_Parser__v2.py program to parse the input file
parser.add_argument(
	'-yl', '--YLoc',
	action='store_true',
	help = 'This argument will parse the results of the YLoc program.'
	)
	#the '-yl' flag will call the yLoc_Parser__v2.py program to parse the input file
parser.add_argument(
	'-ipr', '--InterProScan',
	action='store_true',
	help = 'This argument will parse the results of the InterProScan program, assuming the following options were used: `--goterms --iprlookup --pathways`.'
	)
	#the '-ipr' flag will call the iprScan_Parser.py program to parse the input file
parser.add_argument(
	#'-i', '--input',
	#the above line of code is left in as further clarification of this argument
	dest='input_file',
	metavar='INPUT_FILE',
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 1.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Main Program   ######################################


#import necessary module
import os #allows access to the operating system


#designate input file name as variable
infile = args.input_file.name


#parse arguments
if args.DeepLoc:
	#if -dl argument is called
	cmd = "python deepLoc_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the deepLoc_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.SignalP:
	#if -sp argument is called
	cmd = "python signalP_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the signalP_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.TargetP:
	#if -tp argument is called
	cmd = "python targetP_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the targetP_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.EggNOG:
	#if -en argument is called
	cmd = "python eggNOG_dn_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the eggNOG_dn_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.PFamEggNOG:
	#if -pfen argument is called
	cmd = "python eggNOG_dn_PFam_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the eggNOG_dn_PFam_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.MitoFates:
	#if -mf argument is called
	cmd = "python mitoFates_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the mitoFates_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.YLoc:
	#if -yl argument is called
	cmd = "python yLoc_Parser__v2.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the yLoc_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

if args.InterProScan:
	#if -ipr argument is called
	cmd = "python inprScan_Parser.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the yLoc_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)

```


## Database Construction

### Setting up the Species databases

#### Setting up the directories

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
ls /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta > FileNames.txt
#attempting a for loop
for line in FileNames.txt; do
	line2=${line::-11};
	#line3=${line2:52};
	echo $line2; #>> SpeciesNames.txt;
done
#well that didn't work
#let's try a while loop instead
while read line; do
	line2=${line::-11}; # _edit.fasta has 11 characters
	line3=${line2:52}; #/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/ has 52 characters
	echo $line3 >> SpeciesNames.txt;
done < FileNames.txt
#that worked!
#now let's make the species directories in /home/inf-47-2020/ThesisTrich/DB_Construct/Species_DBs/
while read line; do
	mkdir $line;
done < /home/inf-47-2020/ThesisTrich/DB_Construct/SpeciesNames.txt
#done!
###
#A. lanta integration
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/ directory
ls /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta > FileNames_Alanta.txt
while read line; do
	line2=${line::-11}; # _edit.fasta has 11 characters
	line3=${line2:52}; #/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/ has 52 characters
	echo $line3 >> SpeciesNames_Alanta.txt;
done < FileNames_Alanta.txt
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Species_DBs/ directory
while read line; do
	mkdir $line;
done < ../SpeciesNames_Alanta.txt
#done! moving on...

```

Creating symlinks of the data files (script saved to link_data_files.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: link_data_files.py
Date: 2022-03-07
Author: Virág Varga

Description:
	This program creates symbolic links (symlinks) between parsed data files stored
		in a per-analysis program structure, to to directories specific to the
		strain or species being analyzed.
	This linking is necessary as preparation for the construction of species-specific
		databases with annotations on each protein query.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	subprocess

Procedure:
	1. Loading required modules; defining inputs as command line
		arguments; loading the contents of the reference file as a list.
	2. Matching the substrings contained in the reference file to the subdirectory
		and results file names. When matches are found, creating a symlink from
		the source file to the destination directory.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the following formatting for inputs:
		-> Source and distination directories must be given without a trailing
			forwardslash ("/") at the end of the path. The full path must be given
			as the argument, relative paths will not work.
		-> The input reference file must contain substrings to search for in file
			and directory names in the format of one substring per line.

Usage
	./link_data_files.py parsed_data_dir species_file species_dir
	OR
	python link_data_files.py parsed_data_dir species_file species_dir

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, files; determine command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allows access to the files and directories of the computer/system
import subprocess #allows the running of bash commands from within python

#assign command line arguments; load input and output files
parsed_data_dir = sys.argv[1]
#parsed_data_dir = "TEST_dir/Parsed_dir"
species_file = sys.argv[2]
#species_file = "TEST_dir/Ref_file.txt"
species_dir = sys.argv[3]
#species_dir = "TEST_dir/Assembly_dir"

#easy part first - import the reference file
with open(species_file, "r") as infile:
	#open the reference file for reading
	species_list = infile.read().splitlines()
	#read the contents of the file into a list, and remove end-line characters ("\n")


#Part 2: Loop through directories of analyzed data and symlink the appropriate files

for subdir, dirs, files in os.walk(parsed_data_dir):
	#iterate over the files contained in the subdirectories of the parsed data directory
	for file in files:
		#specifically iterate over the files themselves
		for species in species_list:
			#iterate over the list of species/strains
			if species in file:
				#identify places where the species/strain ID is a substring of a results file
				symlink_path = species_dir + "/" + species
				#define the destination path of the symlink as a string
				source_path = subdir + "/" + file
				#define the source file path as a string
				bash_cmd = "ln -s " + source_path + " " + symlink_path
				#define the bash command to be executed as a string
				#for the next two lines, ref: https://stackoverflow.com/questions/4256107/running-bash-commands-in-python
				process = subprocess.Popen(bash_cmd.split(), stdout=subprocess.PIPE)
				output, error = process.communicate()
				#execute the bash command defined above

```

Running it:

```bash
#model:
python link_data_files.py parsed_data_dir species_file species_dir
#adapted to my data:
#running from the main ThesisTrich/ directory
python Scripts/link_data_files.py PARSED_Results DB_Construct/SpeciesNames.txt DB_Construct/Species_DBs
#this made broken symlinks :/
python /home/inf-47-2020/ThesisTrich/Scripts/link_data_files.py /home/inf-47-2020/ThesisTrich/PARSED_Results /home/inf-47-2020/ThesisTrich/DB_Construct/SpeciesNames.txt /home/inf-47-2020/ThesisTrich/DB_Construct/Species_DBs
#this time it worked!
###
#now need to manually link the DeepLoc files from the preliminary project
#these have unusual naming conventions, so they don't get read properly by the loop
#working in the individual species directories within /home/inf-47-2020/ThesisTrich/DB_Construct/Species_DBs/
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/BM_anaeromoeba_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/BS_anaeromoeba_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/Carpediemonas_membranifera_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/D_fragilis_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/H_meleagridis_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/P_hominis_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/SC_anaeromoeba_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_foetus_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_gallinarum_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_vaginalis_GB_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_vaginalis_RS_DL_DeepLocP.txt .
###
#A. lanta integration
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Species_DBs/ directory
python /home/inf-47-2020/ThesisTrich/Scripts/link_data_files.py /home/inf-47-2020/ThesisTrich/PARSED_Results /home/inf-47-2020/ThesisTrich/DB2_A_lanta/SpeciesNames_Alanta.txt /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Species_DBs
#and now, re-running the DeepLoc symlinks as before
#working in the individual species directories within /home/inf-47-2020/ThesisTrich/DB_Construct/Species_DBs/
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/BM_anaeromoeba_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/BS_anaeromoeba_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/Carpediemonas_membranifera_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/D_fragilis_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/H_meleagridis_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/P_hominis_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/SC_anaeromoeba_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_foetus_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_gallinarum_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_vaginalis_GB_DL_DeepLocP.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/DeepLoc/T_vaginalis_RS_DL_DeepLocP.txt .
#done! 

```

#### Smoothing over file structure

For a number of the analyses performed, the original input FASTA files needed to be split up to save on time and parallelize the computing process. There are therefore multiple results files per proteome.

The parsing of these results files should be performed on the raw results files as they are. The parsed results files that are output by the parsers should then be concatenated as below prior to the construction of the species databases:

```bash
#first, create versions of the files without header lines
cat SPECIES_FILES | grep -v *Part01* | tail -n +2 > SPECIES_temp.txt
#then `cat` them together
cat SPECIES_FILE.Part01 SPECIES_temp.txt > SPECIES_PREDICT_DB
###
#application
#testing it on mock files
cat test*.txt | grep -v *1* | tail -n +2 > testing_temp.txt
cat test1.txt testing_temp.txt > testing_complete.txt
#well that's really not working, so let's try something else
ls test*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 test1.txt > test_columns.txt
#the above has to go after the loop because of the naming convention
cat test_columns.txt *_temp.txt > test_final.txt
#yup that worked!!!
#so we can use this on the parsed IPRScan results files, once they're done
#and others, like targetp...
#unfortunately, this has to be done manually, looping isn't really possible here
#BM_newprots_may21.anaeromoeba
ls BM_newprots_may21.anaeromoeba_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 BM_newprots_may21.anaeromoeba_edit_StandardAA.Part01_IPRScan.txt > BM_anaeromoeba_columns.txt
#the above has to go after the loop because of the naming convention
cat BM_anaeromoeba_columns.txt *_temp.txt > BM_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt
#BS_newprots_may21.anaeromoeba
ls BS_newprots_may21.anaeromoeba_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 BS_newprots_may21.anaeromoeba_edit_StandardAA.Part01_IPRScan.txt > BS_anaeromoeba_columns.txt
#the above has to go after the loop because of the naming convention
cat BS_anaeromoeba_columns.txt *_temp.txt > BS_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt
#EP00764_Aduncisulcus_paluster
ls EP00764_Aduncisulcus_paluster_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 EP00764_Aduncisulcus_paluster_edit_StandardAA.Part01_IPRScan.txt > EP00764_Aduncisulcus_paluster_columns.txt
#the above has to go after the loop because of the naming convention
cat EP00764_Aduncisulcus_paluster_columns.txt *_temp.txt > EP00764_Aduncisulcus_paluster_edit_StandardAA_IPRScan_final.txt
#EP00770_Monocercomonoides_exilis
ls EP00770_Monocercomonoides_exilis_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 EP00770_Monocercomonoides_exilis_edit_StandardAA.Part01_IPRScan.txt > EP00770_Monocercomonoides_exilis_columns.txt
#the above has to go after the loop because of the naming convention
cat EP00770_Monocercomonoides_exilis_columns.txt *_temp.txt > EP00770_Monocercomonoides_exilis_edit_StandardAA_IPRScan_final.txt
#EP00792_Barthelona_sp_PAP020
ls EP00792_Barthelona_sp_PAP020_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 EP00792_Barthelona_sp_PAP020_edit_StandardAA.Part01_IPRScan.txt > EP00792_Barthelona_sp_PAP020_columns.txt
#the above has to go after the loop because of the naming convention
cat EP00792_Barthelona_sp_PAP020_columns.txt *_temp.txt > EP00792_Barthelona_sp_PAP020_edit_StandardAA_IPRScan_final.txt
#Histomonas_meleagridis.PRJNA594289
ls Histomonas_meleagridis.PRJNA594289_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Histomonas_meleagridis.PRJNA594289_edit_StandardAA.Part01_IPRScan.txt > Histomonas_meleagridis.PRJNA594289_columns.txt
#the above has to go after the loop because of the naming convention
cat Histomonas_meleagridis.PRJNA594289_columns.txt *_temp.txt > Histomonas_meleagridis.PRJNA594289_edit_StandardAA_IPRScan_final.txt
#Kipferlia_bialata.PRJDB5223
ls Kipferlia_bialata.PRJDB5223_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Kipferlia_bialata.PRJDB5223_edit_StandardAA.Part01_IPRScan.txt > Kipferlia_bialata.PRJDB5223_columns.txt
#the above has to go after the loop because of the naming convention
cat Kipferlia_bialata.PRJDB5223_columns.txt *_temp.txt > Kipferlia_bialata.PRJDB5223_edit_StandardAA_IPRScan_final.txt
#Pentatrichomonas_hominis.5728.aa
ls Pentatrichomonas_hominis.5728.aa_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Pentatrichomonas_hominis.5728.aa_edit_StandardAA.Part01_IPRScan.txt > Pentatrichomonas_hominis.5728.aa_columns.txt
#the above has to go after the loop because of the naming convention
cat Pentatrichomonas_hominis.5728.aa_columns.txt *_temp.txt > Pentatrichomonas_hominis.5728.aa_edit_StandardAA_IPRScan_final.txt
ls Pentatrichomonas_hominis.5728.aa_edit.*_half_YL_YLparsed.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Pentatrichomonas_hominis.5728.aa_edit.1st_half_YL_YLparsed.txt > Pentatrichomonas_hominis.5728.aa_YL_columns.txt
#the above has to go after the loop because of the naming convention
cat Pentatrichomonas_hominis.5728.aa_YL_columns.txt *YL_YLparsed_temp.txt > Pentatrichomonas_hominis.5728.aa_edit_YL_YLparsed_final.txt
#SC_newprots_may21.anaeromoeba
ls SC_newprots_may21.anaeromoeba_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 SC_newprots_may21.anaeromoeba_edit_StandardAA.Part01_IPRScan.txt > SC_anaeromoeba_columns.txt
#the above has to go after the loop because of the naming convention
cat SC_anaeromoeba_columns.txt *_temp.txt > SC_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt
#Tetratrichomonas_gallinarum.5730.aa
ls Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA.Part01_IPRScan.txt > Tetratrichomonas_gallinarum.5730.aa_IPRScan_columns.txt
#the above has to go after the loop because of the naming convention
cat Tetratrichomonas_gallinarum.5730.aa_IPRScan_columns.txt *IPRScan_temp.txt > Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA_IPRScan_final.txt
ls Tetratrichomonas_gallinarum.5730.aa_edit.*_half_summary_TargetP.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Tetratrichomonas_gallinarum.5730.aa_edit.1st_half_summary_TargetP.txt > Tetratrichomonas_gallinarum.5730.aa_TargetP_columns.txt
#the above has to go after the loop because of the naming convention
cat Tetratrichomonas_gallinarum.5730.aa_TargetP_columns.txt *TargetP_temp.txt > Tetratrichomonas_gallinarum.5730.aa_edit_summary_TargetP_final.txt
ls Tetratrichomonas_gallinarum.5730.aa_edit.*_half_YL_YLparsed.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Tetratrichomonas_gallinarum.5730.aa_edit.1st_half_YL_YLparsed.txt > Tetratrichomonas_gallinarum.5730.aa_YL_columns.txt
#the above has to go after the loop because of the naming convention
cat Tetratrichomonas_gallinarum.5730.aa_YL_columns.txt *YLparsed_temp.txt > Tetratrichomonas_gallinarum.5730.aa_YL_YLparsed_final.txt
#Trichomonas_foetus.PRJNA345179
ls Trichomonas_foetus.PRJNA345179_edit_StandardAA.Part*.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Trichomonas_foetus.PRJNA345179_edit_StandardAA.Part01_IPRScan.txt > Trichomonas_foetus.PRJNA345179_columns.txt
#the above has to go after the loop because of the naming convention
cat Trichomonas_foetus.PRJNA345179_columns.txt *_temp.txt > Trichomonas_foetus.PRJNA345179_edit_StandardAA_IPRScan_final.txt
#Trichomonas_vaginalis_GenBank.PRJNA16084
ls Trichomonas_vaginalis_GenBank.PRJNA16084_edit.*_half_summary_TargetP.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Trichomonas_vaginalis_GenBank.PRJNA16084_edit.1st_half_summary_TargetP.txt > Trichomonas_vaginalis_GenBank.PRJNA16084_TargetP_columns.txt
#the above has to go after the loop because of the naming convention
cat Trichomonas_vaginalis_GenBank.PRJNA16084_TargetP_columns.txt *TargetP_temp.txt > Trichomonas_vaginalis_GenBank.PRJNA16084_summary_TargetP_final.txt
ls Trichomonas_vaginalis_GenBank.PRJNA16084_edit.*_half_YL_YLparsed.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Trichomonas_vaginalis_GenBank.PRJNA16084_edit.1st_half_YL_YLparsed.txt > Trichomonas_vaginalis_GenBank.PRJNA16084_YL_columns.txt
#the above has to go after the loop because of the naming convention
cat Trichomonas_vaginalis_GenBank.PRJNA16084_YL_columns.txt *YLparsed_temp.txt > Trichomonas_vaginalis_GenBank.PRJNA16084_YL_YLparsed_final.txt
#Trichomonas_vaginalis_RefSeq.G3
ls Trichomonas_vaginalis_RefSeq.G3_edit.*_half_summary_TargetP.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Trichomonas_vaginalis_RefSeq.G3_edit.1st_half_summary_TargetP.txt > Trichomonas_vaginalis_RefSeq.G3_TargetP_columns.txt
#the above has to go after the loop because of the naming convention
cat Trichomonas_vaginalis_RefSeq.G3_TargetP_columns.txt *TargetP_temp.txt > Trichomonas_vaginalis_RefSeq.G3_summary_TargetP_final.txt
ls Trichomonas_vaginalis_RefSeq.G3_edit.*_half_YL_YLparsed.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 Trichomonas_vaginalis_RefSeq.G3_edit.1st_half_YL_YLparsed.txt > Trichomonas_vaginalis_RefSeq.G3_YL_columns.txt
#the above has to go after the loop because of the naming convention
cat Trichomonas_vaginalis_RefSeq.G3_YL_columns.txt *YLparsed_temp.txt > Trichomonas_vaginalis_RefSeq.G3_YL_YLparsed_final.txt
###
#A. lanta integration
#the above are repeated in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Species_DBs/ directories

```


Combining all of the parsed files into one, and adding in a column to specify the species designation (script saved to combo_OG_results__v3.py).

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: combo_OG_results__v3.py
Date: 2022-03-16
Author: Virág Varga

Description:
	This program parses the results files for 1 species from the eggNOG_dn_PFam_Parser__v2.py,
		eggNOG_dn_Parser__v2.py, signalP_Parser__v2.py, targetP_Parser__v2.py,
		deepLoc_Parser__v2.py, mitoFates_Parser__v2.py, iprScan_Parser.py and
		yLoc_Parser__v2.py scripts. The data in these parsed files is concatenated
		into one large, flat database.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	numpy

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas import the contents of the de novo PFam via EggNOG, de novo EggNOG
		SignalP, TargetP, InterProScan, DeepLoc, Yloc and MitoFates results files.
	3. Cpncatenating the databases and filling NaN values with "-".
	4. Writing out the results to a tab-delimited text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of the parsed files created thorugh the use of
		eggNOG_dn_PFam_Parser__v2.py, eggNOG_dn_Parser__v2.py, signalP_Parser__v2.py,
		targetP_Parser__v2.py, deepLoc_Parser__v2.py, mitoFates_Parser__v2.py, iprScan_Parser.py
		and yLoc_Parser__v2.py.
	- All input and output files are user-defined: This means the user must ensure that
		the correct file names have been assigned to the program.

Version:
	This is version 3.0 of this program. A species ID column will now be added before the database
		is written out to a file.

Usage
	./combo_OG_results__v3.py PFam_EN_Parsed EggNOG_Parsed DeepLoc_Parsed SignalP_Parsed
		TargetP_Parsed IPRScan_Parsed MitoFates_Parsed YLoc_Parsed Species_ID Output_DB
	OR
	python combo_OG_results__v3.py PFam_EN_Parsed EggNOG_Parsed DeepLoc_Parsed SignalP_Parsed
		TargetP_Parsed IPRScan_Parsed MitoFates_Parsed YLoc_Parsed Species_ID Output_DB

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
import numpy as np #facilitates manipulation of arrays in Python


#assign command line arguments: input & output files
PFam_EN_Parsed = sys.argv[1]
#PFam_EN_Parsed = "BM_anaeromoeba_EN_PFam.txt"
EggNOG_Parsed = sys.argv[2]
#EggNOG_Parsed = "BM_anaeromoeba_EN_eggNOG.txt"
DeepLoc_Parsed = sys.argv[3]
#DeepLoc_Parsed = "BM_anaeromoeba_DL_DeepLoc.txt"
SignalP_Parsed = sys.argv[4]
#SignalP_Parsed = "BM_anaeromoeba\BM_newprots_may21_SignalP.txt"
TargetP_Parsed = sys.argv[5]
IPRScan_Parsed = sys.argv[6]
MitoFates_Parsed = sys.argv[7]
YLoc_Parsed = sys.argv[8]
Species_ID = sys.argv[9]
Output_DB = sys.argv[10]
#Output_DB = "BM_anaeromoeba_DB_TEST.txt"


#Part 2: Importing parsed data file contents into Pandas dataframes

#PFam results
with open(PFam_EN_Parsed, "r") as pfam_infile:
	#open the parsed PFam via eggNOG data file
	pfam_df = pd.read_csv(pfam_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	pfam_df = pfam_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	pfam_df = pfam_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	pfam_df = pfam_df.add_prefix('pfamEN_')
	#add prefix to column names to prevent duplicates between the dataframes

#eggNOG results
with open(EggNOG_Parsed, "r") as eggnog_infile:
	#open the parsed eggNOG data file
	eggnog_df = pd.read_csv(eggnog_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	eggnog_df = eggnog_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	eggnog_df = eggnog_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	eggnog_df = eggnog_df.add_prefix('EN_')
	#add prefix to column names to prevent duplicates between the dataframes

#DeepLoc results
with open(DeepLoc_Parsed, "r") as deeploc_infile:
	#open the parsed DeepLoc data file
	deeploc_df = pd.read_csv(deeploc_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	deeploc_df = deeploc_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	deeploc_df = deeploc_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	deeploc_df = deeploc_df.add_prefix('DeepL_')
	#add prefix to column names to prevent duplicates between the dataframes

#SignalP results
with open(SignalP_Parsed, "r") as signalp_infile:
	#open the parsed SignalP data file
	signalp_df = pd.read_csv(signalp_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	signalp_df = signalp_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	signalp_df = signalp_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	signalp_df = signalp_df.add_prefix('SigP_')
	#add prefix to column names to prevent duplicates between the dataframes

#TargetP results
with open(TargetP_Parsed, "r") as targetp_infile:
	#open the parsed TargetP data file
	targetp_df = pd.read_csv(targetp_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	targetp_df = targetp_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	targetp_df = targetp_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	targetp_df = targetp_df.add_prefix('TarP_')
	#add prefix to column names to prevent duplicates between the dataframes

#InterProScan results
with open(IPRScan_Parsed, "r") as iprscan_infile:
	#open the parsed InterProScan data file
	iprscan_df = pd.read_csv(iprscan_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	iprscan_df = iprscan_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	iprscan_df = iprscan_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	iprscan_df = iprscan_df.add_prefix('iprS_')
	#add prefix to column names to prevent duplicates between the dataframes

#MitoFates results
with open(MitoFates_Parsed, "r") as mitofates_infile:
	#open the parsed MitoFates data file
	mitofates_df = pd.read_csv(mitofates_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	mitofates_df = mitofates_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	mitofates_df = mitofates_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	mitofates_df = mitofates_df.add_prefix('MitoF_')
	#add prefix to column names to prevent duplicates between the dataframes

#YLoc results
with open(YLoc_Parsed, "r") as yloc_infile:
	#open the parsed YLoc data file
	yloc_df = pd.read_csv(yloc_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	yloc_df = yloc_df.set_index('Query')
	#set the first column (containing query sequence names) as an index
	yloc_df = yloc_df.replace(r'^\s*$', np.NaN, regex=True)
	#replace empty cells with 'NaN'
	yloc_df = yloc_df.add_prefix('YLoc_')
	#add prefix to column names to prevent duplicates between the dataframes


#Part 3: Merge the dataframes and write out

merged_df = pd.concat([pfam_df, eggnog_df, iprscan_df, signalp_df, targetp_df, deeploc_df, mitofates_df, yloc_df], axis=1)
#concatenate the dataframes along the x axis, horizontally
merged_df.fillna('-', inplace=True)


#add in species ID column
merged_df = pd.concat([pd.Series(Species_ID, index=merged_df.index, name='Species_Id'), merged_df], axis=1)


#write out results to tab-delimited text file
merged_df.to_csv(Output_DB, sep='\t', index=True)
#since the Query columns got shifted into indexes, need to use `index=True`

```

Using it:

```bash
#unfortunately, these need to be written manually, since all of the directories have slightly different file name structures
#working in the species directories within /home/inf-47-2020/ThesisTrich/DB_Construct/Species_DBs/
#model:
python combo_OG_results__v2.py PFam_EN_Parsed EggNOG_Parsed DeepLoc_Parsed SignalP_Parsed TargetP_Parsed IPRScan_Parsed MitoFates_Parsed YLoc_Parsed Output_DB
#applying it:
python ../../../Scripts/combo_OG_results__v2.py BM_newprots_may21.anaeromoeba_edit.emap.emapper_PFam.txt BM_newprots_may21.anaeromoeba_edit.emap.emapper_eggNOG.txt BM_anaeromoeba_DL_DeepLocP.txt BM_newprots_may21.anaeromoeba_edit_summary_SignalP.txt BM_newprots_may21.anaeromoeba_edit_summary_TargetP.txt BM_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt BM_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults_MFparsed.txt BM_newprots_may21.anaeromoeba_edit_YL_YLparsed.txt BM_newprots_may21.anaeromoeba_predDB.txt
#after some necessary debugging, this is working!!!
#time to use it on the other species
#actually, first: new version that adds in the species IDs in the column after the queries
#going to use the the species IDs used in the categorize_prot_species.py script
python ../../../Scripts/combo_OG_results__v3.py BM_newprots_may21.anaeromoeba_edit.emap.emapper_PFam.txt BM_newprots_may21.anaeromoeba_edit.emap.emapper_eggNOG.txt BM_anaeromoeba_DL_DeepLocP.txt BM_newprots_may21.anaeromoeba_edit_summary_SignalP.txt BM_newprots_may21.anaeromoeba_edit_summary_TargetP.txt BM_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt BM_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults_MFparsed.txt BM_newprots_may21.anaeromoeba_edit_YL_YLparsed.txt BM_newprots_may21.anaeromoeba BM_newprots_may21.anaeromoeba_predDB.txt
#yup, great, going with that
#model:
python combo_OG_results__v3.py PFam_EN_Parsed EggNOG_Parsed DeepLoc_Parsed SignalP_Parsed TargetP_Parsed IPRScan_Parsed MitoFates_Parsed YLoc_Parsed Species_ID Output_DB
#applying it:
python ../../../Scripts/combo_OG_results__v3.py BS_newprots_may21.anaeromoeba_edit.emap.emapper_PFam.txt BS_newprots_may21.anaeromoeba_edit.emap.emapper_eggNOG.txt BS_anaeromoeba_DL_DeepLocP.txt BS_newprots_may21.anaeromoeba_edit_summary_SignalP.txt BS_newprots_may21.anaeromoeba_edit_summary_TargetP.txt BS_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt BS_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults_MFparsed.txt BS_newprots_may21.anaeromoeba_edit_YL_YLparsed.txt BS_newprots_may21.anaeromoeba BS_newprots_may21.anaeromoeba_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Carpediemonas_membranifera.PRJNA719540_edit.emap.emapper_PFam.txt Carpediemonas_membranifera.PRJNA719540_edit.emap.emapper_eggNOG.txt Carpediemonas_membranifera_DL_DeepLocP.txt Carpediemonas_membranifera.PRJNA719540_edit_summary_SignalP.txt Carpediemonas_membranifera.PRJNA719540_edit_summary_TargetP.txt Carpediemonas_membranifera.PRJNA719540_edit_StandardAA_IPRScan.txt Carpediemonas_membranifera.PRJNA719540_edit_StandardAA_nonM_MFresults_MFparsed.txt Carpediemonas_membranifera.PRJNA719540_edit_YL_YLparsed.txt Carpediemonas_membranifera.PRJNA719540 Carpediemonas_membranifera.PRJNA719540_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Dientamoeba_fragilis.43352.aa_edit.emap.emapper_PFam.txt Dientamoeba_fragilis.43352.aa_edit.emap.emapper_eggNOG.txt D_fragilis_DL_DeepLocP.txt Dientamoeba_fragilis.43352.aa_edit_summary_SignalP.txt Dientamoeba_fragilis.43352.aa_edit_summary_TargetP.txt Dientamoeba_fragilis.43352.aa_edit_StandardAA_IPRScan.txt Dientamoeba_fragilis.43352.aa_edit_StandardAA_nonM_MFresults_MFparsed.txt Dientamoeba_fragilis.43352.aa_edit_YL_YLparsed.txt Dientamoeba_fragilis.43352.aa Dientamoeba_fragilis.43352.aa_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00703_Trepomonas_sp_PC1_edit.emap.emapper_PFam.txt EP00703_Trepomonas_sp_PC1_edit.emap.emapper_eggNOG.txt EP00703_Trepomonas_sp_PC1_DL_DeepLocP.txt EP00703_Trepomonas_sp_PC1_edit_summary_SignalP.txt EP00703_Trepomonas_sp_PC1_edit_summary_TargetP.txt EP00703_Trepomonas_sp_PC1_edit_StandardAA_IPRScan.txt EP00703_Trepomonas_sp_PC1_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00703_Trepomonas_sp_PC1_edit_YL_YLparsed.txt EP00703_Trepomonas_sp_PC1 EP00703_Trepomonas_sp_PC1_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00708_Paratrimastix_pyriformis_edit.emap.emapper_PFam.txt EP00708_Paratrimastix_pyriformis_edit.emap.emapper_eggNOG.txt EP00708_Paratrimastix_pyriformis_DL_DeepLocP.txt EP00708_Paratrimastix_pyriformis_edit_summary_SignalP.txt EP00708_Paratrimastix_pyriformis_edit_summary_TargetP.txt EP00708_Paratrimastix_pyriformis_edit_StandardAA_IPRScan.txt EP00708_Paratrimastix_pyriformis_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00708_Paratrimastix_pyriformis_edit_YL_YLparsed.txt EP00708_Paratrimastix_pyriformis EP00708_Paratrimastix_pyriformis_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00764_Aduncisulcus_paluster_edit.emap.emapper_PFam.txt EP00764_Aduncisulcus_paluster_edit.emap.emapper_eggNOG.txt EP00764_Aduncisulcus_paluster_DL_DeepLocP.txt EP00764_Aduncisulcus_paluster_edit_summary_SignalP.txt EP00764_Aduncisulcus_paluster_edit_summary_TargetP.txt EP00764_Aduncisulcus_paluster_edit_StandardAA_IPRScan_final.txt EP00764_Aduncisulcus_paluster_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00764_Aduncisulcus_paluster_edit_YL_YLparsed.txt EP00764_Aduncisulcus_paluster EP00764_Aduncisulcus_paluster_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00766_Chilomastix_caulleryi_edit.emap.emapper_PFam.txt EP00766_Chilomastix_caulleryi_edit.emap.emapper_eggNOG.txt EP00766_Chilomastix_caulleryi_DL_DeepLocP.txt EP00766_Chilomastix_caulleryi_edit_summary_SignalP.txt EP00766_Chilomastix_caulleryi_edit_summary_TargetP.txt EP00766_Chilomastix_caulleryi_edit_StandardAA_IPRScan.txt EP00766_Chilomastix_caulleryi_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00766_Chilomastix_caulleryi_edit_YL_YLparsed.txt EP00766_Chilomastix_caulleryi EP00766_Chilomastix_caulleryi_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00767_Chilomastix_cuspidata_edit.emap.emapper_PFam.txt EP00767_Chilomastix_cuspidata_edit.emap.emapper_eggNOG.txt EP00767_Chilomastix_cuspidata_DL_DeepLocP.txt EP00767_Chilomastix_cuspidata_edit_summary_SignalP.txt EP00767_Chilomastix_cuspidata_edit_summary_TargetP.txt EP00767_Chilomastix_cuspidata_edit_StandardAA_IPRScan.txt EP00767_Chilomastix_cuspidata_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00767_Chilomastix_cuspidata_edit_YL_YLparsed.txt EP00767_Chilomastix_cuspidata EP00767_Chilomastix_cuspidata_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00768_Dysnectes_brevis_edit.emap.emapper_PFam.txt EP00768_Dysnectes_brevis_edit.emap.emapper_eggNOG.txt EP00768_Dysnectes_brevis_DL_DeepLocP.txt EP00768_Dysnectes_brevis_edit_summary_SignalP.txt EP00768_Dysnectes_brevis_edit_summary_TargetP.txt EP00768_Dysnectes_brevis_edit_StandardAA_IPRScan.txt EP00768_Dysnectes_brevis_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00768_Dysnectes_brevis_edit_YL_YLparsed.txt EP00768_Dysnectes_brevis EP00768_Dysnectes_brevis_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00769_Ergobibamus_cyprinoides_edit.emap.emapper_PFam.txt EP00769_Ergobibamus_cyprinoides_edit.emap.emapper_eggNOG.txt EP00769_Ergobibamus_cyprinoides_DL_DeepLocP.txt EP00769_Ergobibamus_cyprinoides_edit_summary_SignalP.txt EP00769_Ergobibamus_cyprinoides_edit_summary_TargetP.txt EP00769_Ergobibamus_cyprinoides_edit_StandardAA_IPRScan.txt EP00769_Ergobibamus_cyprinoides_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00769_Ergobibamus_cyprinoides_edit_YL_YLparsed.txt EP00769_Ergobibamus_cyprinoides EP00769_Ergobibamus_cyprinoides_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00770_Monocercomonoides_exilis_edit.emap.emapper_PFam.txt EP00770_Monocercomonoides_exilis_edit.emap.emapper_eggNOG.txt EP00770_Monocercomonoides_exilis_DL_DeepLocP.txt EP00770_Monocercomonoides_exilis_edit_summary_SignalP.txt EP00770_Monocercomonoides_exilis_edit_summary_TargetP.txt EP00770_Monocercomonoides_exilis_edit_StandardAA_IPRScan_final.txt EP00770_Monocercomonoides_exilis_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00770_Monocercomonoides_exilis_edit_YL_YLparsed.txt EP00770_Monocercomonoides_exilis EP00770_Monocercomonoides_exilis_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00771_Trimastix_marina_edit.emap.emapper_PFam.txt EP00771_Trimastix_marina_edit.emap.emapper_eggNOG.txt EP00771_Trimastix_marina_DL_DeepLocP.txt EP00771_Trimastix_marina_edit_summary_SignalP.txt EP00771_Trimastix_marina_edit_summary_TargetP.txt EP00771_Trimastix_marina_edit_StandardAA_IPRScan.txt EP00771_Trimastix_marina_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00771_Trimastix_marina_edit_YL_YLparsed.txt EP00771_Trimastix_marina EP00771_Trimastix_marina_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00792_Barthelona_sp_PAP020_edit.emap.emapper_PFam.txt EP00792_Barthelona_sp_PAP020_edit.emap.emapper_eggNOG.txt EP00792_Barthelona_sp_PAP020_DL_DeepLocP.txt EP00792_Barthelona_sp_PAP020_edit_summary_SignalP.txt EP00792_Barthelona_sp_PAP020_edit_summary_TargetP.txt EP00792_Barthelona_sp_PAP020_edit_StandardAA_IPRScan_final.txt EP00792_Barthelona_sp_PAP020_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00792_Barthelona_sp_PAP020_edit_YL_YLparsed.txt EP00792_Barthelona_sp_PAP020 EP00792_Barthelona_sp_PAP020_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py EP00701_Giardia_intestinalis_edit.emap.emapper_PFam.txt EP00701_Giardia_intestinalis_edit.emap.emapper_eggNOG.txt EP00701_Giardia_intestinalis_DL_DeepLocP.txt EP00701_Giardia_intestinalis_edit_summary_SignalP.txt EP00701_Giardia_intestinalis_edit_summary_TargetP.txt EP00701_Giardia_intestinalis_edit_StandardAA_IPRScan.txt EP00701_Giardia_intestinalis_edit_StandardAA_nonM_MFresults_MFparsed.txt EP00701_Giardia_intestinalis_edit_YL_YLparsed.txt EP00701_Giardia_intestinalis EP00701_Giardia_intestinalis_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Giardia_intestinalis.PRJNA1439_edit.emap.emapper_PFam.txt Giardia_intestinalis.PRJNA1439_edit.emap.emapper_eggNOG.txt Giardia_intestinalis.PRJNA1439_DL_DeepLocP.txt Giardia_intestinalis.PRJNA1439_edit_summary_SignalP.txt Giardia_intestinalis.PRJNA1439_edit_summary_TargetP.txt Giardia_intestinalis.PRJNA1439_edit_StandardAA_IPRScan.txt Giardia_intestinalis.PRJNA1439_edit_StandardAA_nonM_MFresults_MFparsed.txt Giardia_intestinalis.PRJNA1439_edit_YL_YLparsed.txt Giardia_intestinalis.PRJNA1439 Giardia_intestinalis.PRJNA1439_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py GiardiaDB_GintestinalisADH_edit.emap.emapper_PFam.txt GiardiaDB_GintestinalisADH_edit.emap.emapper_eggNOG.txt GiardiaDB_GintestinalisADH_DL_DeepLocP.txt GiardiaDB_GintestinalisADH_edit_summary_SignalP.txt GiardiaDB_GintestinalisADH_edit_summary_TargetP.txt GiardiaDB_GintestinalisADH_edit_StandardAA_IPRScan.txt GiardiaDB_GintestinalisADH_edit_StandardAA_nonM_MFresults_MFparsed.txt GiardiaDB_GintestinalisADH_edit_YL_YLparsed.txt GiardiaDB_GintestinalisADH GiardiaDB_GintestinalisADH_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py GiardiaDB_GintestinalisBGS_B_edit.emap.emapper_PFam.txt GiardiaDB_GintestinalisBGS_B_edit.emap.emapper_eggNOG.txt GiardiaDB_GintestinalisBGS_B_DL_DeepLocP.txt GiardiaDB_GintestinalisBGS_B_edit_summary_SignalP.txt GiardiaDB_GintestinalisBGS_B_edit_summary_TargetP.txt GiardiaDB_GintestinalisBGS_B_edit_StandardAA_IPRScan.txt GiardiaDB_GintestinalisBGS_B_edit_StandardAA_nonM_MFresults_MFparsed.txt GiardiaDB_GintestinalisBGS_B_edit_YL_YLparsed.txt GiardiaDB_GintestinalisBGS_B GiardiaDB_GintestinalisBGS_B_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py GiardiaDB_GintestinalisBGS_edit.emap.emapper_PFam.txt GiardiaDB_GintestinalisBGS_edit.emap.emapper_eggNOG.txt GiardiaDB_GintestinalisBGS_DL_DeepLocP.txt GiardiaDB_GintestinalisBGS_edit_summary_SignalP.txt GiardiaDB_GintestinalisBGS_edit_summary_TargetP.txt GiardiaDB_GintestinalisBGS_edit_StandardAA_IPRScan.txt GiardiaDB_GintestinalisBGS_edit_StandardAA_nonM_replaceAA__MFresults_MFparsed.txt GiardiaDB_GintestinalisBGS_edit_YL_YLparsed.txt GiardiaDB_GintestinalisBGS GiardiaDB_GintestinalisBGS_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py GiardiaDB_GintestinalisEP15_edit.emap.emapper_PFam.txt GiardiaDB_GintestinalisEP15_edit.emap.emapper_eggNOG.txt GiardiaDB_GintestinalisEP15_DL_DeepLocP.txt GiardiaDB_GintestinalisEP15_edit_summary_SignalP.txt GiardiaDB_GintestinalisEP15_edit_summary_TargetP.txt GiardiaDB_GintestinalisEP15_edit_StandardAA_IPRScan.txt GiardiaDB_GintestinalisEP15_edit_StandardAA_nonM_MFresults_MFparsed.txt GiardiaDB_GintestinalisEP15_edit_YL_YLparsed.txt GiardiaDB_GintestinalisEP15 GiardiaDB_GintestinalisEP15_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Giardia_muris.PRJNA524057_edit.emap.emapper_PFam.txt Giardia_muris.PRJNA524057_edit.emap.emapper_eggNOG.txt Giardia_muris.PRJNA524057_DL_DeepLocP.txt Giardia_muris.PRJNA524057_edit_summary_SignalP.txt Giardia_muris.PRJNA524057_edit_summary_TargetP.txt Giardia_muris.PRJNA524057_edit_StandardAA_IPRScan.txt Giardia_muris.PRJNA524057_edit_StandardAA_nonM_MFresults_MFparsed.txt Giardia_muris.PRJNA524057_edit_YL_YLparsed.txt Giardia_muris.PRJNA524057 Giardia_muris.PRJNA524057_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Histomonas_meleagridis.135588.aa_edit.emap.emapper_PFam.txt Histomonas_meleagridis.135588.aa_edit.emap.emapper_eggNOG.txt H_meleagridis_DL_DeepLocP.txt Histomonas_meleagridis.135588.aa_edit_summary_SignalP.txt Histomonas_meleagridis.135588.aa_edit_summary_TargetP.txt Histomonas_meleagridis.135588.aa_edit_StandardAA_IPRScan.txt Histomonas_meleagridis.135588.aa_edit_StandardAA_nonM_MFresults_MFparsed.txt Histomonas_meleagridis.135588.aa_edit_YL_YLparsed.txt Histomonas_meleagridis.135588.aa Histomonas_meleagridis.135588.aa_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Histomonas_meleagridis.PRJNA594289_edit.emap.emapper_PFam.txt Histomonas_meleagridis.PRJNA594289_edit.emap.emapper_eggNOG.txt Histomonas_meleagridis.PRJNA594289_DL_DeepLocP.txt Histomonas_meleagridis.PRJNA594289_edit_summary_SignalP.txt Histomonas_meleagridis.PRJNA594289_edit_summary_TargetP.txt Histomonas_meleagridis.PRJNA594289_edit_StandardAA_IPRScan_final.txt Histomonas_meleagridis.PRJNA594289_edit_StandardAA_nonM_MFresults_MFparsed.txt Histomonas_meleagridis.PRJNA594289_edit_YL_YLparsed.txt Histomonas_meleagridis.PRJNA594289 Histomonas_meleagridis.PRJNA594289_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Kipferlia_bialata.PRJDB5223_edit.emap.emapper_PFam.txt Kipferlia_bialata.PRJDB5223_edit.emap.emapper_eggNOG.txt Kipferlia_bialata.PRJDB5223_DL_DeepLocP.txt Kipferlia_bialata.PRJDB5223_edit_summary_SignalP.txt Kipferlia_bialata.PRJDB5223_edit_summary_TargetP.txt Kipferlia_bialata.PRJDB5223_edit_StandardAA_IPRScan_final.txt Kipferlia_bialata.PRJDB5223_edit_StandardAA_nonM_30aa_fin_MFresults_MFparsed.txt Kipferlia_bialata.PRJDB5223_edit_YL_YLparsed.txt Kipferlia_bialata.PRJDB5223 Kipferlia_bialata.PRJDB5223_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Pentatrichomonas_hominis.5728.aa_edit.emap.emapper_PFam.txt Pentatrichomonas_hominis.5728.aa_edit.emap.emapper_eggNOG.txt P_hominis_DL_DeepLocP.txt Pentatrichomonas_hominis.5728.aa_edit_summary_SignalP.txt Pentatrichomonas_hominis.5728.aa_edit_summary_TargetP.txt Pentatrichomonas_hominis.5728.aa_edit_StandardAA_IPRScan_final.txt Pentatrichomonas_hominis.5728.aa_edit_StandardAA_nonM_MFresults_MFparsed.txt Pentatrichomonas_hominis.5728.aa_edit_YL_YLparsed_final.txt Pentatrichomonas_hominis.5728.aa Pentatrichomonas_hominis.5728.aa_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py SC_newprots_may21.anaeromoeba_edit.emap.emapper_PFam.txt SC_newprots_may21.anaeromoeba_edit.emap.emapper_eggNOG.txt SC_anaeromoeba_DL_DeepLocP.txt SC_newprots_may21.anaeromoeba_edit_summary_SignalP.txt SC_newprots_may21.anaeromoeba_edit_summary_TargetP.txt SC_newprots_may21.anaeromoeba_edit_StandardAA_IPRScan_final.txt SC_newprots_may21.anaeromoeba_edit_StandardAA_nonM_MFresults_MFparsed.txt SC_newprots_may21.anaeromoeba_edit_YL_YLparsed.txt SC_newprots_may21.anaeromoeba SC_newprots_may21.anaeromoeba_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Spironucleus_salmonicida.PRJNA60811_edit.emap.emapper_PFam.txt Spironucleus_salmonicida.PRJNA60811_edit.emap.emapper_eggNOG.txt Spironucleus_salmonicida.PRJNA60811_DL_DeepLocP.txt Spironucleus_salmonicida.PRJNA60811_edit_summary_SignalP.txt Spironucleus_salmonicida.PRJNA60811_edit_summary_TargetP.txt Spironucleus_salmonicida.PRJNA60811_edit_StandardAA_IPRScan.txt Spironucleus_salmonicida.PRJNA60811_edit_StandardAA_nonM_MFresults_MFparsed.txt Spironucleus_salmonicida.PRJNA60811_edit_YL_YLparsed.txt Spironucleus_salmonicida.PRJNA60811 Spironucleus_salmonicida.PRJNA60811_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Tetratrichomonas_gallinarum.5730.aa_edit.emap.emapper_PFam.txt Tetratrichomonas_gallinarum.5730.aa_edit.emap.emapper_eggNOG.txt T_gallinarum_DL_DeepLocP.txt Tetratrichomonas_gallinarum.5730.aa_edit_summary_SignalP.txt Tetratrichomonas_gallinarum.5730.aa_edit_summary_TargetP_final.txt Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA_IPRScan_final.txt Tetratrichomonas_gallinarum.5730.aa_edit_StandardAA_nonM_MFresults_MFparsed.txt Tetratrichomonas_gallinarum.5730.aa_YL_YLparsed_final.txt Tetratrichomonas_gallinarum.5730.aa Tetratrichomonas_gallinarum.5730.aa_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Trichomonas_foetus.PRJNA345179_edit.emap.emapper_PFam.txt Trichomonas_foetus.PRJNA345179_edit.emap.emapper_eggNOG.txt T_foetus_DL_DeepLocP.txt Trichomonas_foetus.PRJNA345179_edit_summary_SignalP.txt Trichomonas_foetus.PRJNA345179_edit_summary_TargetP.txt Trichomonas_foetus.PRJNA345179_edit_StandardAA_IPRScan_final.txt Trichomonas_foetus.PRJNA345179_edit_StandardAA_nonM_30aa_fin_MFresults_MFparsed.txt Trichomonas_foetus.PRJNA345179_edit_YL_YLparsed.txt Tritrichomonas_foetus.PRJNA345179 Tritrichomonas_foetus.PRJNA345179_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Trichomonas_vaginalis_GenBank.PRJNA16084_edit.emap.emapper_PFam.txt Trichomonas_vaginalis_GenBank.PRJNA16084_edit.emap.emapper_eggNOG.txt T_vaginalis_GB_DL_DeepLocP.txt Trichomonas_vaginalis_GenBank.PRJNA16084_edit_summary_SignalP.txt Trichomonas_vaginalis_GenBank.PRJNA16084_summary_TargetP_final.txt Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_IPRScan.txt Trichomonas_vaginalis_GenBank.PRJNA16084_edit_StandardAA_nonM_30aa_fin_MFresults_MFparsed.txt Trichomonas_vaginalis_GenBank.PRJNA16084_YL_YLparsed_final.txt Trichomonas_vaginalis_GenBank.PRJNA16084 Trichomonas_vaginalis_GenBank.PRJNA16084_predDB.txt
python ../../../Scripts/combo_OG_results__v3.py Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper_PFam.txt Trichomonas_vaginalis_RefSeq.G3_edit.emap.emapper_eggNOG.txt T_vaginalis_RS_DL_DeepLocP.txt Trichomonas_vaginalis_RefSeq.G3_edit_summary_SignalP.txt Trichomonas_vaginalis_RefSeq.G3_summary_TargetP_final.txt Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_IPRScan.txt Trichomonas_vaginalis_RefSeq.G3_edit_StandardAA_nonM_30aa_fin_MFresults_MFparsed.txt Trichomonas_vaginalis_RefSeq.G3_YL_YLparsed_final.txt Trichomonas_vaginalis_RefSeq.G3 Trichomonas_vaginalis_RefSeq.G3_predDB.txt
###
#A. lanta integration
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Species_DBs/ directories
#re-ran all of the above, as well as the A. lanta: 
python ../../../Scripts/combo_OG_results__v3.py Anaeramoeba_lanta_160522_edit.emap.emapper_PFam.txt Anaeramoeba_lanta_160522_edit.emap.emapper_eggNOG.txt Anaeramoeba_lanta_160522_DL_DeepLocP.txt Anaeramoeba_lanta_160522_edit_summary_SignalP.txt Anaeramoeba_lanta_160522_edit_summary_TargetP.txt Anaeramoeba_lanta_160522_edit_StandardAA_IPRScan.txt Anaeramoeba_lanta_160522_edit_StandardAA_nonM_30aa_fin_MFresults_MFparsed.txt Anaeramoeba_lanta_160522_edit_YL_YLparsed.txt Anaeramoeba_lanta_160522 Anaeramoeba_lanta_160522_predDB.txt

```

Compile the species databases into one large flat database: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/Species_DBs/ directory
#create temp versions of the files without column headers
ls */*_predDB.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
#create a file with column headers
head -1 BM_newprots_may21.anaeromoeba/BM_newprots_may21.anaeromoeba_predDB.txt > large_db_columns.txt
#clean up
mv *.txt large_DB_construct/
#concatenate the files together
cat large_DB_construct/large_db_columns.txt large_DB_construct/*_temp.txt > Metamonada_predDB.txt
###
#A. lanta integration
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Species_DBs/ directory
ls */*_predDB.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
#create a file with column headers
head -1 BM_newprots_may21.anaeromoeba/BM_newprots_may21.anaeromoeba_predDB.txt > large_db_columns.txt
#clean up
mkdir large_DB_construct/
mv *.txt large_DB_construct/
#concatenate the files together
cat large_DB_construct/large_db_columns.txt large_DB_construct/*_temp.txt > Metamonada_Alanta_predDB.txt
#before we move on, some quick, basic checks
wc -l Metamonada_Alanta_predDB.txt
# 556048 Metamonada_Alanta_predDB.txt
#which means 556047 actual lines of protein data
wc -l Metamonada_pred_OG_DB.txt
# 541206 Metamonada_pred_OG_DB.txt
#which means 541205 actual lines of protein data
grep -c ">" Anaeramoeba_lanta_160522_edit.fasta
# 14842
#556047-14842=541205
#thank fuck, that matches up
#so we can move on!!!

```

And, finally!!! Adding the OG information into the Metamonad database (script saved to prot_DB_plus_OGs.py)!!!

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: prot_DB_plus_OGs.py
Date: 2022-03-19
Author: Virág Varga

Description:
	This program conbines the data from the large predicted protein function database with
		the parsed results files of the  broccoli_Parser.py, proteinOrtho_Parser.py, 
		orthoFinder_Parser.py and sonicParanoid_Parser.py scripts. 
		The data in these parsed files is concatenated into one large, flat database.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	functools.reduce

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas import the contents of the Broccoli, ProteinOrtho, OrthoFinder and
		SonicParanoid parsed results files.
	3. Cpncatenating the databases and filling NaN values with "-".
	4. Writing out the results to a tab-delimited text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of the parsed files created thorugh the use of
		broccoli_Parser.py, proteinOrtho_Parser.py, orthoFinder_Parser.py and sonicParanoid_Parser.py
		parser programs.
	- All input and output files are user-defined: This means the user must ensure that
		the correct file names have been assigned to the program.

Usage
	./prot_DB_plus_OGs.py Prot_DB Broccoli_Parsed ProteinOrtho_Parsed OrthoFinder_Parsed SonicParanoid_Parsed Output_DB
	OR
	python prot_DB_plus_OGs.py Prot_DB Broccoli_Parsed ProteinOrtho_Parsed OrthoFinder_Parsed SonicParanoid_Parsed Output_DB

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
from functools import reduce #allow same function to be applied iteratively


#assign command line arguments: input & output files
#input parsed OG files
Prot_DB = sys.argv[1]
Broccoli_Parsed = sys.argv[2]
ProteinOrtho_Parsed = sys.argv[3]
OrthoFinder_Parsed = sys.argv[4]
SonicParanoid_Parsed = sys.argv[5]
#output final database
Output_DB = sys.argv[6]


#Part 2: Importing parsed data file contents into Pandas dataframes

#Large protein functional annotation database
with open(Prot_DB, "r") as prot_infile:
	#open the protein functional prediction data file
	prot_df = pd.read_csv(prot_infile, sep='\t', header = 0, low_memory=False)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	prot_df = prot_df.set_index('Query')
	#set the first column (containing query sequence names) as an index

#Broccoli results
with open(Broccoli_Parsed, "r") as broccoli_infile:
	#open the parsed Broccoli data file
	broccoli_df = pd.read_csv(broccoli_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	broccoli_df = broccoli_df.set_index('Query')
	#set the first column (containing query sequence names) as an index

#ProteinOrtho results
with open(ProteinOrtho_Parsed, "r") as proteinortho_infile:
	#open the parsed ProteinOrtho data file
	proteinortho_df = pd.read_csv(proteinortho_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	proteinortho_df.drop(proteinortho_df.columns[1], axis=1, inplace=True)
	#drop the species column of the dataframe
	proteinortho_df = proteinortho_df.set_index('Query')
	#set the first column (containing query sequence names) as an index

#OrthoFinder results
with open(OrthoFinder_Parsed, "r") as orthofinder_infile:
	#open the parsed OrthoFinder data file
	orthofinder_df = pd.read_csv(orthofinder_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	orthofinder_df.drop(orthofinder_df.columns[1], axis=1, inplace=True)
	#drop the species column of the dataframe
	orthofinder_df = orthofinder_df.set_index('Query')
	#set the first column (containing query sequence names) as an index

#SonicParanoid results
with open(SonicParanoid_Parsed, "r") as sonicparanoid_infile:
	#open the parsed SonicParanoid data file
	sonicparanoid_df = pd.read_csv(sonicparanoid_infile, sep='\t', header = 0)
	#read the file into a pandas dataframe
	#specifying that the file is tab-separated with a header line
	sonicparanoid_df.drop(sonicparanoid_df.columns[1], axis=1, inplace=True)
	#drop the species column of the dataframe
	sonicparanoid_df = sonicparanoid_df.set_index('Query')
	#set the first column (containing query sequence names) as an index


#Part 3: Merge the dataframes and write out

#ref: https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes
#save the dataframes to be merged into a list
data_frames = [prot_df, broccoli_df, proteinortho_df, orthofinder_df, sonicparanoid_df]
#merge the dataframes iteratively using reduce
merged_df = reduce(lambda  left,right: pd.merge(left,right,on=['Query'], how='outer'), data_frames).fillna('-')


#merged_df = pd.concat([prot_df, broccoli_df, proteinortho_df, orthofinder_df, sonicparanoid_df], axis=1).reset_index(inplace=True, drop=True)
#concatenate the dataframes along the x axis, horizontally
#merged_df.fillna('-', inplace=True)
#fill empty cells with string "-"


#write out results to tab-delimited text file
merged_df.to_csv(Output_DB, sep='\t', index=True)
#since the Query columns got shifted into indexes, need to use `index=True`

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python prot_DB_plus_OGs.py Prot_DB Broccoli_Parsed ProteinOrtho_Parsed OrthoFinder_Parsed SonicParanoid_Parsed Output_DB
#using it:
python ../Scripts/prot_DB_plus_OGs.py Species_DBs/Metamonada_predDB.txt OG_Data/Broccoli_OGs_parsed__Group-Drop.txt OG_Data/PO_OGs_parsed.txt OG_Data/OF_OGs_parsed.txt OG_Data/SP_OGs_parsed.txt Metamonada_pred_OG_DB.txt &
# [1] 4028229
#decided to background the program because it wasn't disconnecting properly - probably due to the massive file size
# [1]+  Done                    python ../Scripts/prot_DB_plus_OGs.py Species_DBs/Metamonada_predDB.txt OG_Data/Broccoli_OGs_parsed__Group-Drop.txt OG_Data/PO_OGs_parsed.txt OG_Data/OF_OGs_parsed.txt OG_Data/SP_OGs_parsed.txt Metamonada_pred_OG_DB.txt
###
#doing it again with the fixed database
python ../Scripts/prot_DB_plus_OGs.py Species_DBs/Metamonada_predDB.txt OG_Data/Broccoli_OGs_parsed__Group-Drop.txt OG_Data/PO_OGs_parsed.txt OG_Data/OF_OGs_parsed.txt OG_Data/SP_OGs_parsed.txt Metamonada_pred_OG_DB.txt &
# [1] 530338
# [1]+  Done                    python ../Scripts/prot_DB_plus_OGs.py Species_DBs/Metamonada_predDB.txt OG_Data/Broccoli_OGs_parsed__Group-Drop.txt OG_Data/PO_OGs_parsed.txt OG_Data/OF_OGs_parsed.txt OG_Data/SP_OGs_parsed.txt Metamonada_pred_OG_DB.txt
###
#A. lanta integration
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/OG_Data/ directory
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/OG_Programs/A_lanta_Run/Broccoli_OGs_parsed_Alanta.txt .
ln -s /home/inf-47-2020/ThesisTrich/PARSED_Results/OG_Programs/A_lanta_Run/PO_OGs_parsed_Alanta.txt .
python ../../Scripts/remove_duplicatesBR__ALL.py Broccoli_OGs_parsed_Alanta.txt Broccoli_OGs_parsed_Alanta__Group-Drop.txt
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/ drectory
python ../Scripts/prot_DB_plus_OGs.py Species_DBs/Metamonada_Alanta_predDB.txt OG_Data/Broccoli_OGs_parsed_Alanta__Group-Drop.txt OG_Data/PO_OGs_parsed_Alanta.txt OG_Data/OF_OGs_parsed_Alanta.txt OG_Data/SP_OGs_parsed_Alanta.txt Metamonada_Alanta_pred_OG_DB.txt &
# [1] 1838685
# [1]+  Done                    python ../Scripts/prot_DB_plus_OGs.py Species_DBs/Metamonada_Alanta_predDB.txt OG_Data/Broccoli_OGs_parsed_Alanta__Group-Drop.txt OG_Data/PO_OGs_parsed_Alanta.txt OG_Data/OF_OGs_parsed_Alanta.txt OG_Data/SP_OGs_parsed_Alanta.txt Metamonada_Alanta_pred_OG_DB.txt
wc -l Metamonada_Alanta_pred_OG_DB.txt
# 556048 Metamonada_Alanta_pred_OG_DB.txt
#ok, ready to move on! 

```


## OG Program Comparisons

### Checking for duplicates in OG prediction programs

Orthologous clustering programs tend to group proteins into 1 cluster each, but a script was written (saved to check_OG_duplicates.py) to make sure that this was indeed the case.

After running this program on the parsed results of all 4 software, it was discovered that Broccoli actually does occasionally sort proteins into more than 1 cluster. The other 3 software did not do this.

```python
#!/bin/python
"""

Title: check_OG_duplicates.py
Date: 2022-03-02
Author: Virág Varga

Description:
	This program checks a file to see whether elements of the first column
		(Pythonic index 0; must be named "Query") are repeated.
	It was written to determine whether orthologous clustering programs were
		clustering proteins into more than one orthologous group.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	itertools

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the ortholog_groups.tsv file into a
		dataframe.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a file with query IDs in the first column.

Usage
	./check_OG_duplicates.py input_db
	OR
	python check_OG_duplicates.py input_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part1: Assign command-line arguments, import modules

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Broccoli_OGs_parsed.txt"
#input_db = "OF_OGs_parsed.txt"
#input_db = "SP_OGs_parsed.txt"
#input_db = "PO_OGs_parsed.txt"
#input_db = "encoding_summary_ref.txt"

#used the above to queck the query ids themselves weren't replicated


#Part 2: Check for duplicates & report results

#import input database into a Pandas dataframe
ortho_df = pd.read_csv(input_db, sep = '\t', header = 0)

#send query colummn to list
query_list = ortho_df.iloc[:, 0].to_list()


#check if list members are entirely unique
if(len(set(query_list)) == len(query_list)):
	print(input_db + "___ results DO NOT have duplicates among the queries!")
else:
	print(input_db + " results HAVE duplicates among the queries!")
	copy_ortho_df = ortho_df.iloc[:, [0, -1]].copy()
	#the script works up until here
	#I'm able to make a dataframe with the line above that includes only the OG numbers & query IDs
	duplicate_df = pd.concat(g for _, g in copy_ortho_df.groupby("Query") if len(g) > 1)
	base = os.path.basename(input_db)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_DUPLICATES.txt"
	duplicate_df.to_csv(output_file, sep='\t', index=False)

```

### Basic statistics

Calculating some summary statistics:
	- Average number of proteins per cluster
	- 10 largest & 10 smallest clusters, with how many proteins are in these.
	- How many clusters are there (per program)?
	- Are there orthologous groups with only 1 protein? If so, how many?

Answers to the exploratory questions above were printed out to a results file for each program.

The calculations were done using the script below (saved to og_stats__v2.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""
Title: og_stats__v2.py
Date: 2022-03-15
Author: Virág Varga

Description:
	This program performs basic statistical tests on the parsed results of orthologous
		clustering software (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli).

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	os
	statistics
	numpy
	matplotlib.pyplot
	check_og_duplicates.py

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Importing modules and parsing arguments. Running code specific to individual
		arguments.
			If -dupl argument is used, check_og_duplicates.py module will be called
			and run immediately.
	3. Main program code:
		- If necessary, removing species column from dataframe
		- Importing OG data into dictionary with format ortho_dict[OG_ID] = list_of_queries
		- Creating additional dictionary with format og_size_dict[OG_ID] = number_of_queries
		- Identifying number of OGs created by program
		- Identifying largest and smallest OGs (initially search for 10)
		- Identifying OGs with only 1 protein
		- Creating histogram showing distribustion of OG sizes
	4. Writing out results to text file.

Known bugs and limitations:
	- IMPORTANT: The location of the check_og_duplicates.py module is HARDCODED into
		this script! If the files structure of the user is different, then they will
		need to alter that portion of the -dupl argument parsing script.
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The input file used for the program must be an un-pivoted results file of either
		Broccoli, OrthoFinder, SonicParanoid, or ProteinOrtho, following the structure
		used by the parsers used previously in this workflow.
	- The program cannot accept multiple input files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Version:
	This is version 2.0 of the program. The following changes were make to the script,
		to increase ease of program use:
		- The output file names were made reliant on input file names - this allows
			the script to be run in the same directory on different parsed OG files
			orginating from the same program, without overwriting results or requiring
			the names of results files to be manually altered.
		- Made the script more flexible re: number of columns. Now filtered parsed data
			can still be processed, even if the formatting does not match the original
			parsed OG data file.
		- Functionality was added to print a histogram of OG sizes to an image file.

Usage:
	./og_stats__v2.py [-h] [-br] [-of] [-po] [-sp] [-dupl] [-v] INPUT_FILE
	OR
	python og_stats__v2.py [-h] [-br] [-of] [-po] [-sp] [-dupl] [-v] INPUT_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""

#################################   ARGPARSE   #######################################
import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


parser = argparse.ArgumentParser(description =
								 'This program performs basic statistical tests on the parsed results of orthologous \
									clustering software (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli).')
#The most general description of what this program can do is defined here


#adding the arguments that the program can use
parser.add_argument(
	'-br', '--Broccoli',
	action='store_true',
	help = 'This argument will parse the results of the Broccoli program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed Broccoli results
parser.add_argument(
	'-of', '--OrthoFinder',
	action='store_true',
	help = 'This argument will parse the results of the OrthoFinder program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed OrthoFinder results
parser.add_argument(
	'-po', '--ProteinOrtho',
	action='store_true',
	help = 'This argument will parse the results of the ProteinOrtho program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed ProteinOrtho results
parser.add_argument(
	'-sp', '--SonicParanoid',
	action='store_true',
	help = 'This argument will parse the results of the SonicParanoid program.'
	)
	#the '-sp' flag will import the input file in the manner appropriate for the parsed SonicParanoid results
parser.add_argument(
	'-dupl', '--Duplicates',
	action='store_true',
	help = 'This argument will call and execute the check_OG_duplicates.py module program.'
	)
	#the '-dupl' flag will check the parsed results file for duplicates using the check_OG_duplicates.py script module
parser.add_argument(
	#'-i', '--input',
	#the above line of code is left in as further clarification of this argument
	dest='input_file',
	metavar='INPUT_FILE',
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 2.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import os #allows access to the file system
import statistics #allows calculation of statistics in Python
import numpy as np #allows straightforward manipulation of dataframes in Python
import matplotlib.pyplot as plt #enable plotting in Python


#designate input file name as variable
infile = args.input_file.name


#parse arguments
if args.Broccoli:
	#if -br argument is called
	print("Broccoli results recieved")
	#save the input program ID for use in the output file
	prog_id = "Broccoli"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	og_stats_outfile = out_full + "__OG_stats.txt"

if args.OrthoFinder:
	#if -of argument is called
	print("OrthoFinder results recieved")
	#save the input program ID for use in the output file
	prog_id = "OrthoFinder"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	og_stats_outfile = out_full + "__OG_stats.txt"

if args.ProteinOrtho:
	#if -tp argument is called
	print("ProteinOrtho results recieved")
	#save the input program ID for use in the output file
	prog_id = "ProteinOrtho"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	og_stats_outfile = out_full + "__OG_stats.txt"

if args.SonicParanoid:
	#if -en argument is called
	print("SonicParanoid results recieved")
	#save the input program ID for use in the output file
	prog_id = "SonicParanoid"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	og_stats_outfile = out_full + "__OG_stats.txt"

if args.Duplicates:
	#if -dupl argument is called
	cmd = "python ../../Scripts/check_OG_duplicates.py {0}".format(infile)
	#since the `os.system()` method of calling a python script requires an input string
	#the command that would be used to run the yLoc_Parser.py program on its own
	#is formatted into a string together with the name of the input file
	#this is then passed as a command for execution
	os.system(cmd)


#################################   Main Program   ######################################


#Initial setup

#remove 3rd column from Pandas dataframe if necessary
#needed for original parsed results of OrthoFinder, ProteinOrtho & SonicParanoid
col_num = len(ortho_df.columns)
#count number of columns
if col_num == 3:
	#select dataframes with 3 columns
	#remove middle column with species information
	ortho_df.drop(ortho_df.columns[1], axis=1, inplace=True)


#identify OG column name (for use later)
og_col = ortho_df.columns[1]

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#How many clusters are there (per program)?

og_number = ortho_df[og_col].nunique()
#nunique() counts the number of unique occurrences in the selected column


#Average number of proteins per cluster

#create an empty dictionary to populate with the length data
og_size_dict = {}

for key in ortho_dict.keys():
	#iterate through the ortho_dict dictionary by its keys
	length_og = len(ortho_dict[key])
	#save the length of each list of proteins (per OG) to variable length_og
	og_size_dict[key] = length_og
	#populate the og_size_dict dictionary using the OG IDs as the keys and the OG sizes as values

#save the lengths of the OGs to a list
#note that OG ID information is no longer attached to this data
og_size = list(og_size_dict.values())

#calculate the average OG size, and save the value to variable avg_prots_num
avg_prots_num = statistics.mean(og_size)
#calculate the median OG size, and save the value to variable med_prots_num
med_prots_num = statistics.median(og_size)


#10 largest & 10 smallest clusters, with how many proteins are in these.

#get the 10 largest OGs in the og_size list
#ref: https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list
top_10_idx = np.argsort(og_size)[-10:]
#save the indices of those sizes to a variable
top_10_values = [og_size[i] for i in top_10_idx]
#get the actual values that those indices correspond to out of the og_size list

#ensure no duplicate values
set_top_10_values = set(top_10_values)

#create an empty list to hold the OG IDs of the largest OGs
largest_OGs = []

#loop through values of the og_size_dict dictionary to find the OGs that correspond to those sizes
for key, value in og_size_dict.items():
	#loop through the items in the dictionary by both keys & values
	for size in set_top_10_values:
		#loop through the members of the top_10_values list showing largest OG sizes
		if value == size:
			#if the OG length matches in the dictionary and the list
			#append the key associated with that value (ie. Query ID) to the largest_OGs list
			largest_OGs.append(key)


#get the 10 smallest OGs in the og_size list
bottom_10_idx = np.argsort(og_size)[:10]
#save the indices of those sizes to a variable
bottom_10_values = [og_size[i] for i in bottom_10_idx]

#ensure no duplicate values
set_bottom_10_values = set(bottom_10_values)

#create an empty list to hold the OG IDs of the smallest OGs
smallest_OGs = []

#loop through values of the og_size_dict dictionary to find the OGs that correspond to those sizes
for key, value in og_size_dict.items():
	#loop through the items in the dictionary by both keys & values
	for size in set_bottom_10_values:
		#loop through the members of the top_10_values list showing largest OG sizes
		if value == size:
			#if the OG length matches in the dictionary and the list
			#append the key associated with that value (ie. Query ID) to the largest_OGs list
			smallest_OGs.append(key)


#Are there orthologous groups with only 1 protein? If so, how many?

#create empty list for protein query IDs in unique OG
lonely_OGs = []

#look through set_bottom_10_values to see if any of them has a length of 1
if 1 in set_bottom_10_values:
	#if the number 1 is found in the set
	for key, value in og_size_dict.items():
		#loop thorugh the og_size_dictionary to find the places where the value is 1
		if value == 1:
			#where the size of the OG is 1 protein query, save the query ID to a list
			lonely_OGs.append(key)


#Create histogram showing distribustion of OG sizes

#create an empty list to store OG sizes
size_list = []

#loop over the OG size dictionary to extract the OG sizes
for key in og_size_dict.keys():
	#iterate over the OG size dictionary via its keys
	#and save the associated values to the size list
	size_list.append(og_size_dict[key])

#create a histogram from the OG size data
og_size_hist = plt.hist(size_list, bins = 200)

#define the x-axis label of the plot
plt.xlabel("Orthologous Cluster Size (integers)")
#define the y-axis label of the plot
plt.ylabel("Frequency of Cluster Size")
#define the title of the plot
plt.title("Distribution of Orthologous Cluster Sizes")

#use input file name as basis for histogram output file name
histogram_name = out_full + "__OG_stats_histogram.png"
#print resulting histogram to png file
plt.savefig(histogram_name, dpi=500)
#plt.show()


#Writing out the results file

with open(og_stats_outfile, "w") as outfile:
	#open the assigned outfile for writing
	#first, calrify the program that the data comes from (so the information is inside the file, too)
	outfile.write(prog_id + " parsed results file was given as input." + "\n" +
			   "The results of the analysis can be found below:" + "\n\n")
	#write out the number of OGs identified by the program
	outfile.write("The " + prog_id + " program generated " + str(og_number) + " orthologous clusters." + "\n")
	#write the average/mean and median sizes of the OGs identified by the program
	outfile.write("There were on average " + str(avg_prots_num) + " per orthologous cluster, with a median of " + str(med_prots_num) + "." + "\n")
	if len(largest_OGs) == 10:
		#write out the 10 largest OGs and their members
		outfile.write("The largest OG had " + str(top_10_values[9]) + " members." + "\n" +
				"The 10 largest clusters, their length and protein members are as follows: " + "\n")
	if len(largest_OGs) > 10:
		#write out the 10 largest OGs and their members
		outfile.write("The largest OG had " + str(top_10_values[9]) + " members." + "\n" +
				"The " + str(len(largest_OGs)) + " largest clusters, their length and protein members are included below." + "\n" +
				"Note that this program searches initially for the 10 largest OGs. More are reported here because multiple OGs had a length equivalent to the 'largest'." + "\n")
	for key in ortho_dict.keys():
		#looping through the ortho_dict dictionary keys
		for cluster in largest_OGs:
			#looping through the OG IDs of the largest clusters
			if cluster == key:
				#when a member of the list matches a dictionary key
				#convert the list in the dictionary value of query IDs into a string
				joined_OG_list = ",".join(ortho_dict[key])
				#write out the OG ID as well as the protein query IDs associated with it to the outfile
				outfile.write(cluster + "\t" + "size = " + str(len(ortho_dict[key])) + "\n\t" + joined_OG_list + "\n")
	if len(smallest_OGs) == 10:
		#write out the 10 smallest OGs and their members
		outfile.write("The smallest OG had " + str(bottom_10_values[0]) + " members." + "\n" +
				"The 10 smallest clusters, their length and protein members are as follows: " + "\n")
	if len(smallest_OGs) > 10:
		#write out the 10 smallest OGs and their members
		outfile.write("The smallest OG had " + str(bottom_10_values[0]) + " members." + "\n" +
				"The " + str(len(smallest_OGs)) + " smallest clusters, their length and protein members are included below." + "\n" +
				"Note that this program searches initially for the 10 smallest OGs. More are reported here because multiple OGs had a length equivalent to the 'smallest'." + "\n")
	for key in ortho_dict.keys():
		#looping through the ortho_dict dictionary keys
		for cluster in smallest_OGs:
			#looping through the OG IDs of the smallest clusters
			if cluster == key:
				#when a member of the list matches a dictionary key
				#convert the list in the dictionary value of query IDs into a string
				joined_OG_list = ",".join(ortho_dict[key])
				#write out the OG ID as well as the protein query IDs associated with it to the outfile
				outfile.write(cluster + "\t" + "size = " + str(len(ortho_dict[key])) + "\n\t" + joined_OG_list + "\n")
	outfile.write("There are " + str(len(lonely_OGs)) + " orthologous clusters with only 1 protein." + "\n")
	if len(lonely_OGs) > 0:
		#if there are any OGs with only 1 protein member, then enter this loop
		#write out the OG IDs, as well as the protein query IDs of these "clusters"
		outfile.write("The orthologous clusters with only one member and their associated member protein are: " + "\n")
		for key in ortho_dict.keys():
			#loop through the keys of the ortho_dict dictionary
			for element in lonely_OGs:
				#loop through the members of the lonely_OGs list of OG IDs
				if element == key:
					#when a member of the lonely_OGs list matches a key in the ortho_dict dictionary
					#convert the list in the dictionary value of the query ID into a string
					joined_OG_list = "".join(ortho_dict[key])
					#write out the OG ID as well as the associated protein query ID
					outfile.write(element + ": " + joined_OG_list + "\n")

```

Using it:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model:
python og_stats.py [-h] [-br] [-of] [-po] [-sp] [-dupl] [-v] INPUT_FILE
#applying it:
python ../../Scripts/og_stats.py -h
# usage: og_stats.py [-h] [-br] [-of] [-po] [-sp] [-dup] [-v] INPUT_FILE
# This program performs basic statistical tests on the parsed results of orthologous clustering
# software (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli).
# positional arguments:
#   INPUT_FILE
# optional arguments:
#   -h, --help            show this help message and exit
#   -br, --Broccoli       This argument will parse the results of the Broccoli program.
#   -of, --OrthoFinder    This argument will parse the results of the OrthoFinder program.
#   -po, --ProteinOrtho   This argument will parse the results of the ProteinOrtho program.
#   -sp, --SonicParanoid  This argument will parse the results of the SonicParanoid program.
#   -dupl, --Duplicates    This argument will call and execute the check_OG_duplicates.py module
#                         program.
#   -v, --version         show program's version number and exit
python ../../Scripts/og_stats.py -of -dupl OF_OGs_parsed.txt
# OrthoFinder results recieved
# OF_OGs_parsed.txt___ results DO NOT have duplicates among the queries!
#formatting is a bit weird for the duplicates test, but the answer is correct, so I'm leaving it at that!
python ../../Scripts/og_stats.py -sp SP_OGs_parsed.txt
# SonicParanoid results recieved
python ../../Scripts/og_stats.py -br Broccoli_OGs_parsed.txt
# Broccoli results recieved
python ../../Scripts/og_stats.py -po PO_OGs_parsed.txt
# ProteinOrtho results recieved
#now transfer them back to my local computer to have a look

```

*Sidenote*: The results files are *massive* with the printouts of the protein query IDs associated with the smallest and largest files. A Version 2.0 might consider adding an `argparse` command-line option that allows a user to decide whether they want a "long" (ie. with protein query IDs) or "short" (ie. only including summary statistics) outfile. It would also be useful to find a way around the hardcoding issue for the check_og_duplicates.py module. 

#### So let's talk about those summary stats

Quick hits:
   - Broccoli:
      - **Reminder**: Broccoli is the *only* one of the clustering programs that is willing to put protein queries in more than one OG.
      - Number of OGs: 39854
      - Average OG size: 10.145
      - Median OG size: 2.0
      - Size of largest OG: 14218
      - Size of smallest OG: 2
      - Any 1-protien OGs? 0
   - OrthoFinder:
      - Number of OGs: 77562
      - Average OG size: 6.026
      - Median OG size: 2.0
      - Size of largest OG: 2727
      - Size of smallest OG: 2
      - Any 1-protien OGs? 0
   - ProteinOrtho:
      - Number of OGs: 94951
      - Average OG size: 3.545
      - Median OG size: 2.0
      - Size of largest OG: 209
      - Size of smallest OG: 2
      - Any 1-protien OGs? 0
   - SonicParanoid:
      - Number of OGs: 67234
      - Average OG size: 5.538
      - Median OG size: 2.0
      - Size of largest OG: 806
      - Size of smallest OG: 2
      - Any 1-protien OGs? 0

So what does this all mean?
   - None of the programs produced OGs with only 1 protein, so that's good
   - Broccoli's clusters have the highest average, but Broccoli also had duplicates, so that number is less reliable than would be ideal
   - OrthoFinder & SonicParanoid seem to have the most similar results: OG number with a difference of only 10k (vs. +/-20k with the others), have a similar average OG size (within 0.5 proteins). Their largest OG size is less similar than would be preferred - this statistic puts ProteinOrtho closest to SonicParanoid.
   - All of the programs have a median OG size of 2. This might be a point to focus in on:
      - Filtration possibility: rerunning these statistics after filtering out OGs below a threshold of n potein query members (3? Can't make it too high, since the lowest average is ProteinOrtho's 3.545)
      - For programs that keep species information (ie. all except Broccoli) when clustering, checking if those "orthologs" are actually paralogs (ie. from the same species), and if so, excluding them from the analysis, before running the statistics again. In fact, this might be worth doing on a broader scale, not just for OGs with 2 protein members.
   - So far, to my eye, OrthoFinder or SonicParanoid seem like the best bet moving forward [written 13.57 08.03.2022 - may change after advisor meeting on 09.03.2022]

Based on the conversation with Courtney and Julie on 09.03.2022: Need to filter out the clusters that contain only 2 proteins. This will be done through a few approaches:
 - Paralog filtration:
   - Test the contents of OGs to see if the proteins are coming from the same organism.
   - Need to keep in mind that there are duplicates: _T. vaginalis_, _H. meleagridis_, within assemblages of _G. intestinalis_
   - I think the most efficient way to go about this is to create a new dataframe that should be used as a supplementary input (beside the parsed OG databases), where the following information is displayed: protein ID, "official" species designation, categorical species designation
     - This would also be helpful for successive filtrations
     - Courtney suggested, based on the outcome of the paralog filtration, to later try other filtrations, as well (ex. 4 phyla, or even more shallow groups)
     - We can add this sort of information into this database quite easily
     - In fact, let's go ahead and add the phylum information into the initial script, just in case
     - I will need the encoding_summary_ref.txt file as input for this script
 - Will also need a script that simply filters out OGs with only 2 proteins
 - Core orthologs:
   - Courtney suggested I start looking for core orthologs (ie. orthologs that occur within n% of the species)
     - Should definitely do 100%, but then perhaps try 80%?
     - The protein ID to species to phylum database could be helpful for this filtration, too

The scripts that accomplish these tasks can be found in the header below (OG database filtration methods). Once those filtration processes have been run, the summary statistics from above will be rerun on the results.

Running the statistics on various iterations of the OG databases:

```bash
#running the originals with the new version of the script
python ../../Scripts/og_stats__v2.py -of -dupl OF_OGs_parsed.txt
# OrthoFinder results recieved
# OF_OGs_parsed.txt___ results DO NOT have duplicates among the queries!
#formatting is a bit weird for the duplicates test, but the answer is correct, so I'm leaving it at that!
python ../../Scripts/og_stats__v2.py -sp SP_OGs_parsed.txt
# SonicParanoid results recieved
python ../../Scripts/og_stats__v2.py -br Broccoli_OGs_parsed.txt
# Broccoli results recieved
python ../../Scripts/og_stats__v2.py -po PO_OGs_parsed.txt
# ProteinOrtho results recieved
###
# NOTE!!!
# can view histograms with Xming & PutTY x-11 forwarding, then command (ex.)
eog OF_OGs_parsed__OG_stats_histogram.png
###
#paralogs filtered out
python ../../Scripts/og_stats__v2.py -of OF_OGs_parsed_nonParalog.txt
# OrthoFinder results recieved
python ../../Scripts/og_stats__v2.py -sp SP_OGs_parsed_nonParalog.txt
# SonicParanoid results recieved
python ../../Scripts/og_stats__v2.py -br Broccoli_OGs_parsed_nonParalog.txt
# Broccoli results recieved
python ../../Scripts/og_stats__v2.py -po PO_OGs_parsed_nonParalog.txt
# ProteinOrtho results recieved
#OGs with only 2 proteins filtered out
python ../../Scripts/og_stats__v2.py -of OF_OGs_parsed_threshold3.txt
# OrthoFinder results recieved
python ../../Scripts/og_stats__v2.py -sp SP_OGs_parsed_threshold3.txt
# SonicParanoid results recieved
python ../../Scripts/og_stats__v2.py -br Broccoli_OGs_parsed_threshold3.txt
# Broccoli results recieved
python ../../Scripts/og_stats__v2.py -po PO_OGs_parsed_threshold3.txt
# ProteinOrtho results recieved
#the most important one - most highly filtered
#paralogs filtered out, OGs with only 2 proteins filtered out
python ../../Scripts/og_stats__v2.py -of OF_OGs_parsed_nonParalog_threshold3.txt
# OrthoFinder results recieved
python ../../Scripts/og_stats__v2.py -sp SP_OGs_parsed_nonParalog_threshold3.txt
# SonicParanoid results recieved
python ../../Scripts/og_stats__v2.py -br Broccoli_OGs_parsed_nonParalog_threshold3.txt
# Broccoli results recieved
python ../../Scripts/og_stats__v2.py -po PO_OGs_parsed_nonParalog_threshold3.txt
# ProteinOrtho results recieved
###
#Broccoli duplicates removed
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate.txt
# Broccoli results recieved
###
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/Stats_FINAL/ directory
#OGs with 80% species membership
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_OGsMembership80.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate_OGsMembership80.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -of ../OF_OGs_parsed_OGsMembership80.txt
# OrthoFinder results recieved
python ../../../Scripts/og_stats__v2.py -po ../PO_OGs_parsed_OGsMembership80.txt
# ProteinOrtho results recieved
python ../../../Scripts/og_stats__v2.py -sp ../SP_OGs_parsed_OGsMembership80.txt
# SonicParanoid results recieved
#OGs with 100% species membership
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_OGsMembership100.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate_OGsMembership100.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -of ../OF_OGs_parsed_OGsMembership100.txt
# OrthoFinder results recieved
python ../../../Scripts/og_stats__v2.py -po ../PO_OGs_parsed_OGsMembership100.txt
# ProteinOrtho results recieved
python ../../../Scripts/og_stats__v2.py -sp ../SP_OGs_parsed_OGsMembership100.txt
# SonicParanoid results recieved
#minimum 4 phyla shared
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_min4Phyla.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate_min4Phyla.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -of ../OF_OGs_parsed_min4Phyla.txt
# OrthoFinder results recieved
python ../../../Scripts/og_stats__v2.py -po ../PO_OGs_parsed_min4Phyla.txt
# ProteinOrtho results recieved
python ../../../Scripts/og_stats__v2.py -sp ../SP_OGs_parsed_min4Phyla.txt
# SonicParanoid results recieved
#all 5 phyla
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_5Phyla.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate_5Phyla.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -of ../OF_OGs_parsed_5Phyla.txt
# OrthoFinder results recieved
python ../../../Scripts/og_stats__v2.py -po ../PO_OGs_parsed_5Phyla.txt
# ProteinOrtho results recieved
python ../../../Scripts/og_stats__v2.py -sp ../SP_OGs_parsed_5Phyla.txt
###
#at Courtney's request, lowering the threshold
#OGs with 40% species membership
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_OGsMembership40.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate_OGsMembership40.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -of ../OF_OGs_parsed_OGsMembership40.txt
# OrthoFinder results recieved
python ../../../Scripts/og_stats__v2.py -po ../PO_OGs_parsed_OGsMembership40.txt
# ProteinOrtho results recieved
python ../../../Scripts/og_stats__v2.py -sp ../SP_OGs_parsed_OGsMembership40.txt
# SonicParanoid results recieved
#OGs with 50% species membership
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_OGsMembership50.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -br ../Broccoli_OGs_parsed_nonDuplicate_OGsMembership50.txt
# Broccoli results recieved
python ../../../Scripts/og_stats__v2.py -of ../OF_OGs_parsed_OGsMembership50.txt
# OrthoFinder results recieved
python ../../../Scripts/og_stats__v2.py -po ../PO_OGs_parsed_OGsMembership50.txt
# ProteinOrtho results recieved
python ../../../Scripts/og_stats__v2.py -sp ../SP_OGs_parsed_OGsMembership50.txt
# SonicParanoid results recieved

```

Interpreting the results: 
 - ProteinOrtho seems to make far fewer clusters, and those clusters are significantly smaller than the other 3
 - Broccoli has results more similar to OrthoFinder & SonicParanoid, but I hesitate to use it because of the whole "chimeric proteins" mess
 - OrthoFinder vs. SonicParanoid:
   - For membership threshold comparisons, these two are quite similar, usually within 100-200 clusters of each other, and within ~20 proteins in range for cluster membership size
   - However, there is a trend: OrthoFinder has fewer clusters, but those clusters have higher protein query membership counts
   - Interestingly, OrthoFinder originally produced more clusters than SonicParanoid (prior to filtration), with a slightly higher membership count average (by ~1 protein)
   - The largest OG found by OrthoFinder is significantly larger than the largest found by SonicParanoid (2727 vs 809)
   - The median values are within ~5 proteins of each other (5 phyla), with OrthoFinder being slightly larger (52.5 vs 47)
   - The histograms (5 phyla) show both programs having a similar range of sizes for the majority of their OGs, and both have outliers (extremely large OGs). However, the range in size of outliers is much larger for OrthoFinder. 
   - In terms of overlap percentages: 
     - The best overlaps are found between OrthoFinder & SonicParanoid (average score value 52.59%; 38682 OGs meeting 80% similarity threshold)
     - SonicParanoid & ProteinOrtho also have a good similarity (average score value 51.76%; 41629 OGs meeting 80% similarity threshold), as do OrthoFinder & ProteinOrtho (average score value 51.06%; 40908 OGs meeting 80% similarity threshold)
   - SonicParanoid might be better?
     - The fact that after filtration, SonicParanoid has generally more OGs with slightly smaller average protein membership counts; may imply that SonicParanoid is better at teasing apart some of the superfamilies that exist in functional domains. 
     - Ex. The largest OG from OrthoFinder is all RAS and RAS-associated proteins. Maybe some of these are more efficiently categorized by SonicParanoid? 
     - On the other hand, not a good sign if proteins are *too* categorized, either. 
   - Going to proceed with using the data from *both* SonicParanoid and OrthoFinder. 

Looking at the 40/50 data: 
 - More of the SonicParanoid OGs meet the thresholds than OrthoFinder thresholds (with higher percentage of overall OGs remaining, too). But OrthoFinder's cluster sizes are larger. This is the same pattern we observed before. 
 - At 40%: 
   - 1923 OrthoFinder OGs (2.479% of total OGs), 2256 SonicParanoid OGs (3.355% of total OGs)
   - OrthoFinder average cluster size of 67.81 proteins, SonicParanoid average cluster size of 59.52 proteins
   - OrthoFinder median cluster size of 39 proteins, SonicParanoid median cluster size of 38 proteins
 - At 50%: 
   - 1432 OrthoFinder OGs (1.846% of total OGs), 1723 SonicParanoid OGs (2.563% of total OGs)
   - OrthoFinder average cluster size of 81.33 proteins, SonicParanoid average cluster size of 67.86 proteins
   - OrthoFinder median cluster size of 47 proteins, SonicParanoid median cluster size of 44 proteins


### OG database filtration methods

This script creates a database containing the protein query IDs, the associated "official" species names, the species categories, and the phyla (saved to categorize_prot_species.py):

Realized that in order to do this, I actually need an input file that has species information, with the encoding_summary_ref.txt file doesn't contain. So it looks like I'm going to need to make that file first, and I think bash is probably my best bet.

```bash
#working in the /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/ directory
ls *.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	#extract the sequence header
	awk 'sub(/^>/, "")' $file > ${file_base}_prots.txt;
done
#this created files with the the protein headers (all in new lines)
ls *_prots.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	#extract the sequence header
	awk -v species_name="$file_base" -F "\t" 'BEGIN {print $0, file_base}' $file > ${file_base}_species.txt;
done
#this is very much Not Working
#why do I even bother with bash, this language is a nightmare -_-
#I'll just do it with Python
ls *_prots.txt | while read file; do
	python add_species.py $file;
done
#and now we can just cat the files together
cat *_species.txt > species_prots_ref.txt

```

Python script to add the species designations as a second column (saved to add_species.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: add_species.py
Date: 2022-03-02
Author: Virág Varga

Description:
	This program adds the species name (extracted from the file name) to the
		second column of a file containing nothing but a list of protein query IDs.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the input file into a
		dataframe.
	3. Adding the species identifier to the dataframe as a new column, and writing
		out the result to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-determined.

Usage
	./add_species.py input_file
	OR
	python add_species.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part1: Assign command-line arguments, import modules

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line arguments; load input and output files
input_file = sys.argv[1]
#input_file = "EP00771_Trimastix_marina_edit_prots.txt"
#define the output file based on the input file name
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_species.txt"


#Part 2: Import into Pandas and add new species designation column

#import input database into a Pandas dataframe
prot_df = pd.read_csv(input_file, header = None)

#extract species designation
species = out_full.replace('_edit_prots', '')
#create new column filled with species designation
prot_df[len(prot_df.columns)] = species

#write out the resulting dataframe
prot_df.to_csv(output_file, sep='\t', index=False, header=False)

```

Now the actual categorization script:
 - **NOTE**: The _G. intestinalis_ genome available from EukProt is categorized on NCBI as _G. lamblia_ (https://www.ncbi.nlm.nih.gov/assembly/GCF_000002435.1/). After some digging, I found that _G. lamblia_ is a category name for both _G. intestinalis_ and _G. duodenalis_. That this genome is categorized on EukProt as _G. intestinalis_, I take to mean that further study of the genome elucidated the specific species origins of the assembly. This conclusion is further underscored by both genomes having the isolate ID of "WB C6" (ie. isolate WB from assemblage A of _G. intestinalis_).
 - Ref: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC88984/

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: categorize_prot_species.py
Date: 2022-03-12
Author: Virág Varga

Description:
	This program creates a database which includes all protein query IDs, the
		"official" species/strain designation, a categorized species/strain/assemblage
		designation, and the phylum that the species belongs to.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the query protein and species information-containing
		file into a dataframe.
	3. Creating species category and phylum information dictionaries.
	4. Adding the species category and phylum information into the dataframe.
	5. Writing out the dataframe to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a file with query IDs in the first column,
		and the species/strain IDs (derived from proteome file names) in the second column.

Usage
	./categorize_prot_species.py input_db output_db
	OR
	python categorize_prot_species.py input_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part1: Assign command-line arguments, import modules

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "species_prots_ref.txt"
output_db = sys.argv[2]
#output_db = "prots_species_phyla_db.txt"


#Part 2: Import Pandas dataframe

#set names of columns for imported dataframe
colnames=['Query', 'Species_ID']
#import input database into a Pandas dataframe
prot_df = pd.read_csv(input_db, sep = '\t', names=colnames, header=None)


#Part 3: Create species categorization and phylum dictionaries

#create dictionary connecting species designations to species categories
category_dict = {'BM_anaeromoeba': 'BM_newprots_may21.anaeromoeba',
				 'BS_anaeromoeba': 'BS_newprots_may21.anaeromoeba',
				 'Carpediemonas_membranifera': 'Carpediemonas_membranifera.PRJNA719540',
				 'Dientamoeba_fragilis': 'Dientamoeba_fragilis.43352.aa',
				 'Trepomonas_sp_PC1': 'EP00703_Trepomonas_sp_PC1',
				 'Paratrimastix_pyriformis': 'EP00708_Paratrimastix_pyriformis',
				 'Aduncisulcus_paluster': 'EP00764_Aduncisulcus_paluster',
				 'Chilomastix_caulleryi': 'EP00766_Chilomastix_caulleryi',
				 'Chilomastix_cuspidata': 'EP00767_Chilomastix_cuspidata',
				 'Dysnectes_brevis': 'EP00768_Dysnectes_brevis',
				 'Ergobibamus_cyprinoides': 'EP00769_Ergobibamus_cyprinoides',
				 'Monocercomonoides_exilis': 'EP00770_Monocercomonoides_exilis',
				 'Trimastix_marina': 'EP00771_Trimastix_marina',
				 'Barthelona_sp_PAP020': 'EP00792_Barthelona_sp_PAP020',
				 'Giardia_intestinalis_A': ['EP00701_Giardia_intestinalis', 'Giardia_intestinalis.PRJNA1439', 'GiardiaDB_GintestinalisADH'],
				 'Giardia_intestinalis_B': ['GiardiaDB_GintestinalisBGS_B', 'GiardiaDB_GintestinalisBGS'],
				 'Giardia_intestinalis_E': 'GiardiaDB_GintestinalisEP15',
				 'Giardia_muris': 'Giardia_muris.PRJNA524057',
				 'Histomonas_meleagridis': ['Histomonas_meleagridis.135588.aa', 'Histomonas_meleagridis.PRJNA594289'],
				 'Kipferlia_bialata': 'Kipferlia_bialata.PRJDB5223',
				 'Pentatrichomonas_hominis': 'Pentatrichomonas_hominis.5728.aa',
				 'SC_anaeromoeba': 'SC_newprots_may21.anaeromoeba',
				 'Spironucleus_salmonicida': 'Spironucleus_salmonicida.PRJNA60811',
				 'Tetratrichomonas_gallinarum': 'Tetratrichomonas_gallinarum.5730.aa',
				 'Tritrichomonas_foetus': 'Trichomonas_foetus.PRJNA345179',
				 'Trichomonas_vaginalis': ['Trichomonas_vaginalis_GenBank.PRJNA16084', 'Trichomonas_vaginalis_RefSeq.G3']}


#create a dictionary connecting phylum information to species categories
phylum_dict = {'Anaeramoebidae': ['BM_anaeromoeba', 'BS_anaeromoeba', 'SC_anaeromoeba'],
			   'Parabasalia': ['Dientamoeba_fragilis', 'Histomonas_meleagridis', 'Pentatrichomonas_hominis',
					  'Tetratrichomonas_gallinarum', 'Tritrichomonas_foetus', 'Trichomonas_vaginalis'],
			   'Fornicata': ['Carpediemonas_membranifera', 'Trepomonas_sp_PC1', 'Aduncisulcus_paluster',
					'Chilomastix_caulleryi', 'Chilomastix_cuspidata', 'Dysnectes_brevis', 'Ergobibamus_cyprinoides',
					'Giardia_intestinalis_A', 'Giardia_intestinalis_B', 'Giardia_intestinalis_E', 'Giardia_muris',
					'Kipferlia_bialata', 'Spironucleus_salmonicida'],
			   'Preaxostyla': ['Paratrimastix_pyriformis', 'Monocercomonoides_exilis', 'Trimastix_marina'],
			   'Other': 'Barthelona_sp_PAP020'}


#Part 4: Add the species categories and phyla into the dataframe

#create empty column for species category information
prot_df['Species_Category'] = "-"

for index, row in prot_df.iterrows():
	#iterate through the dataframe row by row
	for key in category_dict.keys():
		#iterate over the dictionary via its keys
		if row['Species_ID'] in category_dict[key]:
			#if the species ID associated with a particular query protein
			#is in the value (string or list) associated with a particular key in the category_dict
			#then copy the dictionary key into the Species_Category column of that row
			prot_df.at[index, 'Species_Category'] = key


#create empty column for phylum information
prot_df['Phylum'] = "-"

for index, row in prot_df.iterrows():
	#iterate through the dataframe row by row
	for key in phylum_dict.keys():
		#iterate over the dictionary via its keys
		if row['Species_Category'] in phylum_dict[key]:
			#if the species category associated with a particular query protein
			#is in the value (string or list) associated with a particular key in the phylum_dict
			#then copy the dictionary key into the Phylum column of that row
			prot_df.at[index, 'Phylum'] = key


#Part 5: Write out resulting dataframe to a tab-separated text file

#write out the resulting dataframe
prot_df.to_csv(output_db, sep='\t', index=False)

```

Running it:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ dierctory
ln -s /home/inf-47-2020/ThesisTrich/DataFiles/ProtLists/species_prots_ref.txt .
#model for running the program:
python categorize_prot_species.py input_db output_db
#applying it:
python ../../Scripts/categorize_prot_species.py species_prots_ref.txt Prots_Species_Phyla_DB.txt

```

New verison of the script above, written later to accomadate the integration of the _A. lanta_ proteome (script saved to categorize_prot_species__v2.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: categorize_prot_species__v2.py
Date: 2022-05-23
Author: Virág Varga

Description:
	This program creates a database which includes all protein query IDs, the
		"official" species/strain designation, a categorized species/strain/assemblage
		designation, and the phylum that the species belongs to.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the query protein and species information-containing
		file into a dataframe.
	3. Creating species category and phylum information dictionaries.
	4. Adding the species category and phylum information into the dataframe.
	5. Writing out the dataframe to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a file with query IDs in the first column,
		and the species/strain IDs (derived from proteome file names) in the second column.

Version: 
	This is version 2 of this program, which accomadates the integration of Anaeramoeba lanta
		into the Metamonad database. 

Usage
	./categorize_prot_species.py input_db output_db
	OR
	python categorize_prot_species.py input_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part1: Assign command-line arguments, import modules

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "species_prots_ref.txt"
output_db = sys.argv[2]
#output_db = "prots_species_phyla_db.txt"


#Part 2: Import Pandas dataframe

#set names of columns for imported dataframe
colnames=['Query', 'Species_ID']
#import input database into a Pandas dataframe
prot_df = pd.read_csv(input_db, sep = '\t', names=colnames, header=None)


#Part 3: Create species categorization and phylum dictionaries

#create dictionary connecting species designations to species categories
category_dict = {'Anaeramoeba_lanta': 'Anaeramoeba_lanta_160522', 
				 'BM_anaeromoeba': 'BM_newprots_may21.anaeromoeba',
				 'BS_anaeromoeba': 'BS_newprots_may21.anaeromoeba',
				 'Carpediemonas_membranifera': 'Carpediemonas_membranifera.PRJNA719540',
				 'Dientamoeba_fragilis': 'Dientamoeba_fragilis.43352.aa',
				 'Trepomonas_sp_PC1': 'EP00703_Trepomonas_sp_PC1',
				 'Paratrimastix_pyriformis': 'EP00708_Paratrimastix_pyriformis',
				 'Aduncisulcus_paluster': 'EP00764_Aduncisulcus_paluster',
				 'Chilomastix_caulleryi': 'EP00766_Chilomastix_caulleryi',
				 'Chilomastix_cuspidata': 'EP00767_Chilomastix_cuspidata',
				 'Dysnectes_brevis': 'EP00768_Dysnectes_brevis',
				 'Ergobibamus_cyprinoides': 'EP00769_Ergobibamus_cyprinoides',
				 'Monocercomonoides_exilis': 'EP00770_Monocercomonoides_exilis',
				 'Trimastix_marina': 'EP00771_Trimastix_marina',
				 'Barthelona_sp_PAP020': 'EP00792_Barthelona_sp_PAP020',
				 'Giardia_intestinalis_A': ['EP00701_Giardia_intestinalis', 'Giardia_intestinalis.PRJNA1439', 'GiardiaDB_GintestinalisADH'],
				 'Giardia_intestinalis_B': ['GiardiaDB_GintestinalisBGS_B', 'GiardiaDB_GintestinalisBGS'],
				 'Giardia_intestinalis_E': 'GiardiaDB_GintestinalisEP15',
				 'Giardia_muris': 'Giardia_muris.PRJNA524057',
				 'Histomonas_meleagridis': ['Histomonas_meleagridis.135588.aa', 'Histomonas_meleagridis.PRJNA594289'],
				 'Kipferlia_bialata': 'Kipferlia_bialata.PRJDB5223',
				 'Pentatrichomonas_hominis': 'Pentatrichomonas_hominis.5728.aa',
				 'SC_anaeromoeba': 'SC_newprots_may21.anaeromoeba',
				 'Spironucleus_salmonicida': 'Spironucleus_salmonicida.PRJNA60811',
				 'Tetratrichomonas_gallinarum': 'Tetratrichomonas_gallinarum.5730.aa',
				 'Tritrichomonas_foetus': 'Trichomonas_foetus.PRJNA345179',
				 'Trichomonas_vaginalis': ['Trichomonas_vaginalis_GenBank.PRJNA16084', 'Trichomonas_vaginalis_RefSeq.G3']}


#create a dictionary connecting phylum information to species categories
phylum_dict = {'Anaeramoebidae': ['Anaeramoeba_lanta', 'BM_anaeromoeba', 'BS_anaeromoeba', 'SC_anaeromoeba'],
			   'Parabasalia': ['Dientamoeba_fragilis', 'Histomonas_meleagridis', 'Pentatrichomonas_hominis',
					  'Tetratrichomonas_gallinarum', 'Tritrichomonas_foetus', 'Trichomonas_vaginalis'],
			   'Fornicata': ['Carpediemonas_membranifera', 'Trepomonas_sp_PC1', 'Aduncisulcus_paluster',
					'Chilomastix_caulleryi', 'Chilomastix_cuspidata', 'Dysnectes_brevis', 'Ergobibamus_cyprinoides',
					'Giardia_intestinalis_A', 'Giardia_intestinalis_B', 'Giardia_intestinalis_E', 'Giardia_muris',
					'Kipferlia_bialata', 'Spironucleus_salmonicida'],
			   'Preaxostyla': ['Paratrimastix_pyriformis', 'Monocercomonoides_exilis', 'Trimastix_marina'],
			   'Other': 'Barthelona_sp_PAP020'}


#Part 4: Add the species categories and phyla into the dataframe

#create empty column for species category information
prot_df['Species_Category'] = "-"

for index, row in prot_df.iterrows():
	#iterate through the dataframe row by row
	for key in category_dict.keys():
		#iterate over the dictionary via its keys
		if row['Species_ID'] in category_dict[key]:
			#if the species ID associated with a particular query protein
			#is in the value (string or list) associated with a particular key in the category_dict
			#then copy the dictionary key into the Species_Category column of that row
			prot_df.at[index, 'Species_Category'] = key


#create empty column for phylum information
prot_df['Phylum'] = "-"

for index, row in prot_df.iterrows():
	#iterate through the dataframe row by row
	for key in phylum_dict.keys():
		#iterate over the dictionary via its keys
		if row['Species_Category'] in phylum_dict[key]:
			#if the species category associated with a particular query protein
			#is in the value (string or list) associated with a particular key in the phylum_dict
			#then copy the dictionary key into the Phylum column of that row
			prot_df.at[index, 'Phylum'] = key


#Part 5: Write out resulting dataframe to a tab-separated text file

#write out the resulting dataframe
prot_df.to_csv(output_db, sep='\t', index=False)

```

This script enables the filtration of paralogs from the OG clustering results (saved to filter_paralogs__v2.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_paralogs__v2.py
Date: 2022-03-15
Author: Virág Varga

Description:
	This program performs filters the parsed results of orthologous clustering software
		(OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) in order to exclude paralogs
		(ie. "orthologs" where all member proteins come from the same species or assemblage).

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	os

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Importing modules and parsing arguments. Running code specific to individual
		arguments for import into a Pandas dataframe.
	3. Main program code:
		- Importing reference file into Pandas dataframe
		- Importing OG data into dictionary with format ortho_dict[OG_ID] = list_of_queries
		- Identifying OGs that are paralogs, and creating a list of non-paralog OGs
		- Creating a Pandas dataframe containing only the non-paralog OGs
	4. Writing out dataframe of non-paralog OGs to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The input file used for the program must be an un-pivoted results file of either
		Broccoli, OrthoFinder, SonicParanoid, or ProteinOrtho, following the structure
		used by the parsers used previously in this workflow.
	- The program cannot accept multiple input parsed OG files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Version:
	This is version 2.0 of the program. The following changes were make to the script,
		to increase ease of program use:
		- Made the script more flexible re: number of columns. Now filtered parsed data
			can still be processed, even if the formatting does not match the original
			parsed OG data file.

Usage:
	./filter_paralogs.py [-h] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE
	OR
	python filter_paralogs.py [-h] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""

#################################   ARGPARSE   #######################################

import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


parser = argparse.ArgumentParser(description =
								 'This program performs filters the parsed results of orthologous \
								 clustering software (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) \
								 in order to exclude paralogs (ie. "orthologs" where all member proteins \
								 come from the same species or assemblage).')
#The most general description of what this program can do is defined here


#adding the arguments that the program can use
parser.add_argument(
	'-br', '--Broccoli',
	action='store_true',
	help = 'This argument will parse the results of the Broccoli program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed Broccoli results
parser.add_argument(
	'-of', '--OrthoFinder',
	action='store_true',
	help = 'This argument will parse the results of the OrthoFinder program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed OrthoFinder results
parser.add_argument(
	'-po', '--ProteinOrtho',
	action='store_true',
	help = 'This argument will parse the results of the ProteinOrtho program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed ProteinOrtho results
parser.add_argument(
	'-sp', '--SonicParanoid',
	action='store_true',
	help = 'This argument will parse the results of the SonicParanoid program.'
	)
	#the '-sp' flag will import the input file in the manner appropriate for the parsed SonicParanoid results
parser.add_argument(
	#'-i', '--input',
	#the above line of code is left in as further clarification of this argument
	dest='input_file',
	metavar='INPUT_FILE',
	help = "The input file should be parsed orthologous clustering results.",
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')
parser.add_argument(
	#'-r', '--ref',
	#the above line of code is left in as further clarification of this argument
	dest='ref_file',
	metavar='REFERENCE_FILE',
	help = "The reference file should be in the format: Query\tSpecies_ID\tSpecies_Category\tPhylum.",
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires a reference file, and it should be opened for reading ('r')
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 2.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = args.input_file.name
#designate reference file name as variable
ref_db = args.ref_file.name


#parse arguments
if args.Broccoli:
	#if -br argument is called
	print("Broccoli results recieved")
	#save the input program ID for use in the output file
	prog_id = "Broccoli"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_nonParalog.txt"

if args.OrthoFinder:
	#if -of argument is called
	print("OrthoFinder results recieved")
	#save the input program ID for use in the output file
	prog_id = "OrthoFinder"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_nonParalog.txt"

if args.ProteinOrtho:
	#if -tp argument is called
	print("ProteinOrtho results recieved")
	#save the input program ID for use in the output file
	prog_id = "ProteinOrtho"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_nonParalog.txt"

if args.SonicParanoid:
	#if -en argument is called
	print("SonicParanoid results recieved")
	#save the input program ID for use in the output file
	prog_id = "SonicParanoid"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_nonParalog.txt"


#################################   Main Program   ######################################


#Part 1: Set up data types (dataframe for ref file, dictionary for infile) that will be used

#remove 3rd column from Pandas dataframe if necessary
#needed for original parsed results of OrthoFinder, ProteinOrtho & SonicParanoid
col_num = len(ortho_df.columns)
#count number of columns
if col_num == 3:
	#select dataframes with 3 columns
	#remove middle column with species information
	ortho_df.drop(ortho_df.columns[1], axis=1, inplace=True)


#import reference file into Pandas dataframe
ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)


#identify OG column name (for use later)
og_col = ortho_df.columns[1]

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 2: Identify the non-paralog OGs

#create empty list for non-paralog OGs
non_paralog_OG_list = []

for key in ortho_dict.keys():
	#iterate through the dictionary via its keys
	species_list = []
	#create an empty list to hold the species categories of the protein queries
	OG_prot_list = ortho_dict[key]
	#create list containing the protein queries in the OG, for ease of use
	for prot in OG_prot_list:
		#iterate through the protein queries in the list
		prot_idx = ref_df.index[ref_df['Query'] == prot][0]
		#get the index of the protein query in the reference dataframe
		prot_species = ref_df.iloc[prot_idx]['Species_Category']
		#get the species category assigned to that particular protein query
		#and append it to the species list
		species_list.append(prot_species)
	species_set = set(species_list)
	#convert the species list into a set to remove duplicates
	if len(species_set) > 1:
		#if the length of the species set is greater than 1,
		#then more than 1 species category is represented in the OG
		#if this is the case, save the OG ID to the non_paralog_OG_list
		non_paralog_OG_list.append(key)


#Part 3: Copy non-paralog OG information into a new dataframe & write out

#create new dataframe with non-paralog OGs
non_paralog_df = ortho_df[ortho_df[og_col].isin(non_paralog_OG_list)].copy()
#use `.isin()` to iterate over entire list of non-paralog OGs
#use `.copy()` to ensure the dataframe is seperate from the ortho_df

#Writing out the results to a tab-separated text file
non_paralog_df.to_csv(output_file, sep='\t', index=False)

```

Running it:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model:
python filter_paralogs.py [-h] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE
#testing it:
python ../../Scripts/filter_paralogs.py -h
# usage: filter_paralogs.py [-h] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE
# This program performs filters the parsed results of orthologous clustering software
# (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) in order to exclude paralogs (ie.
# "orthologs" where all member proteins come from the same species or assemblage).
# positional arguments:
#   INPUT_FILE            The input file should be parsed orthologous clustering results.
#   REFERENCE_FILE        The reference file should be in the format: Query Species_ID
#                         Species_Category Phylum.
# optional arguments:
#   -h, --help            show this help message and exit
#   -br, --Broccoli       This argument will parse the results of the Broccoli program.
#   -of, --OrthoFinder    This argument will parse the results of the OrthoFinder program.
#   -po, --ProteinOrtho   This argument will parse the results of the ProteinOrtho program.
#   -sp, --SonicParanoid  This argument will parse the results of the SonicParanoid program.
#   -v, --version         show program's version number and exit
#applying it:
python ../../Scripts/filter_paralogs.py -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_paralogs.py -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_paralogs.py -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_paralogs.py -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved

```

This script enables the filtration of orthologous clusters that only have n protein members (saved to filter_OG_size__v2.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_OG_size__v2.py
Date: 2022-03-15
Author: Virág Varga

Description:
	This program performs filters the parsed results of orthologous clustering software
		(OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) in order to exclude OGs
		that do not meet a user-determined minimum membership size (default = 3 proteins).

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	os

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Importing modules and parsing arguments. Running code specific to individual
		arguments for import into a Pandas dataframe.
	3. Main program code:
		- Importing OG data into dictionary with format ortho_dict[OG_ID] = list_of_queries
		- Identifying OGs that meet the threshold minimum size, and compiling them into a list
		- Creating a Pandas dataframe containing only the OGs that meet the threshold
			minimum size
	4. Writing out dataframe of threshold size-meeting OGs to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The input file used for the program must be an un-pivoted results file of either
		Broccoli, OrthoFinder, SonicParanoid, or ProteinOrtho, following the structure
		used by the parsers used previously in this workflow.
	- The program cannot accept multiple input parsed OG files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Version:
	This is version 2.0 of the program. The following changes were make to the script,
		to increase ease of program use:
		- Made the script more flexible re: number of columns. Now filtered parsed data
			can still be processed, even if the formatting does not match the original
			parsed OG data file.

Usage:
	./filter_OG_size.py [-h] [---threshold_minimum THRESHOLD_MINIMUM] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE
	OR
	python filter_OG_size.py [-h] [---threshold_minimum THRESHOLD_MINIMUM] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""

#################################   ARGPARSE   #######################################

import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


parser = argparse.ArgumentParser(description =
								 'This program performs filters the parsed results of orthologous \
								 clustering software (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) \
								 in order to exclude OGs that do not meet a user-determined minimum \
								 membership size (default = 3 proteins).')
#The most general description of what this program can do is defined here


#adding the arguments that the program can use
parser.add_argument(
	'-br', '--Broccoli',
	action='store_true',
	help = 'This argument will parse the results of the Broccoli program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed Broccoli results
parser.add_argument(
	'-of', '--OrthoFinder',
	action='store_true',
	help = 'This argument will parse the results of the OrthoFinder program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed OrthoFinder results
parser.add_argument(
	'-po', '--ProteinOrtho',
	action='store_true',
	help = 'This argument will parse the results of the ProteinOrtho program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed ProteinOrtho results
parser.add_argument(
	'-sp', '--SonicParanoid',
	action='store_true',
	help = 'This argument will parse the results of the SonicParanoid program.'
	)
	#the '-sp' flag will import the input file in the manner appropriate for the parsed SonicParanoid results
parser.add_argument(
	"--threshold_minimum",
	type=int,
	default=3,
	help = "The minimum number of protein queries that should be part of an OG in order for that OG to be kept. (Default = 3)"
	)
	#the `type=int` argument allows argparse to accept the input as an integer
	#the `default=3` gives a default minimum membership filtration value
	#ref: https://stackoverflow.com/questions/44011031/how-to-pass-a-string-as-an-argument-in-python-without-namespace
	#ref: https://stackoverflow.com/questions/14117415/in-python-using-argparse-allow-only-positive-integers
parser.add_argument(
	#'-i', '--input',
	#the above line of code is left in as further clarification of this argument
	dest='input_file',
	metavar='INPUT_FILE',
	help = "The input file should be parsed orthologous clustering results.",
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 2.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = args.input_file.name
#designate mimium OG membership threshold value as variable
threshold_value = args.threshold_minimum


#parse arguments
if args.Broccoli:
	#if -br argument is called
	print("Broccoli results recieved")
	#save the input program ID for use in the output file
	prog_id = "Broccoli"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_threshold" + str(threshold_value) + ".txt"

if args.OrthoFinder:
	#if -of argument is called
	print("OrthoFinder results recieved")
	#save the input program ID for use in the output file
	prog_id = "OrthoFinder"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_threshold" + str(threshold_value) + ".txt"

if args.ProteinOrtho:
	#if -tp argument is called
	print("ProteinOrtho results recieved")
	#save the input program ID for use in the output file
	prog_id = "ProteinOrtho"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_threshold" + str(threshold_value) + ".txt"

if args.SonicParanoid:
	#if -en argument is called
	print("SonicParanoid results recieved")
	#save the input program ID for use in the output file
	prog_id = "SonicParanoid"
	#import input file into pandas dataframe
	ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
	#define the output file based on the input file name
	base = os.path.basename(infile)
	out_full = os.path.splitext(base)[0]
	output_file = out_full + "_threshold" + str(threshold_value) + ".txt"


#################################   Main Program   ######################################

#Complete setup

#remove 3rd column from Pandas dataframe if necessary
#needed for original parsed results of OrthoFinder, ProteinOrtho & SonicParanoid
col_num = len(ortho_df.columns)
#count number of columns
if col_num == 3:
	#select dataframes with 3 columns
	#remove middle column with species information
	ortho_df.drop(ortho_df.columns[1], axis=1, inplace=True)


#Part 1: Set up dictionary of OG data that will be filtered

#identify OG column name (for use later)
og_col = ortho_df.columns[1]

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 2: Identify OGs that meet the threshold minimum

#create empty list for non-paralog OGs
good_OG_list = []


for key in ortho_dict.keys():
	#iterate through the dictionary via its keys
	if len(ortho_dict[key]) >= threshold_value:
		#identify OGs that meet the minimum threshold value size
		#and add those OGs to the list of good OGs
		good_OG_list.append(key)


#Part 3: Copy threshold size OG information into a new dataframe & write out

#create new dataframe with non-paralog OGs
threshold_df = ortho_df[ortho_df[og_col].isin(good_OG_list)].copy()
#use `.isin()` to iterate over entire list of threshold-meeting OGs
#use `.copy()` to ensure the dataframe is seperate from the ortho_df

#Writing out the results to a tab-separated text file
threshold_df.to_csv(output_file, sep='\t', index=False)

```

Running it:

```bash
#running it in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model:
python filter_OG_size.py [-h] [-br] [-of] [-po] [-sp] [-tm] threshold_min [-v] INPUT_FILE
#testing it:
python ../../Scripts/filter_OG_size.py -h
# usage: filter_OG_size.py [-h] [-br] [-of] [-po] [-sp] [--threshold_minimum THRESHOLD_MINIMUM]
#                          [-v]
#                          INPUT_FILE
# This program performs filters the parsed results of orthologous clustering software
# (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) in order to exclude OGs that do not meet a
# user-determined minimum membership size (default = 3 proteins).
# positional arguments:
#   INPUT_FILE            The input file should be parsed orthologous clustering results.
# optional arguments:
#   -h, --help            show this help message and exit
#   -br, --Broccoli       This argument will parse the results of the Broccoli program.
#   -of, --OrthoFinder    This argument will parse the results of the OrthoFinder program.
#   -po, --ProteinOrtho   This argument will parse the results of the ProteinOrtho program.
#   -sp, --SonicParanoid  This argument will parse the results of the SonicParanoid program.
#   --threshold_minimum THRESHOLD_MINIMUM
#                         The minimum number of protein queries that should be part of an OG in
#                         order for that OG to be kept. (Default = 3)
#   -v, --version         show program's version number and exit
#applying it:
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -br Broccoli_OGs_parsed.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -of OF_OGs_parsed.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -po PO_OGs_parsed.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -sp SP_OGs_parsed.txt
# SonicParanoid results recieved
###
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -br Broccoli_OGs_parsed_nonParalog.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -of OF_OGs_parsed_nonParalog.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -po PO_OGs_parsed_nonParalog.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_size.py --threshold_minimum 3 -sp SP_OGs_parsed_nonParalog.txt
# SonicParanoid results recieved
###
#actually technically I ran all of these other than Broccoli with the -br flag
#I realized that the infile acceptance method is different (for different column numbers),
#but the output files of the previous filtration all have 2 columns
#this issue is fixed in the filter_OG_size__v2.py script
###

```

This script enables the filtration of orthologous clusters that only have n% of species represented (filter_OG_speciesRep__v2.py):
 - Version 2.0 includes the data summary file produced at the end of the program's run. 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_OG_speciesRep__v2.py
Date: 2022-03-22
Author: Virág Varga

Description:
	This program performs filters the parsed results of orthologous clustering software
		(OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) in order to exclude paralogs
		(ie. "orthologs" where all member proteins come from the same species or assemblage).

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	os

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Importing modules and parsing arguments. Running code specific to individual
		arguments for import into a Pandas dataframe.
	3. Main program code:
		- Setting up outfile, importing data into Pandas & variables
		- Setting up data types (dataframe for ref file, dictionary for infile) that will be used
		- Creating dictionary of species represented in each OG
		- Identifying OGs that meet the species percentage representation threshold
	4. Writing out dataframe of minimum species membership threshold-matching OGs to a
		tab-separated text file.
	5. Writing out summary text file. 

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The input file used for the program must be an un-pivoted results file of either
		Broccoli, OrthoFinder, SonicParanoid, or ProteinOrtho, following the structure
		used by the parsers used previously in this workflow.
	- The program cannot accept multiple input parsed OG files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Version: 
	This is version 2.0 of this script. There is now a summary text file produced at the end listing
		the number of OGs that have the threshold minimum percent species represented, as well as 
		the OG IDs of these OGs. 

Usage:
	./filter_OG_speciesRep__v2.py [-h] [--threshold_minimum THRESHOLD_MINIMUM] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE
	OR
	python filter_OG_speciesRep__v2.py [-h] [--threshold_minimum THRESHOLD_MINIMUM] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""

#################################   ARGPARSE   #######################################

import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


parser = argparse.ArgumentParser(description =
								 'This program filters the parsed results of orthologous \
								 clustering software (OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) \
								 based on whether a threshold percentage of species are represented \
								 (default = 85%).')
#The most general description of what this program can do is defined here


#adding the arguments that the program can use
parser.add_argument(
	'-br', '--Broccoli',
	action='store_true',
	help = 'This argument will parse the results of the Broccoli program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed Broccoli results
parser.add_argument(
	'-of', '--OrthoFinder',
	action='store_true',
	help = 'This argument will parse the results of the OrthoFinder program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed OrthoFinder results
parser.add_argument(
	'-po', '--ProteinOrtho',
	action='store_true',
	help = 'This argument will parse the results of the ProteinOrtho program.'
	)
	#the '-br' flag will import the input file in the manner appropriate for the parsed ProteinOrtho results
parser.add_argument(
	'-sp', '--SonicParanoid',
	action='store_true',
	help = 'This argument will parse the results of the SonicParanoid program.'
	)
	#the '-sp' flag will import the input file in the manner appropriate for the parsed SonicParanoid results
parser.add_argument(
	"--threshold_minimum",
	type=int,
	default=85,
	help = "Integer value of minimum percent of species that should be represented in OGs. (Default = 85)"
	)
	#the `type=int` argument allows argparse to accept the input as an integer
	#the `default=3` gives a default minimum membership filtration value
	#ref: https://stackoverflow.com/questions/44011031/how-to-pass-a-string-as-an-argument-in-python-without-namespace
	#ref: https://stackoverflow.com/questions/14117415/in-python-using-argparse-allow-only-positive-integers
parser.add_argument(
	#'-i', '--input',
	#the above line of code is left in as further clarification of this argument
	dest='input_file',
	metavar='INPUT_FILE',
	help = "The input file should be parsed orthologous clustering results.",
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')
parser.add_argument(
	#'-r', '--ref',
	#the above line of code is left in as further clarification of this argument
	dest='ref_file',
	metavar='REFERENCE_FILE',
	help = "The reference file should be in the format: Query\tSpecies_ID\tSpecies_Category\tPhylum.",
	type=argparse.FileType('r')
	)
	#this portion of code specifies that the program requires a reference file, and it should be opened for reading ('r')
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 1.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = args.input_file.name
#designate reference file name as variable
ref_db = args.ref_file.name
#designate mimium OG membership threshold value as variable
threshold_value = args.threshold_minimum


#parse arguments
if args.Broccoli:
	#if -br argument is called
	print("Broccoli results recieved")
	#save the input program ID for use in the output file
	prog_id = "Broccoli"

if args.OrthoFinder:
	#if -of argument is called
	print("OrthoFinder results recieved")
	#save the input program ID for use in the output file
	prog_id = "OrthoFinder"

if args.ProteinOrtho:
	#if -tp argument is called
	print("ProteinOrtho results recieved")
	#save the input program ID for use in the output file
	prog_id = "ProteinOrtho"

if args.SonicParanoid:
	#if -en argument is called
	print("SonicParanoid results recieved")
	#save the input program ID for use in the output file
	prog_id = "SonicParanoid"


#################################   Main Program   ######################################


#Part 1: Set up outfile, import data into Pandas & variables

#define the output files based on the input file name
base = os.path.basename(infile)
out_full = os.path.splitext(base)[0]
#parsed OG output file
output_file = out_full + "_OGsMembership" + str(threshold_value) + ".txt"
#summary output file
output_summary_file = out_full + "_OGsMembership" + str(threshold_value) + "__SUMMARY.txt"

#import input file into pandas dataframe
ortho_df = pd.read_csv(infile, sep = '\t', header = 0)

#remove 3rd column from Pandas dataframe if necessary
#needed for original parsed results of OrthoFinder, ProteinOrtho & SonicParanoid
col_num = len(ortho_df.columns)
#count number of columns
if col_num == 3:
	#select dataframes with 3 columns
	#remove middle column with species information
	ortho_df.drop(ortho_df.columns[1], axis=1, inplace=True)


#import reference file into Pandas dataframe
ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)


#convert membership percentage to a decimal
membership_decimal = float(threshold_value)/100


#Part 2: Set up data types (dataframe for ref file, dictionary for infile) that will be used

#identify OG column name (for use later)
og_col = ortho_df.columns[1]

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 3: Create dictionary of species represented in each OG

#create empty dictionary for species information
species_OG_dict = {}

for key in ortho_dict.keys():
	#iterate over the dictionary via its keys
	species_list = []
	#create empty list that will be populated with species information
	og_prot_list = ortho_dict[key]
	#save list of protein query IDs associated with the given OG
	for prot in og_prot_list:
		#iterate over the list of protein query IDs
		prot_species = ref_df.loc[ref_df['Query'] == prot, 'Species_Category'].iloc[0]
		#with .loc, find the location where the protein query ID is found in the 'Query' column
		#then extract the contents of that cell, as well as the cell in the same row that is in the 'Species_Category' column
		#use slicing and .iloc to extract the contents of the 'Species_Category' column
		#and save the species category to variable prot_species
		#append the species category to the species_list
		species_list.append(prot_species)
	species_set = set(species_list)
	#turn the species_list into a set to eliminate duplicates
	#and save the species_set as the value in the OG to species dictionary
	species_OG_dict[key] = species_set


#Part 4: Identify OGs that meet the species percentage representation threshold

#get number of species represented
total_species = ref_df.Species_Category.unique()
#save list of species categories to a variable
total_species_num = len(total_species)
#count the number of elements of the species category list
#for the original Thesis project workflow, this should be 26

threshold_species_num = round(total_species_num*membership_decimal)
#get the minimum number of species needed to hit the species membership threshold
#round the number to the nearest integer (up or down)


#create new dictionary for data that meets the threshold
threshold_og_list = []

for key in species_OG_dict.keys():
	#iterate over the dictionary via its keys
	if len(species_OG_dict[key]) >= threshold_species_num:
		#identify the OGs that meet the minimum species inclusion threshold number
		#and save the OG to the list of threshold_og_list
		threshold_og_list.append(key)


#Part 5: Copy threshold-meeting OG information into a new dataframe & write out

#create new dataframe with non-paralog OGs
threshold_df = ortho_df[ortho_df[og_col].isin(threshold_og_list)].copy()
#use `.isin()` to iterate over entire list of non-paralog OGs
#use `.copy()` to ensure the dataframe is seperate from the ortho_df

#Writing out the results to a tab-separated text file
threshold_df.to_csv(output_file, sep='\t', index=False)


#Part 6: Create summary data text file

#compute statistics to include in the summary file
threshold_og_num = len(threshold_df[og_col].unique())
#calculate number of OGs included in the filtered dataframe
original_og_num = len(ortho_df[og_col].unique())
#calculate number of OGs included in the original dataframe
percent_ogs_remaining = (threshold_og_num/original_og_num)*100
#calculate percent of OGs remaining
rounded_percent = round(percent_ogs_remaining, 3)
#round the percentage to 3 decimal places

#get list of "good" OGs for summary file
good_OG_list = threshold_df[og_col].unique()


with open(output_summary_file, "w") as outfile: 
	#open the summary file for writing
	outfile.write("The number of orthologous clusters created by the " + prog_id + " program that meet " + "\n" + 
			   "the desired threshold value of " + str(threshold_value) + "% is " + str(threshold_og_num) + "." + "\n\n")
	outfile.write("In contrast, the total number of orthologous clusters created by this program was " + str(original_og_num) + ", " + "\n" + 
			   "which means that " + str(rounded_percent) + "% of the clusters met the desired threshold value." + "\n\n")
	outfile.write("The OGs that met the threshold percent are: " + "\n")
	for element in good_OG_list:
		#iterate through the list of good OGs and write them out to the file
		outfile.write(element + "\n")

```

Running it:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#testing it:
python ../../Scripts/filter_OG_speciesRep.py -h
# usage: filter_OG_speciesRep.py [-h] [-br] [-of] [-po] [-sp] [--threshold_minimum THRESHOLD_MINIMUM]
#                                [-v]
#                                INPUT_FILE REFERENCE_FILE
# This program filters the parsed results of orthologous clustering software (OrthoFinder,
# SonicParanoid, ProteinOrtho, Broccoli) based on whether a threshold percentage of species are
# represented (default = 85%).
# positional arguments:
#   INPUT_FILE            The input file should be parsed orthologous clustering results.
#   REFERENCE_FILE        The reference file should be in the format: Query Species_ID Species_Category
#                         Phylum.
# optional arguments:
#   -h, --help            show this help message and exit
#   -br, --Broccoli       This argument will parse the results of the Broccoli program.
#   -of, --OrthoFinder    This argument will parse the results of the OrthoFinder program.
#   -po, --ProteinOrtho   This argument will parse the results of the ProteinOrtho program.
#   -sp, --SonicParanoid  This argument will parse the results of the SonicParanoid program.
#   --threshold_minimum THRESHOLD_MINIMUM
#                         Integer value of minimum percent of species that should be represented in OGs.
#                         (Default = 85)
#   -v, --version         show program's version number and exit
#model:
python filter_OG_speciesRep.py [-h] [--threshold_minimum THRESHOLD_MINIMUM] [-br] [-of] [-po] [-sp] [-v] INPUT_FILE REFERENCE_FILE
#applying it:
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 80 -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 80 -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 80 -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 80 -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved
#and now search for those that have all species present
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 100 -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 100 -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 100 -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_speciesRep.py --threshold_minimum 100 -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved
### 
#doing the same as above, but with the new script that creates a summary file
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 80 -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 80 -br Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 80 -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 80 -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 80 -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved
#and now search for those that have all species present
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 100 -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 100 -br Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 100 -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 100 -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 100 -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved
###
#at Courteny's request, lowering the threshold
#40% membership
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 40 -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 40 -br Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 40 -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 40 -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 40 -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved
#50% membership
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 50 -br Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 50 -br Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt
# Broccoli results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 50 -of OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# OrthoFinder results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 50 -po PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# ProteinOrtho results recieved
python ../../Scripts/filter_OG_speciesRep__v2.py --threshold_minimum 50 -sp SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
# SonicParanoid results recieved

```

Looking at number of OGs remaining after filtrations:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#base files:
#(these need to be separate because file names & structures are different)
ls Broccoli_OGs_parsed.txt | while read file; do
	echo $file;
	awk -F '\t' '(NR>1) {print $2}' $file | sort | uniq | wc -l;
	#calculate the number of unique OGs in the file
	# `(NR>1)` skips the first line in the file (conatins the header)
	awk 'END { print NR - 1 }' $file;
	#calculate the number of protein queries (= lines - 1)
done
# Broccoli_OGs_parsed.txt
# 39854
# 404323
ls *_OGs_parsed.txt | grep -v "Broccoli_OGs_parsed.txt" | while read file; do
	echo $file;
	awk -F '\t' '(NR>1) {print $3}' $file | sort | uniq | wc -l;
	#calculate the number of unique OGs in the file
	# `(NR>1)` skips the first line in the file (conatins the header)
	awk 'END { print NR - 1 }' $file;
	#calculate the number of protein queries (= lines - 1)
done
# OF_OGs_parsed.txt
# 77562
# 467369
# PO_OGs_parsed.txt
# 94951
# 336591
# SP_OGs_parsed.txt
# 67234
# 372333
#parsed files
ls *_OGs_parsed_*.txt | while read file; do
	echo $file;
	awk -F '\t' '(NR>1) {print $2}' $file | sort | uniq | wc -l;
	#calculate the number of unique OGs in the file
	# `(NR>1)` skips the first line in the file (conatins the header)
	awk 'END { print NR - 1 }' $file;
	#calculate the number of protein queries (= lines - 1)
done
# Broccoli_OGs_parsed_DUPLICATES.txt
# 213
# 657
# Broccoli_OGs_parsed_nonParalog_threshold3.txt
# 16050
# 332199
# Broccoli_OGs_parsed_nonParalog.txt
# 30288
# 360675
# Broccoli_OGs_parsed_OGsMembership100.txt
# 199
# 39034
# Broccoli_OGs_parsed_OGsMembership80.txt
# 909
# 102619
# Broccoli_OGs_parsed_threshold3.txt
# 17125
# 358865
# OF_OGs_parsed_nonParalog_threshold3.txt
# 23107
# 294548
# OF_OGs_parsed_nonParalog.txt
# 42961
# 334256
# OF_OGs_parsed_OGsMembership100.txt
# 195
# 36948
# OF_OGs_parsed_OGsMembership80.txt
# 709
# 79364
# OF_OGs_parsed_threshold3.txt
# 32047
# 376339
# PO_OGs_parsed_nonParalog_threshold3.txt
# 31128
# 179073
# PO_OGs_parsed_nonParalog.txt
# 66952
# 250721
# PO_OGs_parsed_OGsMembership100.txt
# 14
# 1062
# PO_OGs_parsed_OGsMembership80.txt
# 243
# 12771
# PO_OGs_parsed_threshold3.txt
# 35585
# 217859
# SP_OGs_parsed_nonParalog_threshold3.txt
# 17576
# 259133
# SP_OGs_parsed_nonParalog.txt
# 40624
# 305229
# SP_OGs_parsed_OGsMembership100.txt
# 187
# 23191
# SP_OGs_parsed_OGsMembership80.txt
# 877
# 74584
# SP_OGs_parsed_threshold3.txt
# 19841
# 277547

```

Looking at the data above:
 - Most programs (all except ProteinOrtho) are yielding a little under 200 orthologous groups that are present in all species; with 20-30k proteins included within those groups
 - This implies that ProteinOrtho may not be great at finding OGs with these organisms


### Group membership tests

#### Scoring OG membership similarity

Test to compare similarity of clusters between programs (script saved to og_membership_test.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: og_membership_test.py
Date: 2022-03-08
Author: Virág Varga

Description:
	This program compares the parsed results files from Broccoli, OrthoFinder,
		ProteinOrtho and SonicParanoid, in order the score the similarity of the
		orthologous clusters created by these programs.
	The user can set a target threshold average similarity value, under which results
		should not be reported. Default is 50%.

List of functions:
	data_2_pandas(broccoli_db, orthofinder_db, proteinortho_db, sonicparanoid_db):
		Loads input parsed OG databases into Pandas dataframes and dictionaries.
	create_prot_dict(broccoli_df, orthofinder_df, proteinortho_df, sonicparanoid_df):
		Creates the protein query ID to list of assigned OGs dictionary.
	filter_prot_dict(prot_dict):
		Filter the contents of the protein query ID to list of assigned OGs
		dictinary to only include those OGs as keys that were clustered by more than 1
		clustering program.
	membership_test(filt_prot_dict, broccoli_dict, orthofinder_dict, proteinortho_dict, sonicparanoid_dict):
		Create dictionary of all-vs-all comparisons of cluster similarity between the
		orthologous clustering programs.
	avg_membership_scores(comparison_dict):
		Calculate the average similarity comparison score for each pair of orthologous
		clustering programs.
	threshold_test(pg_score_dict):
		Compare the average clustering similarity scores of the pairs of orthologous
		clustering programs to the selected threshold value.

List of standard and non-standard modules used:
	sys
	pandas
	json
	difflib
	statistics

Procedure:
	1. Loading required modules, setting the threshold value to be used in the
		comparison filtration and the corresponding output file name.
	2. Determining input files as command-line arguments, and importing the contents
		of these databases into Pandas dataframes and dictionaries. Dictionaries
		are exported in JSON format. Checkpoint 1 is reached at the completion of
		this step.
	3. Creation of the protein query ID to OG cluster assignments list dictionary.
		Checkpoint 2 is reached at the conclusion of this step, when this dictionary
		is exported in JSON format.
	4. Filtration of the protein query ID to OG assignment list dictionary to exclude
		protein queries that were only clustered by one program. Checkpoint 3 is reached
		at the conclusion of this step, when the new dictionary is exported in JSON
		format.
	5. The all-vs-all OG membership tests are completed, and a dictionary is created
		containing the similarity scores of the clusters created by the programs.
		Checkpoint 4 is reached at the conclusion of this step, when this comparison
		dictionary is exported in JSON format.
	6. The scores inside of the comparison dictionary are averaged. Checkpoint 4.5 is
		reached at the conclusion of this step, when this smaller dictionary is exported
		in JSON format.
	7. The threshold membership percentage value is used to filter the programs with
		the most similar clusters, and a new dictionary is created to containing only
		this data. Checkpoint 5 is reached at the conclusion of this step.
	8. The contents ot the dictionary created in the final step (ie. the final,
		membership threshold-filtered data) is written out to a text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The names of the JSON files created at the checkpoints are not user-defined. This
		means that running the program multiple times in the same directory will
		overwrite existing files from a previous run.
	- The final results file is similarly pre-determined, though the threshold percentage
		value is integrated into the file name, so the program will not overwrite the
		results file if a different threshold value is used.
	- The membership_percent threshold value must be given as an integer, not as a
		decimal percentage.
	- This program is intended for use with parsed Broccoli results from which duplicates have 
		already been removed. 

Usage:
	The full program can be run with:
		./og_membership_test.py broccoli_db orthofinder_db proteinortho_db sonicparanoid_db [membership_percent]
		OR
		python og_membership_test.py broccoli_db orthofinder_db proteinortho_db sonicparanoid_db [membership_percent]

		* Where the membership_percent threshold value should be given as an integer percentage value.

	Alternatively, the program can be run from a checkpoint, using a JSON file produced
		during the earlier stages of the analysis. In such a case, the program should
		be run like so:
			./og_membership_test.py [JSON_file] [membership_percent]
			OR
			python og_membership_test.py [JSON_file] [membership_percent]

		* Where the acceptable input JSON files include:
			- prot_dict.json: This will start the program after Checkpoint 2, by providing a
				complete version of the prot_dict protein query to OG list dictionary.
			- filt_prot_dict.json: This will start the program after Checkpoint 3, by providing
				a complete version of the filt_prot_dict filtered protein query to OG list,
				from which queries only predicted to cluster by one program have been removed.
			- compare_OG_dict.json: This will start the program after Checkpoint 4, by
				providing a version of the compare_dict clustering program to cluster score list
				dictionary.
			- og_score_dict.json: This will start the program after Checkpoint 4.5, by providing
				a version of the OG scoring dictionary in which the average scores have already
				been calculated.
		* Note! The membership testing, which occurs after Checkpoint 3, requires the input of
			dictionaries created from the orthologous clustering dataframes. These are produced
			during the Pandas dataframe imports, and are printed out to JSON files. While they should
			not be added as command-line arguments, they should be in the directory from which this
			program is run.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


# Part 1: Loading required modules, setting the threshold value to be used
# in the comparison filtration and the corresponding output file name.

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
import json #allows import and export of data in JSON format
import difflib #compare and calculate differences between datasets
import statistics #simplify computation of basic statistics in Python


###
# Define the functions that will be used in this script
###


def data_2_pandas(broccoli_db, orthofinder_db, proteinortho_db, sonicparanoid_db):
	# Part 2: Determining input files as command-line arguments, and importing the contents
	# of these databases into Pandas dataframes and dictionaries. Dictionaries
	# are exported in JSON format. Checkpoint 1 is reached at the completion of this step.

	#import Broccoli database into a Pandas dataframe
	broccoli_df = pd.read_csv(broccoli_db, sep = '\t', header = 0)
	#create a dictionary in the format: og_dict[og_id] = list_of_proteins
	grouped_broccoli_df = broccoli_df.groupby('Broccoli_OG')['Query'].apply(list).reset_index(name="OG_members")
	#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
	#the new column of lists is named 'OG_members'
	broccoli_dict = grouped_broccoli_df.set_index('Broccoli_OG').to_dict()['OG_members']
	#the new dataframe is converted into a dictionary,
	#where the OG IDs are the keys, and the lists of protein members of the OGs are the values
	#write out the filtered protein query database
	with open('broccoli_dict.json', 'w') as broccoli_out:
		#open the JSON outfile for writing
		#and write out the contents of the broccoli_dict dictionary
		json.dump(broccoli_dict, broccoli_out)

	#import OrthoFinder database into a Pandas dataframe
	orthofinder_df = pd.read_csv(orthofinder_db, sep = '\t', header = 0)
	#remove middle column with species information
	orthofinder_df.drop(orthofinder_df.columns[1], axis=1, inplace=True)
	#create a dictionary in the format: og_dict[og_id] = list_of_proteins
	grouped_orthofinder_df = orthofinder_df.groupby('OrthoFinder_OG')['Query'].apply(list).reset_index(name="OG_members")
	#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
	#the new column of lists is named 'OG_members'
	orthofinder_dict = grouped_orthofinder_df.set_index('OrthoFinder_OG').to_dict()['OG_members']
	#the new dataframe is converted into a dictionary,
	#where the OG IDs are the keys, and the lists of protein members of the OGs are the values
	#write out the filtered protein query database
	with open('orthofinder_dict.json', 'w') as orthofinder_out:
		#open the JSON outfile for writing
		#and write out the contents of the orthofinder_dict dictionary
		json.dump(orthofinder_dict, orthofinder_out)

	#import ProteinOrtho database into a Pandas dataframe
	proteinortho_df = pd.read_csv(proteinortho_db, sep = '\t', header = 0)
	#remove middle column with species information
	proteinortho_df.drop(proteinortho_df.columns[1], axis=1, inplace=True)
	#create a dictionary in the format: og_dict[og_id] = list_of_proteins
	grouped_proteinortho_df = proteinortho_df.groupby('ProteinOrtho_OG')['Query'].apply(list).reset_index(name="OG_members")
	#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
	#the new column of lists is named 'OG_members'
	proteinortho_dict = grouped_proteinortho_df.set_index('ProteinOrtho_OG').to_dict()['OG_members']
	#the new dataframe is converted into a dictionary,
	#where the OG IDs are the keys, and the lists of protein members of the OGs are the values
	#write out the filtered protein query database
	with open('proteinortho_dict.json', 'w') as proteinortho_out:
		#open the JSON outfile for writing
		#and write out the contents of the proteinortho_dict dictionary
		json.dump(proteinortho_dict, proteinortho_out)

	#import SonicParanoid database into a Pandas dataframe
	sonicparanoid_df = pd.read_csv(sonicparanoid_db, sep = '\t', header = 0)
	#remove middle column with species information
	sonicparanoid_df.drop(sonicparanoid_df.columns[1], axis=1, inplace=True)
	#create a dictionary in the format: og_dict[og_id] = list_of_proteins
	grouped_sonicparanoid_df = sonicparanoid_df.groupby('SonicParanoid_OG')['Query'].apply(list).reset_index(name="OG_members")
	#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
	#the new column of lists is named 'OG_members'
	sonicparanoid_dict = grouped_sonicparanoid_df.set_index('SonicParanoid_OG').to_dict()['OG_members']
	#the new dataframe is converted into a dictionary,
	#where the OG IDs are the keys, and the lists of protein members of the OGs are the values
	#write out the filtered protein query database
	with open('sonicparanoid_dict.json', 'w') as sonicparanoid_out:
		#open the JSON outfile for writing
		#and write out the contents of the filt_sonicparanoid_dict dictionary
		json.dump(sonicparanoid_dict, sonicparanoid_out)


	#Checkpoint 1 - let the user know program progress
	print("1st Checkpoint: All input datasets have been imported as Pandas dataframes.")


	#define objects to return
	return broccoli_df, broccoli_dict, orthofinder_df, orthofinder_dict, proteinortho_df, proteinortho_dict, sonicparanoid_df, sonicparanoid_dict


def create_prot_dict(broccoli_df, orthofinder_df, proteinortho_df, sonicparanoid_df):
	# Part 3: Creation of the protein query ID to OG cluster assignments list dictionary.
	# Checkpoint 2 is reached at the conclusion of this step, when this dictionary is exported in JSON format.

	#get lists of query proteins from the imported dataframes
	#Broccoli
	broccoli_queries = broccoli_df['Query'].to_list()
	#OrthoFinder
	orthofinder_queries = orthofinder_df['Query'].to_list()
	#ProteinOrtho
	proteinortho_queries = proteinortho_df['Query'].to_list()
	#SonicParanoid
	sonicparanoid_queries = sonicparanoid_df['Query'].to_list()

	#concatenate the lists of queries
	prot_queries = broccoli_queries + orthofinder_queries + proteinortho_queries + sonicparanoid_queries
	#turn the list of queries into a set to eliminate duplicates
	prot_queries = set(prot_queries)


	#create an empty dictionary to populate
	prot_dict = {}

	for prot in prot_queries:
		#iterate through the protein query IDs
		#initially, set the variables for all og categories as "-"
		#these will be overwritten in loops if the protein query was used in the given dataframe
		broccoli_og = "-"
		orthofinder_og = "-"
		proteinortho_og = "-"
		sonicparanoid_og = "-"
		if prot in broccoli_queries:
			#see if protein query ID is in the Broccoli dataframe
			broccoli_og = broccoli_df.loc[broccoli_df['Query'] == prot, 'Broccoli_OG'].iloc[0]
			#with .loc, dind the location where the protein query ID is found in the 'Query' column
			#then extract the contents of that cell, as well as the cell in the same row that is in the 'Broccoli_OG' column
			#use slicing and .iloc to extract the contents of the 'Broccoli_OG' column
			#and replace the contents of variable broccoli_og ("-") with the OG ID
		if prot in orthofinder_queries:
			#see if protein query ID is in the OrthoFinder dataframe
			orthofinder_og = orthofinder_df.loc[orthofinder_df['Query'] == prot, 'OrthoFinder_OG'].iloc[0]
			#with .loc, dind the location where the protein query ID is found in the 'Query' column
			#then extract the contents of that cell, as well as the cell in the same row that is in the 'OrthoFinder_OG' column
			#use slicing and .iloc to extract the contents of the 'OrthoFinder_OG' column
			#and replace the contents of variable orthofinder_og ("-") with the OG ID
		if prot in proteinortho_queries:
			#see if protein query ID is in the ProteinOrtho dataframe
			proteinortho_og = proteinortho_df.loc[proteinortho_df['Query'] == prot, 'ProteinOrtho_OG'].iloc[0]
			#with .loc, dind the location where the protein query ID is found in the 'Query' column
			#then extract the contents of that cell, as well as the cell in the same row that is in the 'ProteinOrtho_OG' column
			#use slicing and .iloc to extract the contents of the 'ProteinOrtho_OG' column
			#and replace the contents of variable proteinortho_og ("-") with the OG ID
		if prot in sonicparanoid_queries:
			#see if protein query ID is in the SonicParanoid dataframe
			sonicparanoid_og = sonicparanoid_df.loc[sonicparanoid_df['Query'] == prot, 'SonicParanoid_OG'].iloc[0]
			#with .loc, dind the location where the protein query ID is found in the 'Query' column
			#then extract the contents of that cell, as well as the cell in the same row that is in the 'SonicParanoid_OG' column
			#use slicing and .iloc to extract the contents of the 'SonicParanoid_OG' column
			#and replace the contents of variable sonicparanoid_og ("-") with the OG ID
		prot_dict[prot] = [broccoli_og, orthofinder_og, proteinortho_og, sonicparanoid_og]
		#populate the prot_dict dictionary with the protein query IDs as the keys
		#and a list of the Broccoli, OrthoFinder, ProteinOrtho & SonicParanoid OGs assigned to that protein query as the values


	#Checkpoint 2 - let the user know program progress
	print("2nd Checkpoint: Protein query to OG match list dictionary has been successfully created. \
		  Printing dicitonary to file.")

	with open('prot_dict.json', 'w') as temp_file:
		#open the JSON outfile for writing
		#and write out the contents of the filt_prot_dict dictionary
		json.dump(prot_dict, temp_file)


	#define objects to return
	return prot_dict


def filter_prot_dict(prot_dict):
	# Part 4: Filtration of the protein query ID to OG assignment list dictionary to exclude
	# protein queries that were only clustered by one program. Checkpoint 3 is reached
	# at the conclusion of this step, when the new dictionary is exported in JSON format.

	#remove from the dictionary proteins that only occur in 1 program
	remove_list = []
	#create an empty list for the protein queries to be deleted

	for key in prot_dict.keys():
		#iterate over the keys of the prot_dict dictionary
		prot_OG_test = set(prot_dict[key])
		#save the value list as a set, eliminating duplicates
		if len(prot_OG_test) == 2:
			#if the length of the set is 2 (ie. the query protein is only clustered by one of the programs)
			#add the protein query key to the list of queries to remove
			remove_list.append(key)


	#create empty dictionary for first filtration - removal of proteins with only 1 OG match
	filt_prot_dict = {}

	for key in prot_dict.keys():
		#iterated through the prot_dict dictionary keys (ie. protein query IDs)
		if key not in remove_list:
			#for keys that aren't in the list of queries that need to be removed
			#save both the key and the value to the new filtered dictionary
			filt_prot_dict[key] = prot_dict[key]


	#Checkpoint 3 - let the user know program progress
	print("3rd Checkpoint: Protein queries only clustered by one program have been eliminated \
		  from the protein query to OG list dictionary. Printing dictionary to file.")

	#write out the filtered protein query database
	with open('filt_prot_dict.json', 'w') as temp_file:
		#open the JSON outfile for writing
		#and write out the contents of the filt_prot_dict dictionary
		json.dump(filt_prot_dict, temp_file)


	#define objects to return
	return filt_prot_dict


def membership_test(filt_prot_dict, broccoli_dict, orthofinder_dict, proteinortho_dict, sonicparanoid_dict):
	# Part 5: The all-vs-all OG membership tests are completed, and a dictionary is created
	# containing the similarity scores of the clusters created by the programs.
	# Checkpoint 4 is reached at the conclusion of this step, when this comparison dictionary is exported in JSON format.

	#create empty dictionary for comparison data
	#dictionary format: comparison_dict[og_vs_pair] = list_of_protein_group_comparison_values
	comparison_dict = {}
	#define the keys of the dictionary with empty lists as associated values
	comparison_dict['Br_vs_OF'] = [] #Broccoli vs OrthoFinder
	comparison_dict['Br_vs_PO'] = [] #Broccoli vs ProteinOrtho
	comparison_dict['Br_vs_SP'] = [] #Broccoli vs SonicParanoid
	comparison_dict['OF_vs_PO'] = [] #OrthoFinder vs ProteinOrtho
	comparison_dict['OF_vs_SP'] = [] #OrthoFinder vs SonicParanoid
	comparison_dict['PO_vs_SP'] = [] #ProteinOrtho vs SonicParanoid


	for key in filt_prot_dict.keys():
		#iterate through the prot_dict dictionary using both keys and values
		#only calculate comparisons in places where both of the compared programs have results for that protein
		if filt_prot_dict[key][0] != "-" and filt_prot_dict[key][1] != "-":
			#comparing OGs where a protein query ID is found in both Broccoli and OrthoFinder
			Br_OG = filt_prot_dict[key][0]
			#extract the OG that the protein query belongs to within the Broccoli results
			Br_OG_compare = broccoli_dict[Br_OG]
			#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
			OF_OG = filt_prot_dict[key][1]
			#extract the OG that the protein query belongs to within the OrthoFinder results
			OF_OG_compare = orthofinder_dict[OF_OG]
			#use the extracted OG ID to query the OrthoFinder OG dictionary, to get the list of proteins in that OG
			sm=difflib.SequenceMatcher(None,Br_OG_compare,OF_OG_compare)
			#compare the similarity of the two protein lists
			#and compute a numerical ratio of that similarity
			br_vs_of = sm.ratio()
			#append the similarity ratio to the list associated with the 'Br_vs_OF' key in the comparison dictionary
			comparison_dict['Br_vs_OF'].append(br_vs_of)
		if filt_prot_dict[key][0] != "-" and filt_prot_dict[key][2] != "-":
			#comparing OGs where a protein query ID is found in both Broccoli and ProteinOrtho
			Br_OG = filt_prot_dict[key][0]
			#extract the OG that the protein query belongs to within the Broccoli results
			Br_OG_compare = broccoli_dict[Br_OG]
			#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
			PO_OG = filt_prot_dict[key][2]
			#extract the OG that the protein query belongs to within the ProteinOrtho results
			PO_OG_compare = proteinortho_dict[PO_OG]
			#use the extracted OG ID to query the ProteinOrtho OG dictionary, to get the list of proteins in that OG
			sm=difflib.SequenceMatcher(None,Br_OG_compare,PO_OG_compare)
			#compare the similarity of the two protein lists
			#and compute a numerical ratio of that similarity
			br_vs_po = sm.ratio()
			#append the similarity ratio to the list associated with the 'Br_vs_PO' key in the comparison dictionary
			comparison_dict['Br_vs_PO'].append(br_vs_po)
		if filt_prot_dict[key][0] != "-" and filt_prot_dict[key][3] != "-":
			#comparing OGs where a protein query ID is found in both Broccoli and SonicParanoid
			Br_OG = filt_prot_dict[key][0]
			#extract the OG that the protein query belongs to within the Broccoli results
			Br_OG_compare = broccoli_dict[Br_OG]
			#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
			SP_OG = filt_prot_dict[key][3]
			#extract the OG that the protein query belongs to within the SonicParanoid results
			SP_OG_compare = sonicparanoid_dict[SP_OG]
			#use the extracted OG ID to query the SonicParanoid OG dictionary, to get the list of proteins in that OG
			sm=difflib.SequenceMatcher(None,Br_OG_compare,SP_OG_compare)
			#compare the similarity of the two protein lists
			#and compute a numerical ratio of that similarity
			br_vs_sp = sm.ratio()
			#append the similarity ratio to the list associated with the 'Br_vs_SP' key in the comparison dictionary
			comparison_dict['Br_vs_SP'].append(br_vs_sp)
		if filt_prot_dict[key][1] != "-" and filt_prot_dict[key][2] != "-":
			#comparing OGs where a protein query ID is found in both OrthoFinder and ProteinOrtho
			PO_OG = filt_prot_dict[key][2]
			#extract the OG that the protein query belongs to within the ProteinOrtho results
			PO_OG_compare = proteinortho_dict[PO_OG]
			#use the extracted OG ID to query the ProteinOrtho OG dictionary, to get the list of proteins in that OG
			OF_OG = filt_prot_dict[key][1]
			#extract the OG that the protein query belongs to within the OrthoFinder results
			OF_OG_compare = orthofinder_dict[OF_OG]
			#use the extracted OG ID to query the OrthoFinder OG dictionary, to get the list of proteins in that OG
			sm=difflib.SequenceMatcher(None,PO_OG_compare,OF_OG_compare)
			#compare the similarity of the two protein lists
			#and compute a numerical ratio of that similarity
			of_vs_po = sm.ratio()
			#append the similarity ratio to the list associated with the 'PO_vs_OF' key in the comparison dictionary
			comparison_dict['OF_vs_PO'].append(of_vs_po)
		if filt_prot_dict[key][1] != "-" and filt_prot_dict[key][3] != "-":
			#comparing OGs where a protein query ID is found in both OrthoFinder and SonicParanoid
			SP_OG = filt_prot_dict[key][3]
			#extract the OG that the protein query belongs to within the SonicParanoid results
			SP_OG_compare = sonicparanoid_dict[SP_OG]
			#use the extracted OG ID to query the SonicParanoid OG dictionary, to get the list of proteins in that OG
			OF_OG = filt_prot_dict[key][1]
			#extract the OG that the protein query belongs to within the OrthoFinder results
			OF_OG_compare = orthofinder_dict[OF_OG]
			#use the extracted OG ID to query the OrthoFinder OG dictionary, to get the list of proteins in that OG
			sm=difflib.SequenceMatcher(None,SP_OG_compare,OF_OG_compare)
			#compare the similarity of the two protein lists
			#and compute a numerical ratio of that similarity
			of_vs_sp = sm.ratio()
			#append the similarity ratio to the list associated with the 'SP_vs_OF' key in the comparison dictionary
			comparison_dict['OF_vs_SP'].append(of_vs_sp)
		if filt_prot_dict[key][2] != "-" and filt_prot_dict[key][3] != "-":
			#comparing OGs where a protein query ID is found in both ProteinOrtho and SonicParanoid
			PO_OG = filt_prot_dict[key][2]
			#extract the OG that the protein query belongs to within the ProteinOrtho results
			PO_OG_compare = proteinortho_dict[PO_OG]
			#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
			SP_OG = filt_prot_dict[key][3]
			#extract the OG that the protein query belongs to within the SonicParanoid results
			SP_OG_compare = sonicparanoid_dict[SP_OG]
			#use the extracted OG ID to query the SonicParanoid OG dictionary, to get the list of proteins in that OG
			sm=difflib.SequenceMatcher(None,PO_OG_compare,SP_OG_compare)
			#compare the similarity of the two protein lists
			#and compute a numerical ratio of that similarity
			po_vs_sp = sm.ratio()
			#append the similarity ratio to the list associated with the 'PO_vs_OF' key in the comparison dictionary
			comparison_dict['PO_vs_SP'].append(po_vs_sp)

	'''
	Some references for the list comparisons:

	Using difflib:
	https://docs.python.org/3/library/difflib.html
	https://stackoverflow.com/questions/6709693/calculating-the-similarity-of-two-lists

	Using cosine similarity:
	https://stackoverflow.com/questions/28819272/python-how-to-calculate-the-cosine-similarity-of-two-word-lists
	https://stackoverflow.com/questions/14720324/compute-the-similarity-between-two-lists
	https://en.wikipedia.org/wiki/Cosine_similarity

	Both of these methods yeild similar results, within 0.02 (compared two lists with different similarities).
	I chose to use difflib because of its far simpler implementation.

	'''

	'''
	A note on the scoring mechanism used above:

	The way the scoring is done, if clusters are particularly similar between two programs, with the same
	proteins appearing in the OGs of both, the OG's will in some sense be scored multiple times.

	I justify the choice to allow this because giving more weight to programs that are providing more
	similar clusters is useful when trying to quantify the quality of the orthologous clusters these
	programs produce. A cluster that is identified by multiple programs is more likely to be reliable; and
	programs that are identifying similar clusters are likely more adept at identifying OGs within
	these particularly divergent, unique organisms.

	'''

	#Checkpoint 4 - let the user know program progress
	print("4th Checkpoint: OG scoring dictionary has been successfully created. Printing dictionary to file.")

	with open('compare_OG_dict.json', 'w') as temp_file:
		#open the JSON outfile for writing
		#and write out the contents of the filt_prot_dict dictionary
		json.dump(comparison_dict, temp_file)


	#define objects to return
	return comparison_dict


def avg_membership_scores(comparison_dict):
	# Part 6: The scores inside of the comparison dictionary are averaged. Checkpoint 4.5 is
	# reached at the conclusion of this step, when this smaller dictionary is exported in JSON format.
	#create a new empty dictionary to hold the average scores
	og_score_dict = {}

	for key in comparison_dict.keys():
		#iterate over the comparison score dictionary keys
		og_score_avg = statistics.mean(comparison_dict[key])
		#compute the mean/average of the scores in each list in the comparison dictionary
		#and save the average score per commparison to a new dictionary, using the same keys
		og_score_dict[key] = og_score_avg


	#Checkpoint 4.5 - let the user know program progress
	print("4.5th Checkpoint: OG comparison scores have been successfully created. Printing dictionary to file.")

	with open('og_score_dict.json', 'w') as temp_file:
		#open the JSON outfile for writing
		#and write out the contents of the filt_prot_dict dictionary
		json.dump(og_score_dict, temp_file)


	#define objects to return
	return og_score_dict


def threshold_test(og_score_dict):
	# Part 7: The threshold membership percentage value is used to filter the programs with
	# the most similar clusters, and a new dictionary is created to containing only
	# this data. Checkpoint 5 is reached at the conclusion of this step.

	#create new dictionary for OG average scores that meet the desired threshold value
	threshold_dict = {}

	#convert membership percentage to a decimal
	membership_decimal = float(membership_percent)/100

	for key in og_score_dict.keys():
		#iterate over the average scores dictionary's keys
		if og_score_dict[key] >= membership_decimal:
			#filter the comparisons that are similar enough to pass the given threshold value
			#those comparisons that pass, as well as the associated score average, should be copied to the threshold dictionary
			threshold_dict[key] = og_score_dict[key]


	#Checkpoint 5 - let the user know program progress
	print("5th Checkpoint: Orthologous clustering program evaluation based on user-supplied threshold of " +
		  str(membership_percent) + " has been completed. \
			  Printing results to output file.")

	#define objects to return
	return threshold_dict


###
# Running program, based on command-line input
###


#first, set the membership percentage
if len(sys.argv) == 6: #if the primary method of running the program is used
	#check if the user gave a threshold value for filtering average cluster similarity
	#if so, import the selected membership test threshold as the fifth command-line argument
	membership_percent = sys.argv[5]
if len(sys.argv) == 3: #if the program is run from a later checkpoint
	#check if the user gave a threshold value for filtering average cluster similarity
	#if so, import the selected membership test threshold as the fifth command-line argument
	membership_percent = sys.argv[2]
else:
	#if no threshold value is given by the user, use the default value of 50
	membership_percent = 50

#define the output file with a pre-determined prefix & suffix, separated by the membership percent
membership_results = "OG_membership_results_" + str(membership_percent) + ".txt"


#next, set the dataframe command-line arguments for primary program usage method
if len(sys.argv) >= 5:
	#assign command line arguments; load input and output files
	#import the parsed Broccoli results as the first command-line argument
	broccoli_db = sys.argv[1]
	#broccoli_db = "Broccoli_OGs_parsed.txt"
	#import the parsed OrthoFinder results as the second command-line argument
	orthofinder_db = sys.argv[2]
	#orthofinder_db = "OF_OGs_parsed.txt"
	#import the parsed ProteinOrtho results as the third command-line argument
	proteinortho_db = sys.argv[3]
	#proteinortho_db = "PO_OGs_parsed.txt"
	#import the parsed SonicParanoid results as the fourth command-line argument
	sonicparanoid_db = sys.argv[4]
	#sonicparanoid_db = "SP_OGs_parsed.txt"

	#now, use appropriate functions - in this case, all
	broccoli_df, broccoli_dict, orthofinder_df, orthofinder_dict, proteinortho_df, proteinortho_dict, sonicparanoid_df, sonicparanoid_dict = data_2_pandas(broccoli_db, orthofinder_db, proteinortho_db, sonicparanoid_db)
	prot_dict = create_prot_dict(broccoli_df, orthofinder_df, proteinortho_df, sonicparanoid_df)
	filt_prot_dict = filter_prot_dict(prot_dict)
	comparison_dict = membership_test(filt_prot_dict, broccoli_dict, orthofinder_dict, proteinortho_dict, sonicparanoid_dict)
	og_score_dict = avg_membership_scores(comparison_dict)
	threshold_dict = threshold_test(og_score_dict)


#for secondary usage method, loading prot_dict:
if sys.argv[1] == 'prot_dict.json':
	#open the primary JSON input file
	with open('prot_dict.json') as json_file:
		#open the JSON file containing the filt_prot_dict data
		#and load it as a dictionary
		prot_dict = json.load(json_file)
	#open the clustering program dictionaries
	with open('broccoli_dict.json') as broccoli_in:
		#open the JSON file containing the filt_prot_dict data
		#and load it as a dictionary
		broccoli_dict = json.load(broccoli_in)
	with open('orthofinder_dict.json') as orthofinder_in:
		#open the JSON file containing the orthofinder_dict data
		#and load it as a dictionary
		orthofinder_dict = json.load(orthofinder_in)
	with open('proteinortho_dict.json') as proteinortho_in:
		#open the JSON file containing the proteinortho_dict data
		#and load it as a dictionary
		proteinortho_dict = json.load(proteinortho_in)
	with open('sonicparanoid_dict.json') as sonicparanoid_in:
		#open the JSON file containing the sonicparanoid_dict data
		#and load it as a dictionary
		sonicparanoid_dict = json.load(sonicparanoid_in)

	#now, use appropriate functions - in this case, from filtration on
	filt_prot_dict = filter_prot_dict(prot_dict)
	comparison_dict = membership_test(filt_prot_dict, broccoli_dict, orthofinder_dict, proteinortho_dict, sonicparanoid_dict)
	og_score_dict = avg_membership_scores(comparison_dict)
	threshold_dict = threshold_test(og_score_dict)


#for secondary usage method, loading filt_prot_dict:
if sys.argv[1] == 'filt_prot_dict.json':
	#open the primary JSON intermediate results file
	with open('filt_prot_dict.json') as json_file:
		#open the JSON file containing the filt_prot_dict data
		#and load it as a dictionary
		filt_prot_dict = json.load(json_file)
	#open the clustering program dictionaries
	with open('broccoli_dict.json') as broccoli_in:
		#open the JSON file containing the filt_prot_dict data
		#and load it as a dictionary
		broccoli_dict = json.load(broccoli_in)
	with open('orthofinder_dict.json') as orthofinder_in:
		#open the JSON file containing the orthofinder_dict data
		#and load it as a dictionary
		orthofinder_dict = json.load(orthofinder_in)
	with open('proteinortho_dict.json') as proteinortho_in:
		#open the JSON file containing the proteinortho_dict data
		#and load it as a dictionary
		proteinortho_dict = json.load(proteinortho_in)
	with open('sonicparanoid_dict.json') as sonicparanoid_in:
		#open the JSON file containing the sonicparanoid_dict data
		#and load it as a dictionary
		sonicparanoid_dict = json.load(sonicparanoid_in)

	#now, use approtpriate functions - in this case, from membership test on
	comparison_dict = membership_test(filt_prot_dict, broccoli_dict, orthofinder_dict, proteinortho_dict, sonicparanoid_dict)
	og_score_dict = avg_membership_scores(comparison_dict)
	threshold_dict = threshold_test(og_score_dict)


#for secondary usage method, loading comparison_dict:
if sys.argv[1] == 'comparison_dict.json':
	with open('compare_OG_dict.json') as json_file:
		#open the JSON file containing the filt_prot_dict data
		#and load it as a dictionary
		comparison_dict = json.load(json_file)

	#now, use appropriate functions - in this case, from avergaing the membership scores on
	og_score_dict = avg_membership_scores(comparison_dict)
	threshold_dict = threshold_test(og_score_dict)


#for secondary usage method, loading og_score_dict:
if sys.argv[1] == 'og_score_dict.json':
	with open('og_score_dict.json') as json_file:
		#open the JSON file containing the filt_prot_dict data
		#and load it as a dictionary
		og_score_dict = json.load(json_file)

	#now, use appropriate functions - in this case, only the threshold dictionary creation
	threshold_dict = threshold_test(og_score_dict)


###
# Outfile writing is executed for ALL variations of program runs
###


# Part 8: The contents ot the dictionary created in the final step (ie. the final,
# membership threshold-filtered data) is written out to a text file.

with open(membership_results, "w") as outfile:
	#open the results file for writing
	if bool(threshold_dict) == False:
		#test if the dictionary exists - high threshold values may yield an empty threshold_dict
		outfile.write("The given threshold value of " + str(membership_percent) + "% is too high. No OG comparisons met this criteria.")
	else:
		#otherwise write an introductory line to the file
		outfile.write("The orthologous clustering similarity comparisons that met the desired threshold value of " + str(membership_percent) +
				   "% are listed below:" + "\n\n")
		for key in threshold_dict.keys():
			#iterate through the threshold dictionary by its keys
			#and write out to the results file the comparisons that met the required thresshold value
			outfile.write("For the " + key + " comparison, the average score value is: " + str(threshold_dict[key]) + "\n")

```

Using it:

```bash
#model:
python og_membership_test.py broccoli_db orthofinder_db proteinortho_db sonicparanoid_db [membership_percent]
#applying it:
python ../../Scripts/og_membership_test.py Broccoli_OGs_parsed.txt OF_OGs_parsed.txt PO_OGs_parsed.txt SP_OGs_parsed.txt 45
# 1st Checkpoint: All input datasets have been imported as Pandas dataframes.
# 2nd Checkpoint: Protein query to OG match list dictionary has been successfully created.          Printing dicitonary to file.
# 3rd Checkpoint: Protein queries only clustered by one program have been eliminated        from the protein query to OG list dictionary. Printing dictionary to file.
# 4th Checkpoint: OG scoring dictionary has been successfully created. Printing dictionary to file.
# 4.5th Checkpoint: OG comparison scores have been successfully created. Printing dictionary to file.
# 5th Checkpoint: Orthologous clustering program evaluation based on user-supplied threshold of 45 has been completed.               Printing results to output file.
# Traceback (most recent call last):
#   File "../../Scripts/og_membership_test.py", line 612, in <module>
#     outfile.write("For the " + key + " comparison, the average score value is: " + threshold_dict[key] + "\n")
# TypeError: can only concatenate str (not "float") to str
#oh hey, whoops! but that's an easy fix
#and conveniently enough, it allows me to test out the secondary run method
#model:
python og_membership_test.py [JSON_file] [membership_percent]
#applying it:
python ../../Scripts/og_membership_test.py og_score_dict.json 45
# 5th Checkpoint: Orthologous clustering program evaluation based on user-supplied threshold of 45 has been completed.                       Printing results to output file.
#yes, that worked!!!
#ok, so now for one last full test
#move all results of to a new directory
#and try to see if I can run it in the background
python ../../Scripts/og_membership_test.py Broccoli_OGs_parsed.txt OF_OGs_parsed.txt PO_OGs_parsed.txt SP_OGs_parsed.txt > membership_test_50.log &
# [1] 2793319
#hmmm... in-progress `less` doesn't show anything in the .log file,
#even though the program is definitley running
#well, guess we'll just have to wait and see :shrug:
fg
#ended the job, and then it spat out the results file with the checkpoint information
#so apparently it only prints at the end
#which is weird and not particularly useful, but oh well
#I've spent enough time getting this program to work, I'm not going to figure out how to smooth that over
#so run it again, and this time we can just leave it in the background
python ../../Scripts/og_membership_test.py Broccoli_OGs_parsed.txt OF_OGs_parsed.txt PO_OGs_parsed.txt SP_OGs_parsed.txt > membership_test_50.log &
# [1] 2793588
#and done! it worked properly!
###
#realized the script didn't account for the duplicates in the Broccoli OG IDs, so! 
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/nonDuplicate_45_results/ directory 
python ../../../Scripts/og_membership_test.py ../Broccoli_OGs_parsed_nonDuplicate.txt ../OF_OGs_parsed.txt ../PO_OGs_parsed.txt ../SP_OGs_parsed.txt 45 > membership_test_45.log &
# [1] 4090745
#for some reason doing it this way made the filtration be 50%, but whatever, it's fine

```

#### Species membership thresholds

Program to extract data on sequence lengths of protein queries in a FASTA file (script saved to calc_seq_length.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: calc_seq_length.py
Date: 2022-03-15
Author: Virág Varga

Description:
	This program parses a FASTA file and outputs a dataframe in the format:
		Query\tSequence_Length

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules & assigning command line argument.
	2. Parsing FASTA file to create dictionary of sequence lengths.
	3. Converting dictionary to Pandas dataframe & writing out results to a
		tab-separated text file.


Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is pre-determined (not user-defined), based on
		the name of the input FASTA file.

Usage
	./calc_seq_length.py input_fasta
	OR
	python calc_seq_length.py input_fasta

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import modules, set up command-line arguments

#import necessary modules
import sys #allows execution of script from command line
import pandas as pd #allows manipulation of dataframes in Python


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "Test_Files/EP00771_Trimastix_marina_edit.fasta"
output_file = ".".join(input_fasta.split('.')[:-1]) + '__SeqLength.txt'


#Part 2: Parse FASTA file to create dictionary of sequence lengths

#create empty dictionary to save sequence length data to
seq_length_dict = {}

with open(input_fasta, "r") as infile:
	#open the input FASTA file for reading
	for line in infile:
		#iterate through the file line by line
		line = line.strip()
		#remove the endline character ("\n") from the line
		if line.startswith(">"):
			#identify the header lines
			header = line[1:]
			#save the FASTA header without the ">" character at the beginning
			sequence = next(infile).strip()
			#move to the next line in the file (sequence line), and save that to variable
			#remove endline character ("\n") here, too
			seq_length = len(sequence)
			#calculate and save the sequence length to a variable
			#save the header (protein query ID) and sequence length to the sequence length dictionary
			seq_length_dict[header] = seq_length


#Part 3: Convert dictionary to Pandas dataframe & write out

#convert dictionary to Pandas dataframe
seq_length_df = pd.DataFrame.from_dict(seq_length_dict, orient='index', columns=['Seq_Length'])

#write out results to tab-separated text file
seq_length_df.to_csv(output_file, sep='\t', index=True, header=False)
#need `index=True` because the dictionary to dataframe conversion makes the queries into an index column
#use `header=False` to prevent column headers from printing, so species files can be concatenated easily

```

Using it:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DataFiles/ProtLists/ directory
#running the program in a loop
ls ../EncodedData/*.fasta | while read file; do
	python calc_seq_length.py $file;
done
#moving the files
mv ../EncodedData/*__SeqLength.txt Prot_Length/
cd Prot_Length/
#concatenating the species-based files for use in the next script
cat *__SeqLength.txt > prot_lengths_ref.txt

```

Program to merge the sequence length reference file with the species protein query IDs to species categories & phyla dataframe (script saved to merge_prot_spp_length.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: merge_prot_spp_length.py
Date: 2022-03-12
Author: Virág Varga

Description:
	This program merges the protein query IDs to species categories & phyla dataframe
		with the protein query IDs to sequence lengths dataframe, and writes out the
		resulting dataframe to a tab-separated text file.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Assigning command-line arguments, import modules.
	2. Importing input data into Pandas dataframes.
	3. Merging species and sequence length dataframes.
	4. Writing out the dataframe to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.

Usage
	./merge_prot_spp_length.py species_db seqlength_db output_db
	OR
	python merge_prot_spp_length.py species_db seqlength_db output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Assign command-line arguments, import modules

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line arguments; load input and output files
species_db = sys.argv[1]
#species_db = "Prots_Species_Phyla_DB.txt"
seqlength_db = sys.argv[2]
#seqlength_db = "prot_lengths_ref.txt"
output_db = sys.argv[3]
#output_db = "Prots_Species_Phyla_SeqLength_DB.txt"


#Part 2: Import data into Pandas dataframes

#import query to species categories dataframe into Pandas
species_df = pd.read_csv(species_db, sep = "\t", header=0)

#set names of columns for imported sequence length dataframe
colnames=['Query', 'Sequence_Length']
#import sequence length database into a Pandas dataframe
seq_length_df = pd.read_csv(seqlength_db, sep = '\t', names=colnames, header=None)


#Part 3: Merge dataframes

merged_df = species_df.merge(seq_length_df, how='outer')
#since the species_df is given the merge command, the data from it will be on the left,
#while the data from seq_length_df will be on the right
#the `how='outer'` argument ensures that all queries are kept, even the ones without overlap
#it isn't particularly important here, though, since the dataframes are the same length


#Part 4: Write out resulting dataframe to a tab-separated text file

#write out the resulting dataframe
merged_df.to_csv(output_db, sep='\t', index=False)

```

Using it:

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model:
python merge_prot_spp_length.py species_db seqlength_db output_db
#applying it:
python ../../Scripts/merge_prot_spp_length.py Prots_Species_Phyla_DB.txt prot_lengths_ref.txt Prots_Species_Phyla_SeqLength_DB.txt

```

Dealing with the duplicated (chimeric) proteins from the Broccoli results by removing them (script saved to remove_duplicatesBR__ALL.py): 
 - This program should create an output file which does not include the chimeric proteins in the OGs
 - It should also have an optional argument that allows the user to ask for an additional file which is formatted like so: Query\tBr_GroupedOGs\tBr_nonDuplicateOGs

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: remove_duplicatesBR__ALL.py
Date: 2022-03-21
Author: Virág Varga

Description:
	This program deals with the duplicates in the parsed Broccoli OG data in 1 or 2 ways:
		1. (Mandatory) Outputs a file containing only those protein query IDs that
			were not grouped into more than 1 OG.
		2. (Optional) Outputs an additional results file in the format:
			Query\tBr_Grouped_OGs\tBr_Single_OGs

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules & assigning command line arguments.
	2. Importing data into Pandas dataframe.
	3. Creating dataframe without duplicates and writing it out to a tab-separated
		text file.
	4. (Optional): Creating output dataframe with both grouped and dropped OG ID info,
		and writing it out to a tab-separated text file.


Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the mandatory output file is pre-determined (not user-defined), based on
		the name of the input FASTA file.

Usage
	./remove_duplicatesBR__ALL.py broccoli_db [drop_group_db]
	OR
	python remove_duplicatesBR__ALL.py broccoli_db [drop_group_db]

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import modules, set up command-line arguments

#import necessary modules
import sys #allows execution of script from command line
import pandas as pd #allows manipulation of dataframes in Python


#load input and output files
broccoli_db = sys.argv[1]
#broccoli_db = "Broccoli_OGs_parsed.txt"

#his outfile will be produced with both teh standard and optional runs
#prot_ref_db = "Prots_Species_Phyla_SeqLength_DB.txt"
output_db = ".".join(broccoli_db.split('.')[:-1]) + '_nonDuplicate.txt'


#Part 2: Import data into appropriate data into Pandas dataframe

#import Broccoli database into Pandas dataframe
broccoli_df = pd.read_csv(broccoli_db, sep = "\t", header=0)


#Part 3: Create dataframe without duplicates and write out

#drop all queries IDs that have duplicates
non_duplicate_df = broccoli_df.drop_duplicates(subset=['Query'], keep=False)

#write out the resulting dataframe in tab-separated format
non_duplicate_df.to_csv(output_db, sep='\t', index=False)


#Part 4 (Optional): Create output dataframe with both grouped and dropped OG ID info

if len(sys.argv) == 3:
	#if the program is run with the option to create a databbase with dropped AND clustered OG IDs
	drop_group_db = sys.argv[2]
	#drop_group_db = "Broccoli_OGs_parsed__Group-Drop.txt"
	#assign the last argument as the output file name

	#create dataframe of only duplicated query IDs
	duplicate_df = broccoli_df[broccoli_df.duplicated(subset=['Query'], keep=False)]
	#create dataframe grouping the OG IDs based on the query IDs
	grouped_duplicate_df = duplicate_df.groupby('Query')['Broccoli_OG'].apply(list).reset_index(name="Broccoli_OG")
	#the OGs in the 'Broccoli_OG' column are grouped into lists according to the protein they are associated with
	#the new column of lists is named 'Broccoli_OG'
	#now sort out the formatting of those lists of queries
	grouped_duplicate_df['Broccoli_OG'] = grouped_duplicate_df['Broccoli_OG'].apply(lambda x: ', '.join(map(str, x)))

	#now combine the grouped dataframe with the non-duplicate dataframe
	grouped_df = pd.concat([non_duplicate_df, grouped_duplicate_df], axis=0)
	#rename second column
	grouped_df.rename(columns={'Broccoli_OG':'Br_Grouped_OGs'}, inplace=True)

	#create merged dataframe with both the dropped and and grouped data in the columns
	#first, reset the indices of the dataframes to be merged
	grouped_df.set_index('Query')
	non_duplicate_df.set_index('Query')
	#now perform the actual merge
	merged_df = grouped_df.merge(non_duplicate_df, how='outer')
	#rename third column
	merged_df.rename(columns={'Broccoli_OG':'Br_Single_OGs'}, inplace=True)
	#and fill NaN values with "-"
	merged_df.fillna('-', inplace=True)

	#now write out the results to a tab-separated text file
	merged_df.to_csv(drop_group_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model: 
python remove_duplicatesBR__ALL.py broccoli_db [drop_group_db]
#using it: 
python ../../Scripts/remove_duplicatesBR__ALL.py Broccoli_OGs_parsed.txt Broccoli_OGs_parsed__Group-Drop.txt

```

Finding OGs with at least one representative per phylum (script saved to rep_4_phyla.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: rep_4_phyla.py
Date: 2022-03-21
Author: Virág Varga

Description:
	This program performs filters the parsed results of orthologous clustering software
		(OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) in order to identify OGs
		that are present in either the 4 main phyla, or the 4 main phyla and more basal
		species.
		An optional argument allows the isolation of OGs that are found in all 4 phyla
			and the basal species (Barthelona).

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	sys
	pandas
	os

Procedure:
	1. Importing necessary modules, assigning command-line arguments.
	2. Importing data into Pandas dataframes.
	3. Setting up data types (dataframe for ref file, dictionary for infile) that will be used.
	4. Creating dictionary of phyla represented in each OG.
	5. Identifying OGs that include representatives of all 4 phyla.
	6. Copying threshold-meeting OG information into a new dataframe & writing out.
	7. (Optional) Writing out only those OGs that include the 4 main phyla and Barthelona
		This step can be called with the use of the string "other" as the 4th command-line argument.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file(s) is/are not user-defined.
	- The input file used for the program must be an un-pivoted results file of either
		Broccoli, OrthoFinder, SonicParanoid, or ProteinOrtho, following the structure
		used by the parsers used previously in this workflow.
	- The program cannot accept multiple input parsed OG files simultaneously.
	- In order to activate the optional argument to output OGs including the 4 main phyla
		and Barthelona, the user must provide the string "other" as the 4th command-line argument.

Usage:
	./rep_4_phyla.py input_db ref_db ["other"]
	OR
	python rep_4_phyla.py input_db ref_db ["other"]

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = sys.argv[1]
#designate reference file name as variable
ref_db = sys.argv[2]

#define the output file based on the input file name
base = os.path.basename(infile)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_min4Phyla.txt"


#Part 2: Import data into Pandas dataframes

#import input file into pandas dataframe
ortho_df = pd.read_csv(infile, sep = '\t', header = 0)

#remove 3rd column from Pandas dataframe if necessary
#needed for original parsed results of OrthoFinder, ProteinOrtho & SonicParanoid
col_num = len(ortho_df.columns)
#count number of columns
if col_num == 3:
	#select dataframes with 3 columns
	#remove middle column with species information
	ortho_df.drop(ortho_df.columns[1], axis=1, inplace=True)


#import reference file into Pandas dataframe
ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)


#Part 3: Set up data types (dataframe for ref file, dictionary for infile) that will be used

#identify OG column name (for use later)
og_col = ortho_df.columns[1]

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 4: Create dictionary of phyla represented in each OG

#create empty dictionary for phyla information
phyla_OG_dict = {}

for key in ortho_dict.keys():
	#iterate over the dictionary via its keys
	phyla_list = []
	#create empty list that will be populated with phyla information
	og_prot_list = ortho_dict[key]
	#save list of protein query IDs associated with the given OG
	for prot in og_prot_list:
		#iterate over the list of protein query IDs
		prot_phyla = ref_df.loc[ref_df['Query'] == prot, 'Phylum'].iloc[0]
		#with .loc, find the location where the protein query ID is found in the 'Query' column
		#then extract the contents of that cell, as well as the cell in the same row that is in the 'Phylum' column
		#use slicing and .iloc to extract the contents of the 'Phylum' column
		#and save the phyla category to variable prot_species
		#append the phyla category to the species_list
		phyla_list.append(prot_phyla)
	phyla_set = set(phyla_list)
	#turn the phyla_list into a set to eliminate duplicates
	#and save the phyla_set as the value in the OG to species dictionary
	phyla_OG_dict[key] = phyla_set


#Part 5: Identify OGs that include representatives of all 4 phyla

#create list of 4 main phyla
main_phyla_list = ['Anaeramoebidae', 'Parabasalia', 'Fornicata', 'Preaxostyla']
#create list of 5 existing phyla
full_phyla_list = ref_df.Phylum.unique()


#create new dictionary for data that meets the threshold
threshold_og_list = []

for key in phyla_OG_dict.keys():
	#iterate over the dictionary via its keys
	if (phyla_OG_dict[key] == set(main_phyla_list)) or (phyla_OG_dict[key] == set(full_phyla_list)):
		#identify the OGs that include members of at least 4 phyla
		#and save the OG to the list of threshold_og_list
		threshold_og_list.append(key)


#Part 6: Copy threshold-meeting OG information into a new dataframe & write out

#create new dataframe with good OGs
threshold_df = ortho_df[ortho_df[og_col].isin(threshold_og_list)].copy()
#use `.isin()` to iterate over entire list of OGs
#use `.copy()` to ensure the dataframe is seperate from the ortho_df

#Writing out the results to a tab-separated text file
threshold_df.to_csv(output_file, sep='\t', index=False)


#Part 7 (Optional): Write out only those OGs that include the 4 main phyla and Barthelona

if len(sys.argv) == 4:
	#identify cases where the final, optional argument was used
	if sys.argv[3] == "other":
		#check to ensure that the item in this position isn't a random accident
		output_file_v2 = out_full + "_5Phyla.txt"

		#create new dictionary for data that meets the threshold
		threshold_v2_og_list = []

		for key in phyla_OG_dict.keys():
			#iterate over the dictionary via its keys
			if phyla_OG_dict[key] == set(full_phyla_list):
				#identify the OGs that include members of all 5 phyla
				#and save the OG to the list of threshold_og_list
				threshold_v2_og_list.append(key)

		#create new dataframe with good OGs
		threshold_v2_df = ortho_df[ortho_df[og_col].isin(threshold_v2_og_list)].copy()
		#use `.isin()` to iterate over entire list of OGs
		#use `.copy()` to ensure the dataframe is seperate from the ortho_df

		#Writing out the results to a tab-separated text file
		threshold_v2_df.to_csv(output_file_v2, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model: 
python rep_4_phyla.py input_db ref_db ["other"]
#using it: 
python ../../Scripts/rep_4_phyla.py Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt other
python ../../Scripts/rep_4_phyla.py OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt other
python ../../Scripts/rep_4_phyla.py PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt other
python ../../Scripts/rep_4_phyla.py SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt other
###
#Non-Duplicate Broccoli
python ../../Scripts/rep_4_phyla.py Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt other

```

Reporting OGs with n% overlap when comparing two orthologous clustering programs (script saved to og_overlap_percent.py): 
 - For this program, use as input one of the .json files created by the og_membership_test.py script. This file should probably be either prot_dict.json or filt_prot_dict.json

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: og_overlap_percent.py
Date: 2022-03-21
Author: Virág Varga

Description:
	This program imports the JSON dictionary output by the og_membership_test.py program
		containing information on the OGs associated with a protein query ID from the 4 analysis
		programs used in this workflow (Broccoli, OrthoFinder, ProteinOrtho, SonicParanoid),
		and performs all-vs-all comparisons to identify the OGs with the greatest similarity.
	The program outputs the following files:
		- JSON dictionaries:
			prot_comparison_dict[query_ID] = [[prog1, prog1_OG_ID, prog2, prog2_OG_ID, similiarity_score],
									 [prog1, prog1_OG_ID, prog3, prog3_OG_ID, similiarity_score]]
			comparison_dict[prog_vs_prog] = [[prog1_OG1, prog1_OG2, etc.], [prog2_OG2, prog2_OG2]]
			Filtered version of the dictionary above, excluding protein queries with no comparisons 
				that met the threshold value. 
		- Tab-separated dataframe text files:
			Query\tBroccoli_OG\tOrthoFinder_OG\tPrtoeinOrtho_OG\tSonicParanoid
			And filtered version of the above, removing Query IDs and filtering out duplicate rows
			Dataframes for individual programs, with standard parsed OG format: Query\tPROG_OG
		- Text file summarizing results

List of functions:
	list_duplicates_of(seq,item): Creates list of indexes of repeated element of list
		Source: https://stackoverflow.com/a/5419576/18382033

List of standard and non-standard modules used:
	sys
	json
	difflib
	pandas
	itertools.chain

Procedure:
	1. Importing necessary modules, assigning command-line arguments.
	2. Opening data files and importing into best data types for manipulation.
	3. Finding orthologous groups that meet the threshold similarity using all-vs-all membership tests,
		before filtering out protein queries that have no matching OG pairs that meet the similarity 
		threshold value. 
		Note that 3 JSON dictionaries will be written out at the conclusion of this process.
	4. Creating and writing out comparison dictionaries-based dataframes.
	5. Creating and writing out program-specific OG dataframes.
	6. Writing out summary text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The names of the output files are not user-defined.
	- The input file used for the program must be either the prot_dict.json file or
		filt_prot_dict.json file produced by the og_membership_test.py script.
	- This program must be run from the directory containing the input .json file. It will
		automatically search in that directory for the following files: broccoli_dict.json,
		orthofinder_dict.json, proteinortho_dict.json, and sonicparanoid_dict.json. These
		files are all also produced by the og_membership_test.py program.
	- This program is intended for use with parsed Broccoli results from which duplicates have 
		already been removed. 

Usage:
	./og_overlap_percent.py prot_to_OG_db output_base [membership_threshold]
	OR
	python og_overlap_percent.py prot_to_OG_db output_base [membership_threshold]

This script was written for Python 3.8.12, in Spyder 5.1.5.
"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import json #allows transfer of data into and out of JSON files
import difflib #compare and calculate differences between datasets
import pandas as pd #allows manipulation of dataframes in Python
from itertools import chain #allows manipulation of nested lists


#define function to identify indexes of recorring element in list
#ref: https://stackoverflow.com/questions/5419204/index-of-duplicates-items-in-a-python-list
def list_duplicates_of(seq, item):
	#takes the list to search in as first argument
	#takes the string/element to search for the indices of as the second argument
    start_at = -1
	#begins searching at the final postion in the list
    locs = []
	#defines an empty list to 
    while True:
		#loops through the elements of the list
        try:
            loc = seq.index(item,start_at+1)
			#indexes the location of the element
        except ValueError:
			#when there are no more elements to loop through, break the loop
            break
        else:
			#add the index to the list of indices
            locs.append(loc)
			#and update the index being looped through
            start_at = loc
	#return the list of indices as function output
    return locs


#assign and interpret inputs
prot_to_OG_db = sys.argv[1]
#prot_to_OG_db = "filt_prot_dict.json"

if len(sys.argv) == 3:
	#if no membership threshold is given by the user,
	#then 80% should be used as the default
	membership_threshold = 80
if len(sys.argv) == 4:
	#if the user gives a membership threshold, save the percent value to a variable
	membership_threshold = sys.argv[3]

#convert membership percentage to a decimal
membership_decimal = float(membership_threshold)/100


#define output files based on input
output_base = sys.argv[2]
#output_base = "OG_membership_overlap"

#output files (non-JSON)
#overlap dataframes
output_full_query_db = output_base + "_Query" + str(membership_threshold) + ".txt"
output_non_query_db = output_base + "_nonQuery" + str(membership_threshold) + ".txt"

#species-specific dataframes
broccoli_db = "Broccoli_OGs_parsed_overlap" + str(membership_threshold) + ".txt"
orthofinder_db = "OF_OGs_parsed_overlap" + str(membership_threshold) + ".txt"
proteinortho_db = "PO_OGs_parsed_overlap" + str(membership_threshold) + ".txt"
sonicparanoid_db = "SP_OGs_parsed_overlap" + str(membership_threshold) + ".txt"

#output text file
outfile_summary = output_base + "__SUMMARY_" + str(membership_threshold) + ".txt"


#Part 2: Opening data files and importing into best data types for manipulation

#the filtered protein ID to list of assigned OGs dictionary
with open(prot_to_OG_db) as json_file:
	#open the JSON file containing the filt_prot_dict data
	#and load it as a dictionary
	filt_prot_dict = json.load(json_file)

#open the clustering program dictionaries
with open('broccoli_dict.json') as broccoli_in:
	#open the JSON file containing the filt_prot_dict data
	#and load it as a dictionary
	broccoli_dict = json.load(broccoli_in)
with open('orthofinder_dict.json') as orthofinder_in:
	#open the JSON file containing the orthofinder_dict data
	#and load it as a dictionary
	orthofinder_dict = json.load(orthofinder_in)
with open('proteinortho_dict.json') as proteinortho_in:
	#open the JSON file containing the proteinortho_dict data
	#and load it as a dictionary
	proteinortho_dict = json.load(proteinortho_in)
with open('sonicparanoid_dict.json') as sonicparanoid_in:
	#open the JSON file containing the sonicparanoid_dict data
	#and load it as a dictionary
	sonicparanoid_dict = json.load(sonicparanoid_in)


#Part 3: Finding orthologous groups that meet the threshold similarity using all-vs-all membership tests

#create empty dictionaries for comparison data
#dictionary format: prot_comparison_dict[query_ID] =
#[[prog1, prog1_OG_ID, prog2, prog2_OG_ID, similiarity_score], [prog1, prog1_OG_ID, prog3, prog3_OG_ID, similiarity_score], etc.]
#values from comparisons should only be saved to the dictionary if they meet the threshold similarity value
prot_comparison_dict = {}
#dictionary format: comparison_dict[prog_vs_prog] = [[prog1_OG1, prog1_OG2, etc.], [prog2_OG2, prog2_OG2]]
#values from comparisons should only be saved to the dictionary if they meet the threshold similarity value
comparison_dict = {}
#define the keys of the dictionary with empty lists as associated values
comparison_dict['Br_vs_OF'] = [[], []] #Broccoli vs OrthoFinder
comparison_dict['Br_vs_PO'] = [[], []] #Broccoli vs ProteinOrtho
comparison_dict['Br_vs_SP'] = [[], []] #Broccoli vs SonicParanoid
comparison_dict['OF_vs_PO'] = [[], []] #OrthoFinder vs ProteinOrtho
comparison_dict['OF_vs_SP'] = [[], []] #OrthoFinder vs SonicParanoid
comparison_dict['PO_vs_SP'] = [[], []] #ProteinOrtho vs SonicParanoid


for key in filt_prot_dict.keys():
	#iterate through the prot_dict dictionary using its keys
	prot_comparison_list = []
	#initialize an empty list that will contain the program matching information for the protein query
	#only calculate comparisons in places where both of the compared programs have results for that protein
	if filt_prot_dict[key][0] != "-" and filt_prot_dict[key][1] != "-":
		#comparing OGs where a protein query ID is found in both Broccoli and OrthoFinder
		Br_OG = filt_prot_dict[key][0]
		#extract the OG that the protein query belongs to within the Broccoli results
		Br_OG_compare = broccoli_dict[Br_OG]
		#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
		OF_OG = filt_prot_dict[key][1]
		#extract the OG that the protein query belongs to within the OrthoFinder results
		OF_OG_compare = orthofinder_dict[OF_OG]
		#use the extracted OG ID to query the OrthoFinder OG dictionary, to get the list of proteins in that OG
		sm=difflib.SequenceMatcher(None,Br_OG_compare,OF_OG_compare)
		#compare the similarity of the two protein lists
		#and compute a numerical ratio of that similarity, which is saved to a variable
		br_vs_of = sm.ratio()
		if br_vs_of >= membership_decimal:
			#check whether the score meets the threshold requirement
			br_vs_of_comp_list = ["Broccoli", Br_OG, "OrthoFinder", OF_OG, br_vs_of]
			#create list of information to append to entry for protein query dictionary
			#and append the information to the data comparison list of lists
			prot_comparison_list.append(br_vs_of_comp_list)
			#append the OG IDs for Broccoli and OrthoFinder to the value list matching the
			#'Br_vs_OF' key in the comparison dictionary
			#first save the Broccoli OG to the first list in the list
			comparison_dict['Br_vs_OF'][0].append(Br_OG)
			#then save the OrthoFinder OG to the second list in the list
			comparison_dict['Br_vs_OF'][1].append(OF_OG)
	if filt_prot_dict[key][0] != "-" and filt_prot_dict[key][2] != "-":
		#comparing OGs where a protein query ID is found in both Broccoli and ProteinOrtho
		Br_OG = filt_prot_dict[key][0]
		#extract the OG that the protein query belongs to within the Broccoli results
		Br_OG_compare = broccoli_dict[Br_OG]
		#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
		PO_OG = filt_prot_dict[key][2]
		#extract the OG that the protein query belongs to within the ProteinOrtho results
		PO_OG_compare = proteinortho_dict[PO_OG]
		#use the extracted OG ID to query the ProteinOrtho OG dictionary, to get the list of proteins in that OG
		sm=difflib.SequenceMatcher(None,Br_OG_compare,PO_OG_compare)
		#compare the similarity of the two protein lists
		#and compute a numerical ratio of that similarity
		br_vs_po = sm.ratio()
		if br_vs_po >= membership_decimal:
			#check whether the score meets the threshold requirement
			br_vs_po_comp_list = ["Broccoli", Br_OG, "ProteinOrtho", PO_OG, br_vs_po]
			#create list of information to append to entry for protein query dictionary
			#and append the information to the data comparison list of lists
			prot_comparison_list.append(br_vs_po_comp_list)
			#append the OG IDs for Broccoli and ProteinOrtho to the value list matching the
			#'Br_vs_PO' key in the comparison dictionary
			#first save the Broccoli OG to the first list in the list
			comparison_dict['Br_vs_PO'][0].append(Br_OG)
			#then save the ProteinOrtho OG to the second list in the list
			comparison_dict['Br_vs_PO'][1].append(PO_OG)
	if filt_prot_dict[key][0] != "-" and filt_prot_dict[key][3] != "-":
		#comparing OGs where a protein query ID is found in both Broccoli and SonicParanoid
		Br_OG = filt_prot_dict[key][0]
		#extract the OG that the protein query belongs to within the Broccoli results
		Br_OG_compare = broccoli_dict[Br_OG]
		#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
		SP_OG = filt_prot_dict[key][3]
		#extract the OG that the protein query belongs to within the SonicParanoid results
		SP_OG_compare = sonicparanoid_dict[SP_OG]
		#use the extracted OG ID to query the SonicParanoid OG dictionary, to get the list of proteins in that OG
		sm=difflib.SequenceMatcher(None,Br_OG_compare,SP_OG_compare)
		#compare the similarity of the two protein lists
		#and compute a numerical ratio of that similarity
		br_vs_sp = sm.ratio()
		if br_vs_sp >= membership_decimal:
			#check whether the score meets the threshold requirement
			br_vs_sp_comp_list = ["Broccoli", Br_OG, "SonicParanoid", SP_OG, br_vs_sp]
			#create list of information to append to entry for protein query dictionary
			#and append the information to the data comparison list of lists
			prot_comparison_list.append(br_vs_sp_comp_list)
			#append the OG IDs for Broccoli and SonicParanoid to the value list matching the
			#'Br_vs_SP' key in the comparison dictionary
			#first save the Broccoli OG to the first list in the list
			comparison_dict['Br_vs_SP'][0].append(Br_OG)
			#then save the SonicParanoid OG to the second list in the list
			comparison_dict['Br_vs_SP'][1].append(SP_OG)
	if filt_prot_dict[key][1] != "-" and filt_prot_dict[key][2] != "-":
		#comparing OGs where a protein query ID is found in both OrthoFinder and ProteinOrtho
		PO_OG = filt_prot_dict[key][2]
		#extract the OG that the protein query belongs to within the ProteinOrtho results
		PO_OG_compare = proteinortho_dict[PO_OG]
		#use the extracted OG ID to query the ProteinOrtho OG dictionary, to get the list of proteins in that OG
		OF_OG = filt_prot_dict[key][1]
		#extract the OG that the protein query belongs to within the OrthoFinder results
		OF_OG_compare = orthofinder_dict[OF_OG]
		#use the extracted OG ID to query the OrthoFinder OG dictionary, to get the list of proteins in that OG
		sm=difflib.SequenceMatcher(None,PO_OG_compare,OF_OG_compare)
		#compare the similarity of the two protein lists
		#and compute a numerical ratio of that similarity
		of_vs_po = sm.ratio()
		if of_vs_po >= membership_decimal:
			#check whether the score meets the threshold requirement
			of_vs_po_comp_list = ["OrthoFinder", OF_OG, "ProteinOrtho", PO_OG, of_vs_po]
			#create list of information to append to entry for protein query dictionary
			#and append the information to the data comparison list of lists
			prot_comparison_list.append(of_vs_po_comp_list)
			#append the OG IDs for OrthoFinder and ProteinOrtho to the value list matching the
			#'OF_vs_PO' key in the comparison dictionary
			#first save the OrthoFinder OG to the first list in the list
			comparison_dict['OF_vs_PO'][0].append(OF_OG)
			#then save the ProteinOrtho OG to the second list in the list
			comparison_dict['OF_vs_PO'][1].append(PO_OG)
	if filt_prot_dict[key][1] != "-" and filt_prot_dict[key][3] != "-":
		#comparing OGs where a protein query ID is found in both OrthoFinder and SonicParanoid
		SP_OG = filt_prot_dict[key][3]
		#extract the OG that the protein query belongs to within the SonicParanoid results
		SP_OG_compare = sonicparanoid_dict[SP_OG]
		#use the extracted OG ID to query the SonicParanoid OG dictionary, to get the list of proteins in that OG
		OF_OG = filt_prot_dict[key][1]
		#extract the OG that the protein query belongs to within the OrthoFinder results
		OF_OG_compare = orthofinder_dict[OF_OG]
		#use the extracted OG ID to query the OrthoFinder OG dictionary, to get the list of proteins in that OG
		sm=difflib.SequenceMatcher(None,SP_OG_compare,OF_OG_compare)
		#compare the similarity of the two protein lists
		#and compute a numerical ratio of that similarity
		of_vs_sp = sm.ratio()
		if of_vs_sp >= membership_decimal:
			#check whether the score meets the threshold requirement
			of_vs_sp_comp_list = ["OrthoFinder", OF_OG, "SonicParanoid", SP_OG, of_vs_sp]
			#create list of information to append to entry for protein query dictionary
			#and append the information to the data comparison list of lists
			prot_comparison_list.append(of_vs_sp_comp_list)
			#append the OG IDs for OrthoFinder and SonicParanoid to the value list matching the
			#'OF_vs_SP' key in the comparison dictionary
			#first save the OrthoFinder OG to the first list in the list
			comparison_dict['OF_vs_SP'][0].append(OF_OG)
			#then save the SonicParanoid OG to the second list in the list
			comparison_dict['OF_vs_SP'][1].append(SP_OG)
	if filt_prot_dict[key][2] != "-" and filt_prot_dict[key][3] != "-":
		#comparing OGs where a protein query ID is found in both ProteinOrtho and SonicParanoid
		PO_OG = filt_prot_dict[key][2]
		#extract the OG that the protein query belongs to within the ProteinOrtho results
		PO_OG_compare = proteinortho_dict[PO_OG]
		#use the extracted OG ID to query the Broccoli OG dictionary, to get the list of proteins in that OG
		SP_OG = filt_prot_dict[key][3]
		#extract the OG that the protein query belongs to within the SonicParanoid results
		SP_OG_compare = sonicparanoid_dict[SP_OG]
		#use the extracted OG ID to query the SonicParanoid OG dictionary, to get the list of proteins in that OG
		sm=difflib.SequenceMatcher(None,PO_OG_compare,SP_OG_compare)
		#compare the similarity of the two protein lists
		#and compute a numerical ratio of that similarity
		po_vs_sp = sm.ratio()
		if po_vs_sp >= membership_decimal:
			#check whether the score meets the threshold requirement
			po_vs_sp_comp_list = ["ProteinOrtho", PO_OG, "SonicParanoid", SP_OG, po_vs_sp]
			#create list of information to append to entry for protein query dictionary
			#and append the information to the data comparison list of lists
			prot_comparison_list.append(po_vs_sp_comp_list)
			#append the OG IDs for Broccoli and OrthoFinder to the value list matching the
			#'PO_vs_SP' key in the comparison dictionary
			#first save the ProteinOrtho OG to the first list in the list
			comparison_dict['PO_vs_SP'][0].append(PO_OG)
			#then save the SonicParanoid OG to the second list in the list
			comparison_dict['PO_vs_SP'][1].append(SP_OG)
	#finally, adding the protein comparison data to the dictionary
	prot_comparison_dict[key] = prot_comparison_list
	#assign the list of lists contianing comparison data as the value to the protein query key
	#in the protein comparison dictionary

'''
Some references for the list comparisons:

Using difflib:
https://docs.python.org/3/library/difflib.html
https://stackoverflow.com/questions/6709693/calculating-the-similarity-of-two-lists

Using cosine similarity:
https://stackoverflow.com/questions/28819272/python-how-to-calculate-the-cosine-similarity-of-two-word-lists
https://stackoverflow.com/questions/14720324/compute-the-similarity-between-two-lists
https://en.wikipedia.org/wiki/Cosine_similarity

Both of these methods yeild similar results, within 0.02 (compared two lists with different similarities).
I chose to use difflib because of its far simpler implementation.

'''

'''
A note on the scoring mechanism used above:

The way the scoring is done, if clusters are particularly similar between two programs, with the same
proteins appearing in the OGs of both, the OG's will in some sense be scored multiple times.

I justify the choice to allow this because giving more weight to programs that are providing more
similar clusters is useful when trying to quantify the quality of the orthologous clusters these
programs produce. A cluster that is identified by multiple programs is more likely to be reliable; and
programs that are identifying similar clusters are likely more adept at identifying OGs within
these particularly divergent, unique organisms.

'''

#write out JSON files containing the two dictionaries produced so far

#the prot_comparison_dict
with open('prot_comparison_dict.json', 'w') as temp_file:
	#open the JSON outfile for writing
	#and write out the contents of the filt_prot_dict dictionary
	json.dump(prot_comparison_dict, temp_file)
#the comparison_dict
with open('comparison_dict.json', 'w') as temp_file:
	#open the JSON outfile for writing
	#and write out the contents of the filt_prot_dict dictionary
	json.dump(comparison_dict, temp_file)


#filter the comparison dicitonary to exlude protein query IDs that have no matching OG information
filt_prot_comparison_dict = {}
#create a new empy dictionary to house the information

for key in prot_comparison_dict.keys():
	#iterate over the large comparison dictionary via its keys
	if prot_comparison_dict[key] != []: 
		#eliminate those proteins that did not meet the threshold
		filt_prot_comparison_dict[key] = prot_comparison_dict[key]

#write this database out to a JSON file
with open('filt_prot_comparison_dict.json', 'w') as temp_file:
	#open the JSON outfile for writing
	#and write out the contents of the filt_prot_dict dictionary
	json.dump(filt_prot_comparison_dict, temp_file)


#Part 4: Creation of comparison dictionaries-based dataframes

'''
with open('comparison_dict.json') as json_in:
	#open the JSON file containing the filt_prot_dict data
	#and load it as a dictionary
	comparison_dict = json.load(json_in)
with open('prot_comparison_dict.json') as json_in:
	#open the JSON file containing the filt_prot_dict data
	#and load it as a dictionary
	prot_comparison_dict = json.load(json_in)
with open('filt_prot_comparison_dict.json') as json_in:
	#open the JSON file containing the filt_prot_dict data
	#and load it as a dictionary
	filt_prot_comparison_dict = json.load(json_in)
'''


#create the basis for the dataframe
query_ids = list(filt_prot_comparison_dict.keys())
#get the protein query IDs into a list
query_OG_df = pd.DataFrame(query_ids, columns = ['Query'])
#and now add empty columns for the OG data
query_OG_df["Broccoli_OG"] = "-"
query_OG_df["OrthoFinder_OG"] = "-"
query_OG_df["ProteinOrtho_OG"] = "-"
query_OG_df["SonicParanoid_OG"] = "-"
#make the query column the index column
query_OG_df = query_OG_df.set_index('Query')


#big query & matching OGs dataframe
for key in filt_prot_comparison_dict.keys():
	#iterate over the large filtered comparison dictionary via its keys
	value_list = filt_prot_comparison_dict[key]
	#save the information associated with the protein query to a list
	value_list = list(chain.from_iterable(value_list))
	#un-nest the nested list in variable value_list
	if "Broccoli" in value_list: 
		#identify queries with Broccoli OGs
		br_idx = value_list.index("Broccoli")
		#identify the index of the string "Broccoli"
		#note that doing it this way will only give the index of the first occurance of this string
		#this isn't an issue because only Broccoli assigns proteins to more than 1 OG
		#and this script is intended to be sed on input data from which duplicates have been filtered out
		br_idx = br_idx + 1
		#add 1 to the index to get the index of the OG ID in the value_list
		prot_br_og = value_list[br_idx]
		#identify the Broccoli OG and copy it to a variable
		#and then add the contents of that variable to the dataframe at the appropriate location
		query_OG_df.at[key, 'Broccoli_OG'] = prot_br_og
	if "OrthoFinder" in value_list: 
		#identify queries with OrthoFinder OGs
		of_idx = value_list.index("OrthoFinder")
		#identify the index of the string "OrthoFinder"
		#note that doing it this way will only give the index of the first occurance of this string
		#this isn't an issue because only Broccoli assigns proteins to more than 1 OG
		of_idx = of_idx + 1
		#add 1 to the index to get the index of the OG ID in the value_list
		prot_of_og = value_list[of_idx]
		#identify the OrthoFinder OG and copy it to a variable
		#and then add the contents of that variable to the dataframe at the appropriate location
		query_OG_df.at[key, 'OrthoFinder_OG'] = prot_of_og
	if "ProteinOrtho" in value_list: 
		#identify queries with ProteinOrtho OGs
		po_idx = value_list.index("ProteinOrtho")
		#identify the index of the string "ProteinOrtho"
		#note that doing it this way will only give the index of the first occurance of this string
		#this isn't an issue because only ProteinOrtho assigns proteins to more than 1 OG
		po_idx = po_idx + 1
		#add 1 to the index to get the index of the OG ID in the value_list
		prot_po_og = value_list[po_idx]
		#identify the ProteinOrtho OG and copy it to a variable
		#and then add the contents of that variable to the dataframe at the appropriate location
		query_OG_df.at[key, 'ProteinOrtho_OG'] = prot_po_og
	if "SonicParanoid" in value_list: 
		#identify queries with SonicParanoid OGs
		sp_idx = value_list.index("SonicParanoid")
		#identify the index of the string "OrthoFinder"
		#note that doing it this way will only give the index of the first occurance of this string
		#this isn't an issue because only Broccoli assigns proteins to more than 1 OG
		sp_idx = sp_idx + 1
		#add 1 to the index to get the index of the OG ID in the value_list
		prot_sp_og = value_list[sp_idx]
		#identify the SonicParanoid OG and copy it to a variable
		#and then add the contents of that variable to the dataframe at the appropriate location
		query_OG_df.at[key, 'SonicParanoid_OG'] = prot_sp_og

#write out the query_OG_df to a tab-separated text file
query_OG_df.to_csv(output_full_query_db, sep='\t', index=True)


#remove the query column by re-indexing
nonQuery_OG_df = query_OG_df.reset_index()
#move the query column out of the index
nonQuery_OG_df.drop('Query', axis=1, inplace=True)
#and drop the column
nonQuery_OG_df.drop_duplicates(keep='first', inplace=True)
#filter dataframe by removing duplicate rows

#write out the query_OG_df to a tab-separated text file
nonQuery_OG_df.to_csv(output_non_query_db, sep='\t', index=False)


#Part 5: Create and write out program-specific OG dataframes

#Broccoli
#create empty dictionary to hold the reversed Broccoli dictionary
pivot_broccoli_dict = {}

for keys, values in broccoli_dict.items():
	#iterate over the Broccoli dictionary via its keys and values
    for i in values:
		#iterate over the elements of the list in each value
		#and save to the dictionary in format pivot_dict[Query] = OG
        pivot_broccoli_dict[i]=keys

#filter the dictionary to only include the proteins that meet the overlap threshold
filt_pivot_broccoli_dict = {}
#create a new dictionary to store the data in

#OrthoFinder
#create empty dictionary to hold the reversed Broccoli dictionary
pivot_orthofinder_dict = {}

for keys, values in orthofinder_dict.items():
	#iterate over the OrthoFinder dictionary via its keys and values
    for i in values:
		#iterate over the elements of the list in each value
		#and save to the dictionary in format pivot_dict[Query] = OG
        pivot_orthofinder_dict[i]=keys

#filter the dictionary to only include the proteins that meet the overlap threshold
filt_pivot_orthofinder_dict = {}
#create a new dictionary to store the data in

#PrtoeinOrtho
#create empty dictionary to hold the reversed Broccoli dictionary
pivot_proteinortho_dict = {}

for keys, values in proteinortho_dict.items():
	#iterate over the ProteinOrtho dictionary via its keys and values
    for i in values:
		#iterate over the elements of the list in each value
		#and save to the dictionary in format pivot_dict[Query] = OG
        pivot_proteinortho_dict[i]=keys

#filter the dictionary to only include the proteins that meet the overlap threshold
filt_pivot_proteinortho_dict = {}
#create a new dictionary to store the data in

#SonicParanoid
#create empty dictionary to hold the reversed Broccoli dictionary
pivot_sonicparanoid_dict = {}

for keys, values in sonicparanoid_dict.items():
	#iterate over the SonicParanoid dictionary via its keys and values
    for i in values:
		#iterate over the elements of the list in each value
		#and save to the dictionary in format pivot_dict[Query] = OG
        pivot_sonicparanoid_dict[i]=keys

#filter the dictionary to only include the proteins that meet the overlap threshold
filt_pivot_sonicparanoid_dict = {}
#create a new dictionary to store the data in


for query in query_ids:
	#iterate over the list of protein query IDs
	if query in pivot_broccoli_dict.keys():
		#find those protein query IDs that are in clustered by Broccoli
		#and save the query protein ID and associated OG to the filtered pivoted Broccoli dictionary
		filt_pivot_broccoli_dict[query] = pivot_broccoli_dict[query]
	if query in pivot_orthofinder_dict.keys():
		#find those protein query IDs that are in clustered by OrthoFinder
		#and save the query protein ID and associated OG to the filtered pivoted OrthoFinder dictionary
		filt_pivot_orthofinder_dict[query] = pivot_orthofinder_dict[query]
	if query in pivot_proteinortho_dict.keys():
		#find those protein query IDs that are in clustered by ProteinOrtho
		#and save the query protein ID and associated OG to the filtered pivoted ProteinOrtho dictionary
		filt_pivot_proteinortho_dict[query] = pivot_proteinortho_dict[query]
	if query in pivot_sonicparanoid_dict.keys():
		#find those protein query IDs that are in clustered by SonicParanoid
		#and save the query protein ID and associated OG to the filtered pivoted SonicParanoid dictionary
		filt_pivot_sonicparanoid_dict[query] = pivot_sonicparanoid_dict[query]


#convert the filtered pivoted dictionaries into dataframes & write out

#Broccoli
filt_broccoli_df = pd.DataFrame.from_dict(filt_pivot_broccoli_dict, orient='index')
#create the dataframe
filt_broccoli_df.reset_index(inplace=True)
#reset the index
#and rename the columns
filt_broccoli_df.rename(columns={'index': 'Query', 0: 'Broccoli_OG'}, inplace=True)

#write out to tab-separated text file
filt_broccoli_df.to_csv(broccoli_db, sep='\t', index=False)

#OrthoFinder
filt_orthofinder_df = pd.DataFrame.from_dict(filt_pivot_orthofinder_dict, orient='index')
#create the dataframe
filt_orthofinder_df.reset_index(inplace=True)
#reset the index
#and rename the columns
filt_orthofinder_df.rename(columns={'index': 'Query', 0: 'OrthoFinder_OG'}, inplace=True)

#write out to tab-separated text file
filt_orthofinder_df.to_csv(orthofinder_db, sep='\t', index=False)

#ProteinOrtho
filt_proteinortho_df = pd.DataFrame.from_dict(filt_pivot_proteinortho_dict, orient='index')
#create the dataframe
filt_proteinortho_df.reset_index(inplace=True)
#reset the index
#and rename the columns
filt_proteinortho_df.rename(columns={'index': 'Query', 0: 'ProteinOrtho_OG'}, inplace=True)

#write out to tab-separated text file
filt_proteinortho_df.to_csv(proteinortho_db, sep='\t', index=False)

#SonicParanoid
filt_sonicparanoid_df = pd.DataFrame.from_dict(filt_pivot_sonicparanoid_dict, orient='index')
#create the dataframe
filt_sonicparanoid_df.reset_index(inplace=True)
#reset the index
#and rename the columns
filt_sonicparanoid_df.rename(columns={'index': 'Query', 0: 'SonicParanoid_OG'}, inplace=True)

#write out to tab-separated text file
filt_sonicparanoid_df.to_csv(sonicparanoid_db, sep='\t', index=False)


#Part 6: Write out summary text file

with open(outfile_summary, "w") as outfile: 
	#open the summary file for writing
	outfile.write("The number of orthologous clusters in each pairwise comparison of two orthologous clustering" + "\n" +
			   "programs that met the desired threshold value of " + str(membership_threshold) + "% are listed below:" + "\n\n")
	for key in comparison_dict.keys():
		#iterate through the comparison dictionary via its keys
		#and write out the number of OGs that met that threshold
		outfile.write("For the " + key + " comparison," + "\n" + 
				"\t" + "the number of OGs that meet the threshold similarity is: " + str(lenset((comparison_dict[key][0]))) + "\n")

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/45_results/ directory
#model: 
python og_overlap_percent.py prot_to_OG_db [membership_threshold]
#using it: 
python ../../../Scripts/og_overlap_percent.py filt_prot_dict.json
#we're actually starting off with a test run, only trying to make the comparison dictionaries
#while that runs I'll work on writing the rest of the script,
#but unfortunately the amount of data parsing necessary for these programs to run makes parallelizing the workflow necessary
###
#changed the model run structure a bit since the number of output files increased
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/nonDuplicate_45_results/ directory
#model: 
python og_overlap_percent.py prot_to_OG_db output_base [membership_threshold]
#using it: 
python ../../../Scripts/og_overlap_percent.py filt_prot_dict.json OG_membership_overlap
###
#actually, realized that the membership scoring script didn't account for the duplicates in Broccoli, 
#so I'm currently rerunning the og_membership_test.py script using the non-Duplicat Broccoli file as input
#in the meantime, though, let's make sure this script does actually run properly
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
python ../../../Scripts/og_overlap_percent.py filt_prot_dict.json OG_membership_overlap
# it worked! 
###

```

One final thing that Courtney asked for...

Creating a parsed OGs file that includes a 3rd column (Pythonic index 2) with the percent of species represented in the OG, and a 4th column with a list of the species present in the OG (script saved to og_db_plusSpeciesRep__v2.py)

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: og_db_plusSpeciesRep__v2.py
Date: 2022-04-04
Author: Virág Varga

Description:
	This program parses the parsed results of orthologous clustering software
		(OrthoFinder, SonicParanoid, ProteinOrtho, Broccoli) and creates an output
		file in the format:
			OG_ID\tSpecies_Percent\tSpecies_Represented\tPhylum_Percent\tPhyla_Represented

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	sys
	pandas
	os

Procedure:
	1. Importing necessary modules, assigning command-line arguments.
	2. Importing data into Pandas dataframes.
	3. Setting up data types (dataframe for ref file, dictionary for infile) that will be used.
	4. Creating dictionaries of species categories and phyla represented in each OG.
	5. Creating new dataframes from the dictionaries.
	6. Merging the species and phylum dataframes and writing out results to a tab-separated 
		text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined - it is based on the input file name
	- The input file used for the program must be an un-pivoted results file of either
		Broccoli, OrthoFinder, SonicParanoid, or ProteinOrtho, following the structure
		used by the parsers used previously in this workflow.
	- The program cannot accept multiple input parsed OG files simultaneously.

Version:
	This is version 2.0 of this program. This version has the added functionality of
		adding a second column with the list of phyla represented in a given OG.

Usage:
	./og_db_plusSpeciesRep__v2.py input_db ref_db
	OR
	python og_db_plusSpeciesRep__v2.py input_db ref_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = sys.argv[1]
#infile = "OF_OGs_parsed.txt"
#designate reference file name as variable
ref_db = sys.argv[2]
#ref_db = "Prots_Species_Phyla_DB.txt"

#define the output file based on the input file name
base = os.path.basename(infile)
out_full = os.path.splitext(base)[0]
output_db = out_full + "__OG-Species-PhylaPercent.txt"


#Part 2: Import data into Pandas dataframes

#import input file into pandas dataframe
ortho_df = pd.read_csv(infile, sep = '\t', header = 0)

#remove 3rd column from Pandas dataframe if necessary
#needed for original parsed results of OrthoFinder, ProteinOrtho & SonicParanoid
col_num = len(ortho_df.columns)
#count number of columns
if col_num == 3:
	#select dataframes with 3 columns
	#remove middle column with species information
	ortho_df.drop(ortho_df.columns[1], axis=1, inplace=True)


#import reference file into Pandas dataframe
ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)


#Part 3: Set up data types (dataframe for ref file, dictionary for infile) that will be used

#identify OG column name (for use later)
og_col = ortho_df.columns[1]

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 4: Create dictionaries of species categories and phyla represented in each OG

#get number of species represented
total_species = ref_df.Species_Category.unique()
#save list of species categories to a variable
total_species_num = len(total_species)
#count the number of elements of the species category list
#for the original Thesis project workflow, this should be 26

#create empty dictionary for species information
species_OG_dict = {}
#create empty dictionary for species percentage information
species_OG_percent_dict = {}


#get number of phyla represented
total_phyla = ref_df.Phylum.unique()
#save list of phyla to a variable
total_phyla_num = len(total_phyla)
#count the number of elements of the phylum list
#for the original Thesis project workflow, this should be 5

#create empty dictionary for phylum information
phylum_OG_dict = {}
#create empty dictionary for phylum percentage information
phylum_OG_percent_dict = {}


#now populate the empty dictionaries above
for key in ortho_dict.keys():
	#iterate over the dictionary via its keys
	species_list = []
	#create empty list that will be populated with species information
	phylum_list = []
	#create empty list that will be populated with phylum information
	og_prot_list = ortho_dict[key]
	#save list of protein query IDs associated with the given OG
	for prot in og_prot_list:
		#iterate over the list of protein query IDs
		#first deal with the species representation
		prot_species = ref_df.loc[ref_df['Query'] == prot, 'Species_Category'].iloc[0]
		#with .loc, find the location where the protein query ID is found in the 'Query' column
		#then extract the contents of that cell, as well as the cell in the same row that is in the 'Species_Category' column
		#use slicing and .iloc to extract the contents of the 'Species_Category' column
		#and save the species category to variable prot_species
		#append the species category to the species_list
		species_list.append(prot_species)
		#and now deal with the phylum representation
		prot_phylum = ref_df.loc[ref_df['Query'] == prot, 'Phylum'].iloc[0]
		#with .loc, find the location where the protein query ID is found in the 'Query' column
		#then extract the contents of that cell, as well as the cell in the same row that is in the 'Phylum' column
		#use slicing and .iloc to extract the contents of the 'Phylum' column
		#and save the species category to variable prot_phylum
		#append the species category to the phylum_list
		phylum_list.append(prot_phylum)
	#first deal with the species category data
	species_set = set(species_list)
	#turn the species_list into a set to eliminate duplicates
	#and save the species_set as the value in the OG to species dictionary
	species_OG_dict[key] = species_set
	#now populate the dictionary of species percentages
	species_percent = round(len(species_set)/total_species_num, 3)
	#get the percent of species represented as a decimal to 3 decimal places
	#and save it to the species percentage dictionary
	species_OG_percent_dict[key] = species_percent
	#and next deal with the phylum data
	phylum_set = set(phylum_list)
	#turn the phylum_list into a set to eliminate duplicates
	#and save the phylum_set as the value in the OG to phlya dictionary
	phylum_OG_dict[key] = phylum_set
	#now populate the dictionary of phylum percentages
	phylum_percent = round(len(phylum_set)/total_phyla_num, 3)
	#get the percent of phyla represented as a decimal to 3 decimal places
	#and save it to the phylum percentage dictionary
	phylum_OG_percent_dict[key] = phylum_percent


#Part 5: Create new dataframes

#create new dataframe with OGs and species percentages
species_percent_df = pd.DataFrame.from_dict(species_OG_percent_dict, orient='index', columns=['Species_Percent'])
#note that this method of conversion puts the dictionary keys in the index

#create new column for list of species represented in the OG
species_percent_df['Species_Represented'] = "-"

#now add in the lists of species represented per OG
for og_key in species_OG_dict.keys():
	#iterate over the dictionary of species included in OGs via its keys
	#and add in the list of species at the appropriate locations
	species_percent_df.at[og_key, 'Species_Represented'] = list(species_OG_dict[og_key])

#turn the lists in the cells into comma-separated strings
species_percent_df['Species_Represented'] = species_percent_df['Species_Represented'].apply(lambda x: ', '.join(map(str, x)))

#pull the OG ID column out of the index
species_percent_df.reset_index(inplace=True)
#and rename the first column of the dataframe according to the program
species_percent_df.rename(columns={'index': og_col}, inplace=True)


#create new dataframe with OGs and phylum percentages
phylum_percent_df = pd.DataFrame.from_dict(phylum_OG_percent_dict, orient='index', columns=['Phylum_Percent'])
#note that this method of conversion puts the dictionary keys in the index

#create new column for list of phyla represented in the OG
phylum_percent_df['Phyla_Represented'] = "-"

#now add in the lists of species represented per OG
for og_key in phylum_OG_dict.keys():
	#iterate over the dictionary of phyla included in OGs via its keys
	#and add in the list of phyla at the appropriate locations
	phylum_percent_df.at[og_key, 'Phyla_Represented'] = list(phylum_OG_dict[og_key])

#turn the lists in the cells into comma-separated strings
phylum_percent_df['Phyla_Represented'] = phylum_percent_df['Phyla_Represented'].apply(lambda x: ', '.join(map(str, x)))

#pull the OG ID column out of the index
phylum_percent_df.reset_index(inplace=True)
#and rename the first column of the dataframe according to the program
phylum_percent_df.rename(columns={'index': og_col}, inplace=True)


#Part 6: Merge dataframes and write out resulting dataframe to file

#merge dataframes on the OG column
total_percent_df = species_percent_df.merge(phylum_percent_df, how='outer')
#since the species_percent_df is given the merge command, the data from it will be on the left,
#while the data from phylum_percent_df will be on the right
#the `how='outer'` argument ensures that all queries are kept, even the ones without overlap
#it isn't particularly important here, though, since the dataframes are the same length

#Writing out the results to a tab-separated text file
total_percent_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#model: 
python og_db_plusSpeciesRep.py input_db ref_db
#applying it: 
python ../../Scripts/og_db_plusSpeciesRep.py Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep.py Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep.py OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep.py PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep.py SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt
###
#using version 2 of the program
#model: 
python og_db_plusSpeciesRep__v2.py input_db ref_db
#applying it: 
python ../../Scripts/og_db_plusSpeciesRep__v2.py Broccoli_OGs_parsed.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep__v2.py Broccoli_OGs_parsed_nonDuplicate.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep__v2.py OF_OGs_parsed.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep__v2.py PO_OGs_parsed.txt Prots_Species_Phyla_DB.txt
python ../../Scripts/og_db_plusSpeciesRep__v2.py SP_OGs_parsed.txt Prots_Species_Phyla_DB.txt


```

Script to use the data tables created above in order to actually filter the database (script saved to filter_OG_speciesDB.py)
 - Needs to take the following as input: species representation %, (list of) specific secies, (list of) specific phyla
 - Need to convert comma-separated strings in columns into lists
 - Add outfile name suffix that user can define (partial naming)
 - Default to writing out only the OGs in a file; add option of writing out relevant section of the input dataframe

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_OG_speciesDB.py
Date: 2022-04-04
Author: Virág Varga

Description:
	This program filters the species and phyla representation per OG database created by
		the og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with
		the same column structure), based on the desired representation criteria of the user.
	The following can be used to filter for representation:
		- Presence or absence of specific species (or list of species) in an OG
		- Presence or absence of specific phylum (or list of phyla) in an OG
		- Percentage of species or phyla represented in an OG

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	os

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Parsing arguments.
		- Importing modules, determining inputs & outputs, importing data
		- Determining filtration category and performing dataframe querying

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The input file used for the program must be a file created by the og_db_plusSpeciesRep__v2.py
		program (or a filtered version of the same, with the same column structure).
	- The program cannot accept multiple input program files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Inputs & Outputs:
	- Inputs:
		+ The mandatory input file is a file created by the og_db_plusSpeciesRep__v2.py program
			(or a filtered version of the same, with the same column structure).
		+ This program accepts input query species/phyla lists in the following formats:
			- Singular species/phylum provided on the command line
			- Comma-separated list of species/phyla provided on the command line (ex. `ID_1,ID_2,ID_3`)
			- File containing list of species/phyla in format: ID1\nID2 etc.
		+ If the threshold option is selected, an integer percentage value must be given
			(ex. 80 for 80%), otherwise the default is 80.
	- Outputs:
		+ The program will output a text file containing the OGs that met the search criteria in
			the format: OG_ID1\nOG_ID2\nOG_ID3 etc
		+ If the -out flag is used, the program will output a filtered version of the dataframe,
			including all rows which match the query.

Usage:
	./filter_OG_speciesDB.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID]
		[-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE
	OR
	python filter_OG_speciesDB.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID]
		[-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#################################   ARGPARSE   #######################################

import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


#The most general description of what this program can do is defined here
parser = argparse.ArgumentParser(description =
								 'This program filters the species and phyla representation per OG database created by \
									the og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with \
									the same column structure), based on the desired representation criteria of the user. \
								The following can be used to filter for representation: \
									- Presence or absence of specific species (or list of species) in an OG \
									- Presence or absence of specific phylum (or list of phyla) in an OG \
									- Percentage of species or phyla represented in an OG')

#create a group of arguments which will be required
requiredArgNames = parser.add_argument_group('required arguments')
#ref: https://stackoverflow.com/questions/24180527/argparse-required-arguments-listed-under-optional-arguments

#adding the arguments that the program can use
requiredArgNames.add_argument(
	'-type', '--filtration_type',
	metavar='FILTRATION_TYPE',
	dest='filt_type',
	choices=['include', 'exclude', 'threshold'],
	#ref: https://stackoverflow.com/questions/15836713/allowing-specific-values-for-an-argparse-argument
	help = 'This argument requires the user to specify the type of filtration that should be done. \
		Use "include" to include species/phylum IDs, "exclude" to exclude species/phylum IDs, \
			"threshold" to test for inclusion percentage >= the threshold value given',
	required=True
	)
	#the '-type' flag is used to tell the program what kind of analysis is going to be done
requiredArgNames.add_argument(
	'-cat', '--filtration_category',
	metavar='FILTRATION_CATEGORY',
	dest='filt_cat',
	choices=['species', 'phylum'],
	help = 'This argument requires the user to specify the type of data the filtration should be performed on. \
		The options are: "species" OR "phylum".',
	required=True
	)
	#the '-prog' flag is used to tell the program which pragram's data is being used as input

parser.add_argument(
	'-single', '--single_id',
	metavar='SINGLE_ID',
	dest='single_ID',
	help = 'This argument takes a single species or phylum ID as input.'
	)
	#the '-single' flag will import the input single species/phylum as a simple string
parser.add_argument(
	'-list', '--id_list',
	metavar='ID_LIST',
	dest='ID_list',
	help = 'This argument takes a comma-separated list of species or phylum IDs as input (ex. ID1,ID2,ID3).'
	)
	#the '-list' flag will import the input list of species/phyla IDs from the command line
parser.add_argument(
	'-file', '--ID_file',
	dest='ID_file',
	metavar='ID_FILE',
	help = "The file containing species or phylum IDs should be in the format: ID\nID\nID etc.",
	type=argparse.FileType('r')
	)
	#the 'file' flag will import the list of species or phyla IDs from the given file
parser.add_argument(
	'-val', '--threshold_value',
	dest='threshold',
	metavar='INTEGER',
	type=int,
	default=80,
	help = "Integer value of minimum percent of species or phyla that should be represented in OGs. (Default = 80)"
	)
	#the `type=int` argument allows argparse to accept the input as an integer
	#the `default=80` gives a default minimum membership filtration value
	#ref: https://stackoverflow.com/questions/44011031/how-to-pass-a-string-as-an-argument-in-python-without-namespace
	#ref: https://stackoverflow.com/questions/14117415/in-python-using-argparse-allow-only-positive-integers
parser.add_argument(
	'-suf', '--suffix',
	metavar='SUFFIX',
	dest='suffix',
	help = 'This argument allows the user to define a suffix to be used for the output file name.'
	)
	#the '-suf' flag allows the user to define a suffix to be used for the output file name.
parser.add_argument(
	'-out', '--filt_db_out',
	action='store_true',
	help = 'This argument will enable the program to print the entire filtered database, not only the list of OG IDs.'
	)
	#the '-out' flag will enable the program to print the entire filtered database, not only the list of OG IDs
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 1.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files
requiredArgNames.add_argument(
	'-i', '--input',
	dest='input_file',
	metavar='INPUT_FILE',
	help = "The input file should be a species and phyla representation per OG database created by \
		the og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with \
		the same column structure).",
	type=argparse.FileType('r'),
	required=True
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#Part 1: Import modules, determine inputs & outputs, import data

#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = args.input_file.name
#import the dataframe into Pandas
ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
#identify OG column name (for use later)
og_col = ortho_df.columns[0]


#define the output file basename based on the input file name
base = os.path.basename(infile)
out_full = os.path.splitext(base)[0]


#import the query list
if args.single_ID:
	#if the option to enter only one query species/phylum on the command line was selected
	search_IDs = args.single_ID
	#save the query species/phylum to a variable
	#and save that variable to a 1-member list
	search_list = [search_IDs]

if args.ID_list:
	#if the option to give a list of species/phyla IDs was given
	search_IDs = args.ID_list
	#save the contents of the comma-separated string to a variable
	#and save the species/phyla IDs to a list based on comma placement in the input string
	search_list = search_IDs.split(",")

if args.ID_file:
	#if an input file was given containing the species/phyla IDs to search
	#save the file to a variable
	search_file = args.ID_file
	#save the contents of the file (should be a column of species/phyla IDs) to a list
	search_list = [line.rstrip('\n') for line in search_file]


#Part 2: Determine filtration category and perform dataframe querying

if args.filt_cat == 'species':
	#if the species category was selected
	#identify the relevant dataframe columns and save to variables
	percent_col = "Species_Percent"
	#species percent column
	rep_col = "Species_Represented"
	#list of species represented
if args.filt_cat == 'phylum':
	#if the phylum category was selected
	#identify the relevant dataframe columns and save to variables
	percent_col = "Phylum_Percent"
	#phylum percent column
	rep_col = "Phyla_Represented"
	#list of phylum represented


if args.filt_type == 'threshold':
	#if the threshold option was selected
	#parse the relevant argument to determine the threshold value
	threshold_value = args.threshold
	#now determine the decimal value of the threshold percent
	threshold_decimal = threshold_value/100

	#filter the relevant dataframe column on the basis of the threshold value
	filt_ortho_df = ortho_df[ortho_df[percent_col] >= threshold_decimal]
	#create a list of the OG IDs that met the threshold
	og_list = filt_ortho_df[og_col].unique()

	#now create the relevant results file(s)
	if len(og_list) == 0: 
		#check if any OGs have hit the threshold
		#and if not, print message to the console
		print("No OGs have met the filtration criteria!")
	else: 
		#if there are OGs to report
		if args.suffix:
			#if the user has provided a suffix
			suffix_out = args.suffix
			#save the suffix to a variable
			#and designate the outfile name
			output_file = out_full + "_" + suffix_out + "_" + str(threshold_value) + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_" + suffix_out + "_filtDB_" + str(threshold_value) + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)
		else:
			#if the user has not given a file suffix, use the default outfile name
			output_file = out_full + "_filt" + str(threshold_value) + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_filtDB_" + str(threshold_value) + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)


if args.filt_type == 'include':
	#if the inclusion option was selected
	#first, convert the relevant data colum elements into a list (vs. comma-separated string)
	ortho_df[rep_col] = ortho_df[rep_col].str.split(', ')
	#now see if the species/phyla are all represented in the list
	#ref: https://stackoverflow.com/questions/60932036/check-if-pandas-column-contains-all-elements-from-a-list
	search_set = set(search_list)
	#first create a set of the list of species/phyla to search for
	#then search the appropriate column for all OGs that include all of the species/phyla in the set
	filt_ortho_df = ortho_df[ortho_df[rep_col].map(search_set.issubset)].copy()
	#create a list of the OG IDs that met the threshold
	og_list = filt_ortho_df[og_col].unique()
	
	#now create the relevant results file(s)
	if len(og_list) == 0: 
		#check if any OGs have hit the threshold
		#and if not, print message to the console
		print("No OGs have met the filtration criteria!")
	else: 
		#if there are OGs to report
		if args.suffix:
			#if the user has provided a suffix
			suffix_out = args.suffix
			#save the suffix to a variable
			#and designate the outfile name
			output_file = out_full + "_incl_" + suffix_out + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				#turn the lists in the cells into comma-separated strings
				filt_ortho_df[rep_col] = filt_ortho_df[rep_col].apply(lambda x: ', '.join(map(str, x)))
				output_db = out_full + "_incl_" + suffix_out + "_filtDB" + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)
		else:
			#if the user has not given a file suffix, use the default outfile name
			output_file = out_full + "_include.txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				#turn the lists in the cells into comma-separated strings
				filt_ortho_df[rep_col] = filt_ortho_df[rep_col].apply(lambda x: ', '.join(map(str, x)))
				output_db = out_full + "_include_filtDB.txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)


if args.filt_type == 'exclude':
	#if the exclusion option was selected
	#then search the appropriate column for all OGs that do not include the species/phyla
	#ref: https://stackoverflow.com/questions/26577516/how-to-test-if-a-string-contains-one-of-the-substrings-in-a-list-in-pandas
	filt_ortho_df = ortho_df[~ortho_df[rep_col].str.contains('|'.join(search_list))].copy()
	#create a list of the OG IDs that met the threshold
	og_list = filt_ortho_df[og_col].unique()
	
	#now create the relevant results file(s)
	if len(og_list) == 0: 
		#check if any OGs have hit the threshold
		#and if not, print message to the console
		print("No OGs have met the filtration criteria!")
	else: 
		#if there are OGs to report
		if args.suffix:
			#if the user has provided a suffix
			suffix_out = args.suffix
			#save the suffix to a variable
			#and designate the outfile name
			output_file = out_full + "_excl_" + suffix_out + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_excl_" + suffix_out + "_filtDB" + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)
		else:
			#if the user has not given a file suffix, use the default outfile name
			output_file = out_full + "_exclude.txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_exclude_filtDB.txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#testing at home
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt | sort | uniq
# Fornicata
# Fornicata, Other
# Fornicata, Other, Preaxostyla
# Fornicata, Preaxostyla
# Other, Preaxostyla
# Parabasalia
# Parabasalia, Fornicata
# Parabasalia, Fornicata, Other, Preaxostyla
# Parabasalia, Fornicata, Preaxostyla
# Parabasalia, Other, Preaxostyla
# Parabasalia, Preaxostyla
# Preaxostyla
# Preaxostyla, Anaeramoebidae, Fornicata, Other, Parabasalia
#model:
python filter_OG_speciesDB.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID] [-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE
#applying it:
python ../filter_OG_speciesDB.py -type include -cat phylum -single Parabasalia -out -i SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt
#checking results: 
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_include_filtDB.txt | sort | uniq
# Parabasalia
# Parabasalia, Fornicata
# Parabasalia, Fornicata, Other, Preaxostyla
# Parabasalia, Fornicata, Preaxostyla
# Parabasalia, Other, Preaxostyla
# Parabasalia, Preaxostyla
# Preaxostyla, Anaeramoebidae, Fornicata, Other, Parabasalia
python ../filter_OG_speciesDB.py -type exclude -cat phylum -single Parabasalia -out -i SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt
#checking results: 
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_exclude_filtDB.txt | sort | uniq
# Fornicata
# Fornicata, Other
# Fornicata, Other, Preaxostyla
# Fornicata, Preaxostyla
# Other, Preaxostyla
# Preaxostyla
python ../filter_OG_speciesDB.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -suf test_suf -out -i SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt
#checking results: 
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_incl_test_suf_filtDB.txt | sort | uniq
# Preaxostyla, Anaeramoebidae, Fornicata, Other, Parabasalia
python ../filter_OG_speciesDB.py -type exclude -cat phylum -file filt_test_file.txt -suf test_file -out -i SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt
#checking results: 
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_excl_test_file_filtDB.txt | sort | uniq
# No OGs have met the filtration criteria!
# Other, Preaxostyla
# Parabasalia
# Parabasalia, Other, Preaxostyla
# Parabasalia, Preaxostyla
# Preaxostyla
#this is correct, I was trying to exclude Fornicata
python ../filter_OG_speciesDB.py -type threshold -cat species -out -i SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt
#checking results: 
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_filtDB_80.txt | sort | uniq
# Preaxostyla, Anaeramoebidae, Fornicata, Other, Parabasalia
python ../filter_OG_speciesDB.py -type threshold -cat species -val 50 -out -i SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200.txt
#checking results: 
awk -F'\t' '{ if (NR!=1) { print $5 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_filtDB_50.txt | sort | uniq
# Preaxostyla, Anaeramoebidae, Fornicata, Other, Parabasalia
awk -F'\t' '{if (NR!=1) { print $3 } }' SP_OGs_parsed__OG-Species-PhylaPercent__HEAD200_filtDB_50.txt | sort | uniq
# Chilomastix_caulleryi, Tetratrichomonas_gallinarum, Monocercomonoides_exilis, SC_anaeromoeba, Giardia_intestinalis_B, Spironucleus_salmonicida, Dientamoeba_fragilis, Ergobibamus_cyprinoides, Kipferlia_bialata, Pentatrichomonas_hominis, Giardia_intestinalis_E, Giardia_muris, Carpediemonas_membranifera, Aduncisulcus_paluster, Dysnectes_brevis, BS_anaeromoeba, Paratrimastix_pyriformis, Trichomonas_vaginalis, Tritrichomonas_foetus, Histomonas_meleagridis, Chilomastix_cuspidata, BM_anaeromoeba, Giardia_intestinalis_A, Barthelona_sp_PAP020, Trimastix_marina, Trepomonas_sp_PC1
# Tetratrichomonas_gallinarum, Monocercomonoides_exilis, SC_anaeromoeba, Giardia_intestinalis_B, Dientamoeba_fragilis, Kipferlia_bialata, Pentatrichomonas_hominis, Giardia_intestinalis_E, Giardia_muris, Carpediemonas_membranifera, Aduncisulcus_paluster, Dysnectes_brevis, BS_anaeromoeba, Paratrimastix_pyriformis, Trichomonas_vaginalis, Tritrichomonas_foetus, Histomonas_meleagridis, Chilomastix_cuspidata, BM_anaeromoeba, Giardia_intestinalis_A, Barthelona_sp_PAP020, Trimastix_marina, Trepomonas_sp_PC1
###
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/OG_Data/ directory
#testing it: 
python filter_OG_speciesDB.py -h
# usage: filter_OG_speciesDB.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID] [-list ID_LIST]
#                               [-file ID_FILE] [-val INTEGER] [-suf SUFFIX] [-out DB_OUT] [-v] -i INPUT_FILE

# This program filters the species and phyla representation per OG database created by the og_db_plusSpeciesRep__v2.py
# program (or a filtered version of the same, with the same column structure), based on the desired representation
# criteria of the user. The following can be used to filter for representation: - Presence or absence of specific
# species (or list of species) in an OG - Presence or absence of specific phylum (or list of phyla) in an OG -
# Percentage of species or phyla represented in an OG

# optional arguments:
#   -h, --help            show this help message and exit
#   -single SINGLE_ID, --single_id SINGLE_ID
#                         This argument takes a single species or phylum ID as input.
#   -list ID_LIST, --id_list ID_LIST
#                         This argument takes a comma-separated list of species or phylum IDs as input (ex.
#                         ID1,ID2,ID3).
#   -file ID_FILE, --ID_file ID_FILE
#                         The file containing species or phylum IDs should be in the format: ID ID ID etc.
#   -val INTEGER, --threshold_value INTEGER
#                         Integer value of minimum percent of species or phyla that should be represented in OGs.
#                         (Default = 80)
#   -suf SUFFIX, --suffix SUFFIX
#                         This argument allows the user to define a suffix to be used for the output file name.
#   -out, --filt_db_out   This argument will enable the program to print the entire filtered database, not only the list
#                         of OG IDs.
#   -v, --version         show program's version number and exit

# required arguments:
#   -type FILTRATION_TYPE, --filtration_type FILTRATION_TYPE
#                         This argument requires the user to specify the type of filtration that should be done. Use
#                         "include" to include species/phylum IDs, "exclude" to exclude species/phylum IDs, "threshold"
#                         to test for inclusion percentage >= the threshold value given
#   -cat FILTRATION_CATEGORY, --filtration_category FILTRATION_CATEGORY
#                         This argument requires the user to specify the type of data the filtration should be performed
#                         on. The options are: "species" OR "phylum".
#   -i INPUT_FILE, --input INPUT_FILE
#                         The input file should be a species and phyla representation per OG database created by the
#                         og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with the same column
#                         structure).
#model: 
python filter_OG_speciesDB.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID] [-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE
#
python filter_OG_speciesDB.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID] [-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE

```

### OG set extraction

Finally, before we can move on to the filtration based on pathways, need to have a way to extract specific portions of the large Metamonad database on an OG-basis. The script should take 3 separate types of input: 
 - Singular OG provided on the command line
 - Comma-separated list of OGs provided on the command line (ex. `OG_1,OG_2,OG_3`)
 - File containing list of OGs in format OG_ID1\nOG_ID2 etc. 

Can obtain the data for that final type of input using awk and bash, like so:

```bash
#from parsed OG results of OrthoFinder, ProteinOrtho, & SonicParanoid: 
awk -F'\t' '{ if (NR!=1) { print $3 } }' FILE | sort | uniq
#need the if statement so that awk skips the header line
#from parsed OG results of Broccoli; and filtered OG results of all OG programs: 
awk -F'\t' '{ if (NR!=1) { print $2 } }' FILE | sort | uniq
#from parsed or whole Metamonad database: 
awk -F'\t' '{ if (NR!=1) { print $34 } }' FILE | sort | uniq #for Broccoli OGs without duplicates
awk -F'\t' '{ if (NR!=1) { print $33 } }' FILE | sort | uniq #for Broccoli OGs with duplicates
awk -F'\t' '{ if (NR!=1) { print $36 } }' FILE | sort | uniq #for OrthoFinder OGs
awk -F'\t' '{ if (NR!=1) { print $35 } }' FILE | sort | uniq #for ProteinOrtho OGs
awk -F'\t' '{ if (NR!=1) { print $37 } }' FILE | sort | uniq #for SonicParanoid OGs

```

The script to extract portion of the Metamonad database based on OGs (saved to extract_OG_prots.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: extract_OG_prots.py
Date: 2022.03.30
Author: Virág Varga

Description:
	This program the species database generated by prot_DB_plus_OGs.py (Metamonada_pred_OG_DB.txt)
		- or a filtered version of the same, with the same column structure - in order to
		extract only those rows that contain protein queries predicted to be part of a
		list of OGs that the user specifies via command-line input.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	os.path

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the input database.
	3. Filtering the dataframe based on list of OG IDs & writing out the results to a
		tab-separated text file.

Known bugs and limitations:
	- There is only very limited quality-checking integrated into the code - only the
		identification of the program whose OG IDs are being used as input is checked.
	- This program requires the input of a flat database created by prot_DB_plus_OGs.py
		(named Metamonada_pred_OG_DB.txt in the original workflow), or a filtered verison
		of the same, with the same column structure maintained.
	- Both the input and output files have user-defined sections:
		The input file is entirely user-defined; while the output file has a user-defined
		extension. If no extension is given, the default output file name will be the
		base of the input file name with an extension in the format:
			"'__' + OG_program + 'OG-selection.txt'"

Usage
	./extract_OG_prots.py input_db OG_program input_OGs [output_extension]
	OR
	python extract_OG_prots.py input_db OG_program input_OGs [output_extension]

	Note that this program accepts OG program options of:
		Br_Grouped_OGs OR Br_Single_OGs OR ProteinOrtho_OG OR OrthoFinder_OG OR SonicParanoid_OG
	Note that this program accepts input OG lists in the following formats:
		- Singular OG provided on the command line
		- Comma-separated list of OGs provided on the command line (ex. `OG_1,OG_2,OG_3`)
			IMPORTANT: For the grouped Broccoli OGs, a semi-colon should be used as the separator.
			This allows lists of OGs to be used as input (ex. `OG_1,OG_2;OG_3` for 
			["OG_1,OG_2", "OG_3"]). 
			Please note that this means the file/list structure should be slightly different from 
			the formatting found in the databases, as no space (" ") characters are used in the 
			command-line input. This should not present an issue for text file input. 
			Please also note that if this option is used, quotes must be used on the command line, 
			like so: 
				`python extract_OG_prots.py Metamonada_pred_OG_DB.txt Br_Grouped_OGs "OG_169,OG_1038;OG_1" Br_group_test`
		- File containing list of OGs in format: OG_ID1\nOG_ID2 etc.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
import os.path #helps return path information for files


#assign command line arguments; load input and output files
#assign the Metamonad database as input_db
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__HEAD.txt"

#assign the identifyer for the OG program to variable OG_program
OG_program = sys.argv[2]
#OG_program = "OrthoFinder_OG"
#identify the specific program whose OG data needs to be collected
#try:
if OG_program == "Br_Grouped_OGs":
	#if the OG program selected is the grouped Broccoli OGs,
	#save the contents of OG_program to variable og_id_col
	og_id_col = OG_program
elif OG_program == "Br_Single_OGs":
	#if the OG program selected is the non-duplicated Broccoli OGs,
	#save the contents of OG_program to variable og_id_col
	og_id_col = OG_program
elif OG_program == "ProteinOrtho_OG":
	#if the OG program selected is the grouped ProteinOrtho OGs,
	#save the contents of OG_program to variable og_id_col
	og_id_col = OG_program
elif OG_program == "OrthoFinder_OG":
	#if the OG program selected is the OrthoFinder OGs,
	#save the contents of OG_program to variable og_id_col
	print("this worked as intended")
	og_id_col = OG_program
elif OG_program == "SonicParanoid_OG":
	#if the OG program selected is the SonicParanoid OGs,
	#save the contents of OG_program to variable og_id_col
	og_id_col = OG_program
else:
	#if none of the above OG program options are selected,
	#print the following error message to the console
	print("You have entered an invalid orthologous clustering program selection. \n \
	   Please use one of the following: \n \
		   Br_Grouped_OGs OR Br_Single_OGs OR ProteinOrtho_OG OR OrthoFinder_OG OR SonicParanoid_OG")
	#and exit the program
	sys.exit(1)

#save the selection of OGs to filter for
input_OGs = sys.argv[3]
#input_OGs = "og_list_test.txt"
#input_OGs = "OG0000000"
#input_OGs = "OG0000000,OG0003024"

if os.path.isfile(input_OGs):
	#if the input selection of OGs is a file
	with open(input_OGs, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of OG IDs) to a list
		og_list = [line.rstrip('\n') for line in infile]
else:
	#if the input OG list is a string instead of a text file
	if OG_program == "Br_Grouped_OGs":
		#if the grouped Broccoli OGs option is given, 
		#save the OG IDs to a list based on semi-colon placement in the input string
		og_list = input_OGs.split(";")
		#then add spaces after comma values in the list
		#this is how to account for the duplicates in the list
		og_list = [br_list.replace(',', ', ') for br_list in og_list]
	else: 
		#for lliterally every single other option
		#save the OG IDs to a list based on comma placement in the input string
		og_list = input_OGs.split(",")

#assign output file name
if len(sys.argv) == 5:
	#if the user provides an extension name
	output_extension = sys.argv[4]
	#output_extension = "filt_OG0000000andOG0003024"
	#create output file name based on input file name and user-defined extension
	output_db = ".".join(input_db.split('.')[:-1]) + '__' + output_extension + '.txt'
else:
	#if the user does not provide an extension name
	#create output file name based on input file name
	output_db = ".".join(input_db.split('.')[:-1]) + '__' + OG_program + 'OG-selection.txt'


#Part 2: Create Pandas dataframe from input data

#read in the input OG database file, assigning the first row as a header row
ortho_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.


#Part 3: Filter dataframe based on list of OG IDs & write out

#filter the dataframe
filt_og_df = ortho_df[ortho_df[og_id_col].isin(og_list)].copy()
#only search the values in the og_id_col for matches
#check to see whether the rows contain the values found in the og_list
#copy the rows that match these conditions to a new dataframe


#write out the filtered dataframe to a tab-separated text file
filt_og_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python extract_OG_prots.py input_db OG_program input_OGs [output_extension]
#applying it: 
python ../Scripts/extract_OG_prots.py Metamonada_pred_OG_DB.txt Br_Grouped_OGs "OG_169,OG_1038;OG_1" Br_group_test
#the above was a test
#below, the actual filtrations: 
python ../Scripts/extract_OG_prots.py Metamonada_pred_OG_DB.txt OG_program input_OGs [output_extension]


```


## Pathway Filtration

First, need to identify the relevant columns, and the data they contain. 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
head -1 Metamonada_pred_OG_DB.txt
# Query   Species_Id      pfamEN_hit      pfamEN_evalue   pfamEN_sum_score        EN_seed_ortholog  EN_evalue        EN_score        EN_eggNOG_OGs   EN_Preferred_name       EN_GOs  EN_KEGG_ko      EN_KEGG_Pathway    EN_KEGG_Reaction        EN_PFAMs        iprS_Signature_accession        iprS_Signature_description iprS_Score      iprS_InterPro_annotations-accession     iprS_InterPro_annotations-description      iprS_GO_annotations     iprS_Pathways_annotations       SigP_Prediction SigP_SP_Probability        TarP_Prediction TarP_Probability        DeepL_Location  DeepL_Probability       MitoF_Prediction   MitoF_Probability_of_presequence        YLoc_Prediction YLoc_Probability        Br_Grouped_OGs     Br_Single_OGs   ProteinOrtho_OG OrthoFinder_OG  SonicParanoid_OG
###
#Courtney stuff
grep "OG0000000" Metamonada_pred_OG_DB.txt | cut -f 2-5 | head
grep "OG0000000" Metamonada_pred_OG_DB.txt | cut -f 2-5 | cut -f 2 | sort | uniq
grep "OG0000000" Metamonada_pred_OG_DB.txt > OG0000000_temp.txt
#this is how we figured out this OG was primarily RAS PFam domains
###
awk -F'\t' '{ print $3 }' Metamonada_pred_OG_DB.txt #EggNOG PFams predictions
#need to include the filed separator argument or we will once again spend half an hour thinking I'd seriously fucked up database concatenation :/
awk -F'\t' '{ print $23 }' Metamonada_pred_OG_DB.txt | sort | uniq #SignalP Prediction
#for unique elements of column do something like the above
#but this is taking,,, a while. so let's at least multitask
ls Metamonada_pred_OG_DB.txt | while read file; do
	awk -F'\t' '{ print $23 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #SignalP Prediction
	awk -F'\t' '{ print $25 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #TargetP Prediction
	awk -F'\t' '{ print $27 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #DeepLoc Prediction
	awk -F'\t' '{ print $29 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #MitoFates Prediction
	awk -F'\t' '{ print $31 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #YLoc Prediction
done &
# [1] 304614
#loop errored out on the last line (YLoc), so running that independently...
head -n -1 Metamonada_filtration_categories.txt > temp.txt ; mv temp.txt foo.txt
#to remove the last line in the file, which contains the "YLoc_Prediction" header
awk -F'\t' '{ print $31 }' Metamonada_pred_OG_DB.txt | sort | uniq >> Metamonada_filtration_categories.txt &
# [1] 338486
#ah, wait, after looking at the file, it looks kind of wonky, I think the `sort` worked in a way I didn't expect
ls Metamonada_pred_OG_DB.txt | while read file; do
	newline="\n"; #save a newline character to variable newline
	awk -F'\t' '{ print $23 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #SignalP Prediction
	echo $newline >> Metamonada_filtration_categories.txt; #add newline between results categories to easily differentiate
	awk -F'\t' '{ print $25 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #TargetP Prediction
	echo $newline >> Metamonada_filtration_categories.txt; #add newline between results categories to easily differentiate
	awk -F'\t' '{ print $27 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #DeepLoc Prediction
	echo $newline >> Metamonada_filtration_categories.txt; #add newline between results categories to easily differentiate
	awk -F'\t' '{ print $29 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #MitoFates Prediction
	echo $newline >> Metamonada_filtration_categories.txt; #add newline between results categories to easily differentiate
	awk -F'\t' '{ print $31 }' $file | sort | uniq >> Metamonada_filtration_categories.txt; #YLoc Prediction
	echo $newline >> Metamonada_filtration_categories.txt; #add newline between results categories to easily differentiate
done &
# [1] 338706
#this worked!

```

Looking at the resulting file (data from `cat Metamonada_filtration_categories.txt`): 
 - SigP_Prediction
   - SP(Sec/SPI)
 - TarP_Prediction
   - mTP
   - SP
 - DeepL_Location
   - Cell_membrane
   - Cytoplasm
   - Endoplasmic_reticulum
   - Extracellular
   - Golgi_apparatus
   - Lysosome/Vacuole
   - Mitochondrion
   - Nucleus
   - Peroxisome
   - Plastid
 - MitoF_Prediction
   - Possessing mitochondrial presequence
 - YLoc_Prediction
   - cytoplasm
   - ER
   - extracellular space
   - Golgi apparatus
   - lysosome
   - mitochondrion
   - nucleus
   - peroxisome
   - plasma membrane



### Secretory Pathway

The following programs have data suitable for score-filtration of the secretory pathway: 
 - SignalP
   - Use categories: SP(Sec/SPI)
 - TargetP
   - Use categories: SP
 - DeepLoc
   - Use categories: Cell_membrane, Extracellular
 - YLoc
   - Use categories: extracellular space, plasma membrane

The following program was written to score the quality of prediction for the secretory pathway (script saved to score_secretoryPathway.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: score_secretoryPathway.py
Date: 2022.03.26
Author: Virág Varga

Description:
	This program scores the predictions generated for the protein queries present
		in the species database generated by prot_DB_plus_OGs.py (Metamonada_pred_OG_DB.txt)
		- or a filtered version of the same, with the same column structure - in order to
		guage the likelihood of a particular protein query belonging to the secretory pathway,
		on the basis of prediction data generated by the SignalP, TargetP, DeepLoc and
		YLoc programs.
		These scores are then used to generate a filtered version of the input dataframe.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	numpy

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the input database, and creating a
		new column in the dataframe for the results of the scoring.
	3. Scoring the predictions generated for each protein query sequence in the file,
		and adding the results to the dataframe.
	4. Filtering the dataframe based on the scores, and writing out results to two
		tab-separated text files: one containing all of the protein queries with their
		associated scores, and one containing only the contents of the filtered dataframe.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a flat database created by prot_DB_plus_OGs.py
		(named Metamonada_pred_OG_DB.txt in the original workflow), or a filtered verison
		of the same, with the same column structure maintained.
	- Only the input file name is user-defined; the naming of the output file is based on
		the input file name.
	- The score threshold should be a value between 0-4. The default value is 4
		(ie. good rating from all 4 prediction programs).

Version:
	This program is based on the score_OG_DB.py script, which was written to score the
		likelihood of a protein belonging to the secretory pathway during the 2021
		exploratory project, which was expanded into the Thesis work of which this
		program is a part.
	This script uses data from a greater number of programs as input, and both the script
		structure and scoring mechanism have been changed accordingly to reflect this.

Usage
	./score_secretoryPathway.py input_db [score_threshold]
	OR
	python score_secretoryPathway.py input_db [score_threshold]

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
import numpy as np #facilitates manipulation of arrays in Python


#determine threshold value for "good" score
if len(sys.argv) == 3:
	#see if the user has provided a score percent value
	#and if they have, save that value to variable score_threshold
	score_threshold = sys.argv[2]
else:
	#if a score threshold isn't provided,
	#use a score of 4 as the default
	score_threshold = 4


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__HEAD.txt"
#output_db and filtered_db names are based on the input_db name
#output_db will contain a copy of the original database, plus scores
output_db = ".".join(input_db.split('.')[:-1]) + '__scoreSecretion.txt'
#filtered_db will contain a filtered version of the scored database
filtered_db = ".".join(input_db.split('.')[:-1]) + '__scoreSecretionGood_' + str(score_threshold) + '.txt'


#Part 2: Create Pandas dataframe from input data

#read in the input OG database file, assigning the first row as a header row
ortho_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.
#set the first column (containing the protein query ids) as an index
ortho_df.set_index('Query', inplace=True)

#add new column to the database, for the scoring
ortho_df['Secretion_Score'] = np.nan
#using 'NaN' here instead of 'None' because 'NaN' is numeric


#Part 3: Perform scoring of predictions for protein secretion

#now iterate through the ortho_df dataframe, and score each protein based on the DeepLoc & SignalP results
for index, row in ortho_df.iterrows():
	#iterate through the dataframe row by row
	score = 0
	#create a counter for the score
	#this will be rewritten to 0 with every iteration of the loop
	dl_pred = row['DeepL_Location']
	#save the DeepLoc prediction to variable dl_pred
	sp_pred = row['SigP_Prediction']
	#save the SignalP prediction to variable sp_pred
	tp_pred = row['TarP_Prediction']
	#save the TargetP prediction to variable tp_pred
	yl_pred = row['YLoc_Prediction']
	#save the YLoc prediction to variable yl_pre
	if dl_pred == 'Extracellular' or dl_pred == 'Cell_membrane':
		#identify proteins DeepLoc predicts to be extracellular or targeted to the cell membrane
		dl_prob = row['DeepL_Probability']
		#save the probability of the prediction to variable dl_prob
		dl_prob = pd.to_numeric(dl_prob)
		#ensure that the value is read as numeric
		if dl_prob >= 0.5:
			#if dl_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score += 1
		else:
			#if dl_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	if tp_pred == 'SP':
		#identify proteins TargetP predicts to be targeted for secretion
		tp_prob = row['TarP_Probability']
		#save the probability that the protein is targeted for secretion tp_prob
		tp_prob = pd.to_numeric(tp_prob)
		#ensure that the value is read as numeric
		if tp_prob >= 0.5:
			#if tp_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score += 1
		else:
			#if tp_cm_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	if sp_pred == 'SP(Sec/SPI)':
		#identify proteins SignalP identifies to be secreted
		sp_prob = row['SigP_SP_Probability']
		#save te probability that the protein will be secreted to variable sp_prob
		sp_prob = pd.to_numeric(sp_prob)
		#ensure that the value is read as numeric
		if sp_prob >= 0.5:
			#if sp_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score +=1
		else:
			#if sp_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	if yl_pred == 'extracellular space' or dl_pred == 'plasma membrane':
		#identify proteins YLoc predicts to be extracellular or targeted to the cell membrane
		yl_prob = row['YLoc_Probability']
		#save the probability of the prediction to variable yl_prob
		yl_prob = pd.to_numeric(yl_prob)
		#ensure that the value is read as numeric
		if yl_prob >= 0.5:
			#if yl_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score += 1
		else:
			#if yl_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	#fill in the calculated score for the protein in the appropriate location in the dataframe
	ortho_df.at[index, 'Secretion_Score'] = score


#Part 4: Filter the dataframe based on the scores, and write out results

#create new filtered datafarme based on the scores
filt_ortho_df = ortho_df[ortho_df['Secretion_Score'] >= float(score_threshold)]
#only scores >= the threshold score will be kept in this new dataframe


if filt_ortho_df.empty:
	#let the user know if no protein queries met the scoring threshold used
	print("No protein queries met the desired score threshold of " + str(score_threshold) + "!")
else:
	#if the dataframe exists
	#write the scored dataframe out to the assigned result file
	filt_ortho_df.to_csv(filtered_db, sep = '\t', index=True)
	#resulsts will be written out to a tab-separated text file

#next write out the scored dataframe to its assigned outfile
ortho_df.to_csv(output_db, sep = '\t', index=True)
#results will be written out to a tab-separated text file

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python score_secretoryPathway.py input_db [score_threshold]
#applying it: 
python ../Scripts/score_secretoryPathway.py Metamonada_pred_OG_DB.txt &
# [1] 547370
###
#A. lanta integration
python ../Scripts/score_secretoryPathway.py Metamonada_Alanta_pred_OG_DB.txt &
# [1] 1839678

```

### Mitochondrial Pathway

The following programs have data suitable for score-filtration: 
 - MitoFates
   - Use categories: Possessing mitochondrial presequence
 - TargetP
   - Use categories: mTP
 - DeepLoc
   - Use categories: Mitochondrion
 - YLoc
   - Use categories: mitochondrion

The following program was written to score the quality of prediction for the mitochondrial pathway (script saved to score_mitochondrialPathway.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: score_mitochondrialPathway.py
Date: 2022.03.26
Author: Virág Varga

Description:
	This program scores the predictions generated for the protein queries present
		in the species database generated by prot_DB_plus_OGs.py (Metamonada_pred_OG_DB.txt)
		- or a filtered version of the same, with the same column structure - in order to
		guage the likelihood of a particular protein query belonging to the mitochondrial pathway,
		on the basis of prediction data generated by the MitoFates, TargetP, DeepLoc and
		YLoc programs.
		These scores are then used to generate a filtered version of the input dataframe.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	numpy

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the input database, and creating a
		new column in the dataframe for the results of the scoring.
	3. Scoring the predictions generated for each protein query sequence in the file,
		and adding the results to the dataframe.
	4. Filtering the dataframe based on the scores, and writing out results to two
		tab-separated text files: one containing all of the protein queries with their
		associated scores, and one containing only the contents of the filtered dataframe.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a flat database created by prot_DB_plus_OGs.py
		(named Metamonada_pred_OG_DB.txt in the original workflow), or a filtered verison
		of the same, with the same column structure maintained.
	- Only the input file name is user-defined; the naming of the output file is based on
		the input file name.
	- The score threshold should be a value between 0-4. The default value is 4
		(ie. good rating from all 4 prediction programs).

Version:
	This program is based on the score_OG_DB.py script, which was written to score the
		likelihood of a protein belonging to the secretory pathway during the 2021
		exploratory project, which was expanded into the Thesis work of which this
		program is a part.
	This script uses data from a greater number of programs as input, and scores the
		probability of a protein being targeted to the mitochondria. Both the script
		structure and scoring mechanism have been changed accordingly to reflect this.

Usage
	./score_mitochondrialPathway.py input_db [score_threshold]
	OR
	python score_mitochondrialPathway.py input_db [score_threshold]

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python
import numpy as np #facilitates manipulation of arrays in Python


#determine threshold value for "good" score
if len(sys.argv) == 3:
	#see if the user has provided a score percent value
	#and if they have, save that value to variable score_threshold
	score_threshold = sys.argv[2]
else:
	#if a score threshold isn't provided,
	#use a score of 4 as the default
	score_threshold = 4


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__HEAD.txt"
#output_db and filtered_db names are based on the input_db name
#output_db will contain a copy of the original database, plus scores
output_db = ".".join(input_db.split('.')[:-1]) + '__scoreMitochondria.txt'
#filtered_db will contain a filtered version of the scored database
filtered_db = ".".join(input_db.split('.')[:-1]) + '__scoreMitochondriaGood_' + str(score_threshold) + '.txt'


#Part 2: Create Pandas dataframe from input data

#read in the input OG database file, assigning the first row as a header row
ortho_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.
#set the first column (containing the protein query ids) as an index
ortho_df.set_index('Query', inplace=True)

#add new column to the database, for the scoring
ortho_df['Mitochondria_Score'] = np.nan
#using 'NaN' here instead of 'None' because 'NaN' is numeric


#Part 3: Perform scoring of predictions for protein secretion

#now iterate through the ortho_df dataframe, and score each protein based on the DeepLoc & SignalP results
for index, row in ortho_df.iterrows():
	#iterate through the dataframe row by row
	score = 0
	#create a counter for the score
	#this will be rewritten to 0 with every iteration of the loop
	dl_pred = row['DeepL_Location']
	#save the DeepLoc prediction to variable dl_pred
	mf_pred = row['MitoF_Prediction']
	#save the MitoFates prediction to variable mf_pred
	tp_pred = row['TarP_Prediction']
	#save the TargetP prediction to variable tp_pred
	yl_pred = row['YLoc_Prediction']
	#save the YLoc prediction to variable yl_pre
	if dl_pred == 'Mitochondrion':
		#identify proteins DeepLoc predicts to be targeted to the mitochondria
		dl_prob = row['DeepL_Probability']
		#save the probability of the prediction to variable dl_prob
		dl_prob = pd.to_numeric(dl_prob)
		#ensure that the value is read as numeric
		if dl_prob >= 0.5:
			#if dl_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score += 1
		else:
			#if dl_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	if tp_pred == 'mTP':
		#identify proteins TargetP predicts to be targeted to the mitochondria
		tp_prob = row['TarP_Probability']
		#save the probability of targeting tp_prob
		tp_prob = pd.to_numeric(tp_prob)
		#ensure that the value is read as numeric
		if tp_prob >= 0.5:
			#if tp_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score += 1
		else:
			#if tp_cm_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	if mf_pred == 'Possessing mitochondrial presequence':
		#identify proteins MitoFates identifies as being targeted to the mitochondria
		mf_prob = row['MitoF_Probability_of_presequence']
		#save the probability of targeting to the mitochondria to variable mf_prob
		mf_prob = pd.to_numeric(mf_prob)
		#ensure that the value is read as numeric
		if mf_prob >= 0.5:
			#if mf_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score +=1
		else:
			#if mf_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	if yl_pred == 'mitochondrion':
		#identify proteins YLoc predicts to be targeted to the mitochondria
		yl_prob = row['YLoc_Probability']
		#save the probability of the prediction to variable yl_prob
		yl_prob = pd.to_numeric(yl_prob)
		#ensure that the value is read as numeric
		if yl_prob >= 0.5:
			#if yl_prob is greater than or equal to 0.5 (>=0.5),
			#add 1 point to the score
			score += 1
		else:
			#if yl_prob is less than 0.5,
			#add 0.5 to the score
			score += 0.5
	#fill in the calculated score for the protein in the appropriate location in the dataframe
	ortho_df.at[index, 'Mitochondria_Score'] = score


#Part 4: Filter the dataframe based on the scores, and write out results

#create new filtered datafarme based on the scores
filt_ortho_df = ortho_df[ortho_df['Mitochondria_Score'] >= float(score_threshold)]
#only scores >= the threshold score will be kept in this new dataframe


if filt_ortho_df.empty:
	#let the user know if no protein queries met the scoring threshold used
	print("No protein queries met the desired score threshold of " + str(score_threshold) + "!")
else:
	#if the dataframe exists
	#write the scored dataframe out to the assigned result file
	filt_ortho_df.to_csv(filtered_db, sep = '\t', index=True)
	#resulsts will be written out to a tab-separated text file

#next write out the scored dataframe to its assigned outfile
ortho_df.to_csv(output_db, sep = '\t', index=True)
#results will be written out to a tab-separated text file

```

Using it: 

```bash
#working in the / directory
#model: 
python score_mitochondrialPathway.py input_db [score_threshold]
#applying it: 
python ../Scripts/score_mitochondrialPathway.py Metamonada_pred_OG_DB__scoreSecretion.txt &
# [1] 550202
python ../Scripts/score_mitochondrialPathway.py Metamonada_pred_OG_DB.txt &
# [1] 557793
###
#A. lanta integration
python ../Scripts/score_mitochondrialPathway.py Metamonada_Alanta_pred_OG_DB__scoreSecretion.txt &
# [1] 1842643
python ../Scripts/score_mitochondrialPathway.py Metamonada_Alanta_pred_OG_DB.txt &
# [2] 1842775

```

### Filtering based on the scores

For simplified filtration of OGs, created a smaller version of the database to use as input for filtering efforts. 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
head -1 Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria.txt
# Query   Species_Id      pfamEN_hit      pfamEN_evalue   pfamEN_sum_score        EN_seed_ortholog        EN_evalue       EN_score       EN_eggNOG_OGs   EN_Preferred_name       EN_GOs  EN_KEGG_ko      EN_KEGG_Pathway EN_KEGG_Reaction        EN_PFAMs      iprS_Signature_accession iprS_Signature_description      iprS_Score      iprS_InterPro_annotations-accession     iprS_InterPro_annotations-description  iprS_GO_annotations     iprS_Pathways_annotations       SigP_Prediction SigP_SP_Probability     TarP_Prediction        TarP_Probability        DeepL_Location  DeepL_Probability       MitoF_Prediction        MitoF_Probability_of_presequence       YLoc_Prediction YLoc_Probability        Br_Grouped_OGs  Br_Single_OGs   ProteinOrtho_OG OrthoFinder_OG  SonicParanoid_OG       Secretion_Score Mitochondria_Score
#grabbing the follwing columns: Query, Species_Id, pfamEN_hit, pfamEN_evalue, OrthoFinder_OG, SonicParanoid_OG, Secretion_Score, Mitochondria_Score
awk -F'\t' '{ print $1"\t"$2"\t"$3"\t"$4"\t"$36"\t"$37"\t"$38"\t"$39  }' Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria.txt > Metamonada_pred_OG_DB__filt_scores-pfam.txt
###
#A. lanta integration
#minor alteration made to include additional useful annotations
#EggNOG OGs, KEGG ko, & IPRScan accession
awk -F'\t' '{ print $1"\t"$2"\t"$3"\t"$4"\t"$9"\t"$12"\t"$19"\t"$36"\t"$37"\t"$38"\t"$39  }' Metamonada_Alanta_pred_OG_DB__scoreSecretion__scoreMitochondria.txt > Metamonada_Alanta_pred_OG_DB__filt_scores-Annot.txt

```

Filtering the large Metamonad database on the results of the scoring (script saved to filter_scored_prots.py): 
 - Parse scores columns to grab proteins of user-specified score
 - User input for which score to look for: Mitochondria or Secretome

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_scored_prots.py
Date: 2022.04.04
Author: Virág Varga

Description:
	This program filters the proteins included in the Metamonad database (or a subset
		of the same) based on the threshold score and scoring category column provided
		by the user.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the input database.
	3. Filtering the dataframe based on the score threshold, and writing out results to a
		tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a flat database created by prot_DB_plus_OGs.py
		(named Metamonada_pred_OG_DB.txt in the original workflow), or a filtered version
		of the same, plus scoring columns generated by score_mitochondrialPathway.py and/or
		score_secretoryPathway.py (or alternative scoring script not in original workflow).
	- Only the input file name is user-defined; the naming of the output file is based on
		the input file name.
	- The score threshold should be a value between 0-4. The default value is 4
		(ie. good rating from all 4 prediction programs in the original workflow).

Usage
	./filter_scored_prots.py input_db score_col score_threshold
	OR
	python filter_scored_prots.py input_db score_col score_threshold

	Where the user should give the column name of the scores column being filtered as
		input for the score_col variable.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__filt_scores-pfam.txt"

#determine type of score filtration
score_col = sys.argv[2]

if score_col == "Secretion_Score":
	#if the user selected the Secretome filtration option,
	#save the category name of the Secretome scores to the score_type variable
	score_type = "Secretome"
elif score_col == "Mitochondria_Score":
	#if the user selected the Mitochondria filtration option,
	#save the category name of the Mitochondria scores to the score_type variable
	score_type = "Mitochondria"
else:
	#if none of the above core filtration options are selected,
	#print the following message to the console in order to gain user input
	score_type = input("You have selected a score column not included in the original workflow. \n Please provide a score type category:")

#determine threshold value for "good" score
if len(sys.argv) == 4:
	#see if the user has provided a score value
	#and if they have, save that value to variable score_threshold
	score_threshold = sys.argv[3]
else:
	#if a score threshold isn't provided,
	#use a score of 4 as the default
	score_threshold = 4

#output_db name is based on the input_db name, filtration type & score
output_db = ".".join(input_db.split('.')[:-1]) + '__filt-' + score_type + str(score_threshold) + '.txt'


#Part 2: Create Pandas dataframe from input data

#read in the input OG database file, assigning the first row as a header row
ortho_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)


#Part 3: Filter the dataframe based on the scores, and write out results

#create new filtered datafarme based on the scores
filt_ortho_df = ortho_df[ortho_df[score_col] >= float(score_threshold)]
#only scores >= the threshold score will be kept in this new dataframe


if filt_ortho_df.empty:
	#let the user know if no protein queries met the scoring threshold used
	print("No protein queries met the desired score threshold of " + str(score_threshold) + "!")
else:
	#if the dataframe exists
	#write the scored dataframe out to the assigned result file
	filt_ortho_df.to_csv(output_db, sep = '\t', index=False)
	#results will be written out to a tab-separated text file

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python filter_scored_prots.py input_db score_col score_threshold
#applying it:
python filter_scored_prots.py Metamonada_pred_OG_DB__filt_scores-pfam.txt Secretion_Score 4
python filter_scored_prots.py Metamonada_pred_OG_DB__filt_scores-pfam.txt Mitochondria_Score 4


```

Filtering OGs for scores (script saved to filter_scored_OGs.py): 
 - Filter for OGs that contain x% scores of n
 - Select score (& OG) category from the command line

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_scored_OGs.py
Date: 2022.04.07
Author: Virág Varga

Description:
	This program filters the proteins included in the Metamonad database (or a subset
		of the same) based on a user-provided threshold score for either the Mitochondrial 
		or Secretory pathway, as well as a user-provided minimum percent of proteins in an 
		OG for a selected program that should meet that score. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Creating Pandas dataframe & OG dictionary from input data.
	3. Creating dictionary of scores in each OG. 
	4. Evaluating OG quality based on scores. 
	5. Filtering the dataframe based on the good quality OGs and writing out results 
		to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a flat database created by prot_DB_plus_OGs.py
		(named Metamonada_pred_OG_DB.txt in the original workflow), or a filtered version
		of the same, plus scoring columns generated by score_mitochondrialPathway.py and/or
		score_secretoryPathway.py (or alternative scoring script not in original workflow).
	- Only the input file name is user-defined; the naming of the output file is based on
		the input file name.
	- The score threshold should be a value between 0-4, and the inclusion percentage 
		should be a percentage value (ex. 80 for 80% matching the minimum threshold 
		score).

Usage
	./filter_scored_OGs.py input_db score_col og_col score_min percent_inclusion
	OR
	python filter_scored_OGs.py input_db score_col og_col score_min percent_inclusion

	Where the user should give the column name of the scores column being filtered as
		input for the score_col variable; the column name of the OG program being 
		filtered on as the input for the score_col variable; a value between 0-4 for the 
		score_min variable; and a numeric percentage value for the percent_inclusion
		variable (ex. 80 for 80% inclusion).

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__filt_scores-pfam.txt"

#determine type of score filtration
score_col = sys.argv[2]
#score_col = "Secretion_Score"

if score_col == "Secretion_Score":
	#if the user selected the Secretome filtration option,
	#save the category name of the Secretome scores to the score_type variable
	score_type = "Secretome"
elif score_col == "Mitochondria_Score":
	#if the user selected the Mitochondria filtration option,
	#save the category name of the Mitochondria scores to the score_type variable
	score_type = "Mitochondria"
else:
	#if none of the above core filtration options are selected,
	#print the following message to the console in order to gain user input
	score_type = input("You have selected a score column not included in the original workflow. \n Please provide a score type category:")

#determine category of OG program
og_col = sys.argv[3]
#og_col = "SonicParanoid_OG"

#determine score threshold to use
score_min = sys.argv[4]
#score_min = 4

#determine the % of proteins matching the score threshold that make an OG "good"
percent_inclusion = sys.argv[5]
#percent_inclusion = 85


#output_db name is based on the input_db name, filtration type & score
output_db = ".".join(input_db.split('.')[:-1]) + '__filt-' + score_type + str(percent_inclusion) + "-" + str(score_min) + '.txt'


#Part 2: Create Pandas dataframe & OG dictionary from input data

#read in the input OG database file, assigning the first row as a header row
ortho_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values
del ortho_dict['-']
#there will likely be proteins that do not have OG assignments, and so are "assigned" to OG '-'
#remove these from teh dicitonary before proceeding


#Part 3: Create dictionary of scores in each OG

#create empty dictionary for score information
score_dict = {}

for key in ortho_dict.keys():
	#iterate over the dictionary via its keys
	score_list = []
	#create empty list that will be populated with score information
	og_prot_list = ortho_dict[key]
	#save list of protein query IDs associated with the given OG
	for prot in og_prot_list:
		#iterate over the list of protein query IDs
		prot_score = ortho_df.loc[ortho_df['Query'] == prot, score_col].iloc[0]
		#with .loc, find the location where the protein query ID is found in the 'Query' column
		#then extract the contents of that cell, as well as the cell in the same row that is in the applicable score column
		#use slicing and .iloc to extract the contents of the applicable score column
		#and save the score to variable prot_score
		#append the score to the score_list
		score_list.append(prot_score)
	#save the list of scores as the value in the OG to scores dictionary
	score_dict[key] = score_list


#Part 4: Evaluate OG quality based on scores

#convert the score thresholds into usable values
#need to float() to ensure the input won't be read as a string
score_min = float(score_min)
#first the minimum score
#and then the threshold score, rounded to 3 decimal places
threshold_decimal = round(float(percent_inclusion)/100, 3)

#create empty list for good OGs
good_OG_list = []


for key in score_dict.keys():
	#iterate over the score dictionary via its keys
	og_score_list = score_dict[key]
	#save the list of scores to a list independent of the dictionary for ease of manipulation
	og_length = len(og_score_list)
	#determine the size of the og based on its length
	good_og_length = og_length*threshold_decimal
	#save the minimum number of good scores needed to mark the OG as good to a variable
	good_count = 0
	#set up a counter for the good scores
	for item in og_score_list:
		#iterate over the list of scores
		if item >= score_min:
			#if the value of the score is >= the minimum score value given
			#increase the counter by one
			good_count +=1
	#now compare the number of good scores to the length of the OG (ie. number of protein members)
	if good_count >= good_og_length:
		#if the number of good proteins is >= the minimum number of good proteins to deem the OG good
		#then save the OG ID to the good OG list
		good_OG_list.append(key)


#Part 5: Filter the dataframe based on the scores, and write out results

#create new filtered datafarme based on the scores
filt_ortho_df = ortho_df[ortho_df[og_col].isin(good_OG_list)]
#only rows containing OGs that have been deemed good will be kept


if filt_ortho_df.empty:
	#let the user know if no protein queries met the scoring threshold used
	print("No protein queries met the desired score thresholds!")
else:
	#if the dataframe exists
	#write the scored dataframe out to the assigned result file
	filt_ortho_df.to_csv(output_db, sep = '\t', index=False)
	#results will be written out to a tab-separated text file

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python filter_scored_OGs.py input_db score_col og_col score_min percent_inclusion
#applying it:
python ../Scripts/filter_scored_OGs.py Metamonada_pred_OG_DB__filt_scores-pfam.txt Secretion_Score SonicParanoid_OG score_min percent_inclusion
python ../Scripts/filter_scored_OGs.py Metamonada_pred_OG_DB__filt_scores-pfam.txt Mitochondria_Score SonicParanoid_OG score_min percent_inclusion
python ../Scripts/filter_scored_OGs.py Metamonada_pred_OG_DB__filt_scores-pfam.txt Secretion_Score OrthoFinder_OG score_min percent_inclusion
python ../Scripts/filter_scored_OGs.py Metamonada_pred_OG_DB__filt_scores-pfam.txt Mitochondria_Score OrthoFinder_OG score_min percent_inclusion


```

#### Comparison to known data

For the sake of having more reliable data, looked into the literature for Trichomonas and Giardia, to find proteins known to be associated with the secretome or the mitochondria/hydrogenosome. The following articles were used: 
 - Trichomonas: 
   - Secretome: 
   - Mitochondria/Hydrogenosome: 
 - Giardia: 
   - Secretome: 
   - Mitochondria/Hydreogenosome: 

Script to query reference data file for encoded/un-encoded protein IDs (script saved to query_prot_ids.py)

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: query_prot_ids.py
Date: 2022-04-11
Author: Virág Varga

Description:
	This program parses the protein ID encoding reference file in order to extract queried
		portions of the dataframe (ie. encoded and unencoded value pairs).

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	sys
	pandas
	os
	datetime.datetime

Procedure:
	1. Importing necessary modules, assigning command-line arguments.
	2. Importing data into Pandas dataframe.
	3. Querying the reference dataframe for the desired data & writing out 
		results to a tab-separated text file. 

Known bugs and limitations:
	- There is only limited quality-checking integrated into the code, relating to 
		command-line inputs.
	- The default name of the output file is based on query time.
	- The input file must be in the format: encoded_prot_ID\toriginal_prot_ID

Usage:
	./query_prot_ids.py input_db query_type query_ids [ouptut_name]
	OR
	python query_prot_ids.py input_db query_type query_ids [output_name]
	
	Where the query_type can be either: "encoded" OR "unencoded"
	Where the list of query_ids can be given in the following formats:
		- Singular protein ID provided on the command line
		- Comma-separated list of protein IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of protein IDs in format: ID1\nID2 etc.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files
from datetime import datetime #access data from system regarding date & time


#designate input file name as variable
input_db = sys.argv[1]
#input_db = "encoding_summary_ref.txt"


#determine the type of query input being used
query_type = sys.argv[2]
#query_type = "unencoded"
if query_type == "encoded":
	#if the input query type is an encoded protein ID
	#designate the query column as the encoded dta column
	query_col = "Encoded"
	print("Query type: encoded")
elif query_type == "unencoded":
	#if the input query type is an unencoded protein ID
	#designate the query column as the unencoded dta column
	query_col = "Unencoded"
	print("Query type: unencoded")
else:
	#if the user does not determine the input query type as encoded or unencoded
	#display this error message
	print("Please select query type: encoded OR unencoded")
	#and exit the program
	sys.exit(1)

#save the list of query IDs to search for to a list
query_ids = sys.argv[3]
#query_ids = "TVAG_343440,TVAG_343470"
if os.path.isfile(query_ids):
	#if the input selection of OGs is a file
	with open(query_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = query_ids.split(",")

#define the output file
if len(sys.argv) == 4:
	#if no output file name is provided by the user
	base = os.path.basename(input_db)
	out_full = os.path.splitext(base)[0]
	#first extract base file name
	#then determine date & time of query
	now = datetime.now()
	time_now = now.strftime("%d-%m-%Y--%H%M%S")
	#and create the resulting outfile name
	output_db = out_full + "__QUERY_" + time_now + ".txt"
elif len(sys.argv) == 5:
	#if the user provides an output file name
	output_db = sys.argv[4]
else:
	#if the wrong number of command line arguments are used
	#display this error message
	print("The command-line input format is incorrect. It should be: \n \
	   python query_prot_ids.py input_db query_type query_ids [output_name]")
	#and exit the program
	sys.exit(1)


#Part 2: Import data into Pandas dataframes

#create list of column names to use for dataframe upon import into Pandas
column_names=["Encoded", "Unencoded"]

#import input file into pandas dataframe
ref_df = pd.read_csv(input_db, sep = '\t', header = None, names = column_names)
#use `header = None, names = column_names`
#because the reference file has no header line
#and the column names need to be manually determined based on the list created


#Part 3: Query the reference dataframe for the desired data & write out

#search the appropriate column for all to extract the rows of the dataframe where the IDs are found
#filt_ref_df = ref_df[ref_df[query_col].isin(query_list)].copy()
filt_ref_df = ref_df[ref_df[query_col].str.contains('|'.join(query_list))].copy()
#need to use the str.contains() method 
#because the names of the proteins are sometimes part of a longer name


filt_ref_df.to_csv(output_db, sep = '\t', index=False)
#results will be written out to a tab-separated text file

```

Script to extract rows of the Metamonad database with specific strings in the sepcified columns (default Query column) (script saved to extract_prot_db.py):

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: extract_prot_db.py
Date: 2022.04.16
Author: Virág Varga

Description:
	This program filters the species database generated by prot_DB_plus_OGs.py
		(Metamonada_pred_OG_DB.txt) - or a filtered version of the same, with the same
		key columns - in order to extract only those rows that contain protein queries
		used as input. Alternately, searching for a match in a difference column 
		can be specified. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas
	os.path

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas to import the contents of the input database.
	3. Filtering the dataframe based on list of protein IDs & writing out the results to a
		tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a flat database created by prot_DB_plus_OGs.py
		(named Metamonada_pred_OG_DB.txt in the original workflow), or a filtered verison
		of the same.
	- Both the input and output files are user-defined.

Usage
	./extract_prot_db.py input_db input_prots output_db [search_col]
	OR
	python extract_prot_db.py input_db input_prots output_db [search_col]

	Where the list of query_ids can be given in the following formats:
		- Singular protein ID provided on the command line
		- Comma-separated list of protein IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of protein IDs in format: ID1\nID2 etc.
	Where the search_col variable specifies the column of the database to be
		searched. Default is "Query".

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files
from datetime import datetime #access data from system regarding date & time


#designate input file name as variable
input_db = sys.argv[1]
#input_db = "encoding_summary_ref.txt"


#determine the type of query input being used
query_type = sys.argv[2]
#query_type = "unencoded"
if query_type == "encoded":
	#if the input query type is an encoded protein ID
	#designate the query column as the encoded dta column
	query_col = "Encoded"
	print("Query type: encoded")
elif query_type == "unencoded":
	#if the input query type is an unencoded protein ID
	#designate the query column as the unencoded dta column
	query_col = "Unencoded"
	print("Query type: unencoded")
else:
	#if the user does not determine the input query type as encoded or unencoded
	#display this error message
	print("Please select query type: encoded OR unencoded")
	#and exit the program
	sys.exit(1)

#save the list of query IDs to search for to a list
query_ids = sys.argv[3]
#query_ids = "TVAG_343440,TVAG_343470"
#query_ids = "Control_data/Reference_prot_ids_Tvag_mito_filt.txt"
#query_ids = "Control_data/Reference_prot_ids_Tvag_sec_filt.txt"

if os.path.isfile(query_ids):
	#if the input selection of OGs is a file
	with open(query_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
		#eliminate duplicates
		query_list = list(set(query_list))
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = query_ids.split(",")
	#eliminate duplicates
	query_list = list(set(query_list))

#define the output file
if len(sys.argv) == 4:
	#if no output file name is provided by the user
	base = os.path.basename(input_db)
	out_full = os.path.splitext(base)[0]
	#first extract base file name
	#then determine date & time of query
	now = datetime.now()
	time_now = now.strftime("%d-%m-%Y--%H%M%S")
	#and create the resulting outfile name
	output_db = out_full + "__QUERY_" + time_now + ".txt"
elif len(sys.argv) == 5:
	#if the user provides an output file name
	output_db = sys.argv[4]
else:
	#if the wrong number of command line arguments are used
	#display this error message
	print("The command-line input format is incorrect. It should be: \n \
	   python query_prot_ids.py input_db query_type query_ids [output_name]")
	#and exit the program
	sys.exit(1)


#Part 2: Import data into Pandas dataframes

#create list of column names to use for dataframe upon import into Pandas
column_names=["Encoded", "Unencoded"]

#import input file into pandas dataframe
ref_df = pd.read_csv(input_db, sep = '\t', header = None, names = column_names)
#use `header = None, names = column_names`
#because the reference file has no header line
#and the column names need to be manually determined based on the list created


#Part 3: Query the reference dataframe for the desired data & write out

#search the appropriate column for all to extract the rows of the dataframe where the IDs are found
#filt_ref_df = ref_df[ref_df[query_col].isin(query_list)].copy()
filt_ref_df = ref_df[ref_df[query_col].str.contains('|'.join(query_list))].copy()
#need to use the str.contains() method 
#because the names of the proteins are sometimes part of a longer name


filt_ref_df.to_csv(output_db, sep = '\t', index=False)
#results will be written out to a tab-separated text file

```

Using it: 

```bash
#checking for duplicated protein IDs in the control file: 
awk -F '\t' '{print $3}' Reference_prot_ids.txt | uniq -d
# this returns nothing, which is correct
#tho it actually doesn't entirely seem to work 
#so I accounted for it in the script
#column 3 contains the protein IDs, and `uniq -d` returns duplicates
###
#model: 
python query_prot_ids.py input_db query_type query_ids [output_name]
#applying it: 
#create input files
awk -F '\t' '(NR>1) {print $3}' Reference_prot_ids_Tvag_sec.txt > Reference_prot_ids_Tvag_sec_filt.txt
awk -F '\t' '(NR>1) {print $3}' Reference_prot_ids_Tvag_mito.txt > Reference_prot_ids_Tvag_mito_filt.txt
#and run it
python ../query_prot_ids.py ../encoding_summary_ref.txt unencoded Reference_prot_ids_Tvag_sec_filt.txt encodingSummary_Tvag_sec_ctrl.txt
# Query type: unencoded
python ../query_prot_ids.py ../encoding_summary_ref.txt unencoded Reference_prot_ids_Tvag_mito_filt.txt encodingSummary_Tvag_mito_ctrl.txt
# Query type: unencoded
#this file is shorter than the control file so... I don't know what to do with that
#extract encoded protein ID list
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Tvag_sec_ctrl.txt > encodingSummary_Tvag_sec_ctrl_encoded.txt
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Tvag_mito_ctrl.txt > encodingSummary_Tvag_mito_ctrl_encoded.txt
#extract these rows from a larger version of the Metamonad database
#model:
python extract_prot_db.py input_db input_prots [search_col]
#applying it: 
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
python ../Scripts/extract_prot_db.py Metamonada_pred_OG_DB__filt_scores-pfam.txt encodingSummary_Tvag_mito_ctrl_encoded.txt Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_mito_ctrl.txt
python ../Scripts/extract_prot_db.py Metamonada_pred_OG_DB__filt_scores-pfam.txt encodingSummary_Tvag_sec_ctrl_encoded.txt Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_ctrl.txt
#get list of OGs associated with these proteins
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_mito_ctrl.txt | sort | uniq > Tvag_mito_ctrl_OF_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_mito_ctrl.txt | sort | uniq > Tvag_mito_ctrl_SP_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_ctrl.txt | sort | uniq > Tvag_sec_ctrl_SP_OGs.txt
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_ctrl.txt | sort| uniq > Tvag_sec_ctrl_OG_OGs.txt
#checking the predictions
#secreted proteins
awk -F '\t' '(NR>1) {print $7}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_ctrl.txt | sort | uniq -c
# 644 0.0
#  13 0.5
#  40 1.0
#   1 1.5
#  16 2.0
#   3 2.5
#   7 3.0
#   2 3.5
#   1 4.0
#which is... NOT GREAT!!1!
#mitochondrial/hydrogenosome proteins
awk -F '\t' '(NR>1) {print $8}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_mito_ctrl.txt | sort | uniq -c
#  32 0.0
#   6 0.5
#   3 1.0
#   4 1.5
#   2 2.0
#   3 3.0
#   1 4.0
#and again,,, not great! 
###
#new control data: de Miguel 2010 T. vaginalis secretome
#ref: https://stackoverflow.com/questions/7209629/extract-string-from-brackets
echo $str | cut -d "[" -f2 | cut -d "]" -f1 Tvag_ctrl_deMiguel2010_secretome.txt
awk -F'[][]' '{print $2}' Tvag_ctrl_deMiguel2010_secretome.txt
sed 's/.*\[\([^]]*\)\].*/\1/g' Tvag_ctrl_deMiguel2010_secretome.txt
# s/          <-- this means it should perform a substitution
# .*          <-- this means match zero or more characters
# \[          <-- this means match a literal [ character
# \(          <-- this starts saving the pattern for later use
# [^]]*       <-- this means match any character that is not a [ character
#                 the outer [ and ] signify that this is a character class
#                 having the ^ character as the first character in the class means "not"
# \)          <-- this closes the saving of the pattern match for later use
# \]          <-- this means match a literal ] character
# .*          <-- this means match zero or more characters
# /\1         <-- this means replace everything matched with the first saved pattern
#                 (the match between "\(" and "\)" )
# /g          <-- this means the substitution is global (all occurrences on the line)
awk -F'[][]' '{print $2}' Tvag_ctrl_deMiguel2010_secretome.txt > Tvag_ctrl_deMiguel2010_secretome_IDs.txt
python ../query_prot_ids.py ../encoding_summary_ref.txt unencoded Tvag_ctrl_deMiguel2010_secretome_IDs.txt encodingSummary_Tvag_sec_Miguel_ctrl.txt
# Query type: unencoded
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Tvag_sec_Miguel_ctrl.txt > encodingSummary_Tvag_sec_Miguel_ctrl_encoded.txt
python ../../Scripts/extract_prot_db.py ../Metamonada_pred_OG_DB__filt_scores-pfam.txt encodingSummary_Tvag_sec_Miguel_ctrl_encoded.txt Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_Miguel_ctrl.txt
awk -F '\t' '(NR>1) {print $7}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_Miguel_ctrl.txt | sort | uniq -c
#  64 0.0
#  19 0.5
#  67 1.0
#   3 1.5
#  22 2.0
#   6 2.5
#  11 3.0
#   1 3.5
#and once again! Not Great!!! tho better than before lol
#number of OrthoFinder OGs
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-pfam__Tvag_sec_Miguel_ctrl.txt | sort | uniq | wc -l
# 168
#will lose some of these that are probably Trich-specific
###
#ran into some issues with the protein ID names, 
#so we're going to have to come back to this -_-

```

Script to match _T. vaginalis_ protein names to encoded protein IDs, taking into account the issue that a number of the names have been changed (script saved to query_prot_ids__v2.py): 

The table of gene aliases was downloaded from TrichDB on 27.04.2022, with the following protocol: 
 1. Open TrichDB database: https://trichdb.org/trichdb/app/
 2. On the sidebar, click on "Organisms", then in the drop-down menu that appears select "Organisms: Genome Info and Stats"
 3. Select "_Trichomonas vaginalis_ G3"
 4. Scroll down the page to select "Download Genome Data from TrichDB"
 5. Click on "txt/" (the following information is also provided: "[last modified: ]2022-04-13 11:15   [Size:] -   [Description] Codon usage or gene alias tables in tab delimited text file.")
 6. Select "TrichDB-57_TvaginalisG3_GeneAliases.txt" (the following information is also provided: "[Last modified:] 2022-04-13 11:15   [Size:] 3.3M   [Description] Gene information table")
 7. The webpage provides no inbuilt way to save the text file displayed. I did the following: CTRL+A → CTRL+C → CRTL+V in a local text file on my computer named "TrichDB-57_TvaginalisG3_GeneAliases.txt"


```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: query_prot_ids__v2.py
Date: 2022-04-27
Author: Virág Varga

Description:
	This program parses the protein ID encoding reference file in order to extract queried
		portions of the dataframe (ie. encoded and unencoded value pairs).
		It can accomadate the name variation of the Trichomonas vaginalis proteins.

List of functions:
	No functions are used in this script.

List of standard and non-standard modules used:
	argparse
	sys
	pandas
	os
	datetime.datetime
	numpy
	itertools.chain

Procedure:
	Begin by parsing arguments. Then:
		1. Importing necessary modules, assigning command-line arguments.
		2. Importing data into Pandas dataframe.
		3. Querying the reference dataframe for the desired data & writing out
			results to a tab-separated text file.

Known bugs and limitations:
	- There is only limited quality-checking integrated into the code, relating to
		command-line inputs.
	- The default name of the output file is based on query time.
	- The reference file must be in the format: encoded_prot_ID\toriginal_prot_ID

Version:
	This is Version 2.0 of this program, which takes into account the fact that a
		number of the protein IDs used for Trichomonas vaginalis have had their official
		names changed. To accomadate this, an input file containing the protein ID
		aliases can be provided as input.

Usage:
	./query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS
	OR
	python query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS

	Where the QUERY_TYPE can be either: "encoded" OR "unencoded"
	Where the list of QUERY_IDS can be given in the following formats:
		- Singular protein ID provided on the command line
		- Comma-separated list of protein IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of protein IDs in format: ID1\nID2 etc.
	Where the TVAG_FILE provided must be the T. vaginalis protein name aliasses file obtained
		from TrichDB.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#################################   ARGPARSE   #######################################
import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


parser = argparse.ArgumentParser(description =
								 'This program parses the protein ID encoding reference file in order to extract queried \n \
									portions of the dataframe (ie. encoded and unencoded value pairs). \n \
									It can accomadate the name variation of the Trichomonas vaginalis proteins.')
#The most general description of what this program can do is defined here


#adding the arguments that the program can use
parser.add_argument(
	dest='ref_db',
	metavar='REF_DB',
	type=argparse.FileType('r'),
	help = 'The reference file should have encoded protein names in the first column \
		and original protein names in the second column.'
	)
	#this portion of code specifies that the program requires a reference file, and it should be opened for reading ('r')
parser.add_argument(
	dest='query_type',
	metavar='QUERY_TYPE',
	choices=['encoded', 'unencoded'],
	help = 'The query type must be specified as either: "encoded" OR "unencoded".'
	)
	#this portion of code specifies the options for the query type: encoded OR unencoded
parser.add_argument(
	dest='query_ids',
	metavar='QUERY_IDS',
	help = 'Here specify the query protein IDs in one of the following formats: \n \
		single protein ID OR comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) \n \
			OR file with protein IDs separated by newlines \n \
			When using unencoded query IDs, a key portion of the protein name is sufficient - \n \
				The entirety of the protein header does not need to be used.'
	)
	#this portion of code specifies that a query protein ID (or list of them) is required,
	#as well as the formats that can be used

parser.add_argument(
	'-tvag', '--Tvag_ref',
	dest='tvag_ref',
	metavar='TVAG_FILE',
	help = "The T. vaginalis protein name aliases file from TrichDB should be used as input.",
	type=argparse.FileType('r')
	)
	#the '-tvag' flag allows for the accomadation of the alternative names used for T. vaginalis proteins
parser.add_argument(
	'-out', '--outname',
	metavar='OUT_NAME',
	dest='out_name',
	help = 'This argument allows the user to define an output file name.'
	)

parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 2.0'
	)
	#This portion of the code specifies the version of the program; currently 2.0
	#The user can call this flag ('-v') without specifying input and output files


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files
from datetime import datetime #access data from system regarding date & time
import numpy as np #allows manipulation of arrays in Python
from itertools import chain #treats consecutive sequences as single sequence


#designate input file name as variable
input_db = args.ref_db.name
#input_db = "encoding_summary_ref.txt"


#determine the type of query input being used
query_type = args.query_type
#query_type = "unencoded"
if query_type == "encoded":
	#if the input query type is an encoded protein ID
	#designate the query column as the encoded dta column
	query_col = "Encoded"
	print("Query type: encoded")
elif query_type == "unencoded":
	#if the input query type is an unencoded protein ID
	#designate the query column as the unencoded dta column
	query_col = "Unencoded"
	print("Query type: unencoded")
else:
	#if the user does not determine the input query type as encoded or unencoded
	#display this error message
	print("Please select query type: encoded OR unencoded")
	#and exit the program
	sys.exit(1)


#save the list of query IDs to search for to a list
query_ids = args.query_ids
#query_ids = "TVAG_343440,TVAG_343470"
#query_ids = "Control_data/Reference_prot_ids_Tvag_mito_filt.txt"
#query_ids = "Control_data/Tvag_ctrl_deMiguel2010_secretome_IDs.txt"

if os.path.isfile(query_ids):
	#if the input selection of OGs is a file
	with open(query_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
		#eliminate duplicates
		query_list = list(set(query_list))
		#also remove spaces in list elements, if they exist
		query_list = [x.strip(' ') for x in query_list]
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = query_ids.split(",")
	#eliminate duplicates
	query_list = list(set(query_list))


#define the output file
if args.out_name:
	#if the user provides an output file name
	#use that file name as the output file name
	output_db = args.out_name
else:
	#if no output file name is provided by the user
	base = os.path.basename(input_db)
	out_full = os.path.splitext(base)[0]
	#first extract base file name
	#then determine date & time of query
	now = datetime.now()
	time_now = now.strftime("%d-%m-%Y--%H%M%S")
	#and create the resulting outfile name
	output_db = out_full + "__QUERY_" + time_now + ".txt"


#Part 2: Import data into Pandas dataframes

#create list of column names to use for dataframe upon import into Pandas
column_names=["Encoded", "Unencoded"]

#import input file into pandas dataframe
ref_df = pd.read_csv(input_db, sep = '\t', header = None, names = column_names)
#use `header = None, names = column_names`
#because the reference file has no header line
#and the column names need to be manually determined based on the list created


#Part 3: Query the reference dataframe for the desired data & write out

if args.tvag_ref:
	#if the user provides the T. vaginalis protein aliases file

	#save the file name to a variable
	tvag_file = args.tvag_ref.name
	#tvag_file = "Control_data/TrichDB-57_TvaginalisG3_GeneAliases.txt"
	#and read the file into a new Pandas dataframe
	#ref: https://stackoverflow.com/questions/27020216/import-csv-with-different-number-of-columns-per-row-using-pandas
	#Loop the data lines
	with open(tvag_file, 'r') as temp_f:
		#open the file for reading
		#and get the numbber of columns in each line
		col_count = [ len(l.split("\t")) for l in temp_f.readlines() ]
	#Generate column names  (names will be 0, 1, 2, ..., maximum columns - 1)
	column_names2 = [i for i in range(0, max(col_count))]
	#Read in the tab-separated text file
	tvag_alias_df = pd.read_csv(tvag_file, sep="\t", header=None, names=column_names2)


	#identify the alternative names that exist for those proteins
	#ref: https://stackoverflow.com/questions/26640129/search-for-string-in-all-pandas-dataframe-columns-and-filter
	mask = np.column_stack([tvag_alias_df[col].str.contains('|'.join(query_list), na=False) for col in tvag_alias_df])
	#can use `np.column_stack` to stack 1-D arrays as columns into a 2-D array
	#To check every column, use `for col in df` to iterate through the column names,
	#and then call `str.contains()` on each column, using `na=False` t prevet the NaN values from causing an error
	#this will yield an arrary where each row has a list of True/False based on whether there is a match
	filt_alias_df = tvag_alias_df.loc[mask.any(axis=1)]
	#use `.loc` to select portions of the database that evaluated as True


	#extract the portions of the reference dataframe containing those protein IDs
	filt_alias_list = filt_alias_df.values.tolist()
	#combine all values in the dataframe into a list
	#this list will contain nested lists (1 per each row), so need to un-nest them
	filt_alias_list = list(chain.from_iterable(filt_alias_list))
	#use chain to un-nest the list
	clean_alias_list = [x for x in filt_alias_list if str(x) != 'nan']
	#remove nan values from the list
	alias_setlist = list(set(clean_alias_list))
	#if there are duplicates, remove them

	#search the appropriate column for all to extract the rows of the dataframe where the IDs are found
	filt_ref_df = ref_df[ref_df[query_col].str.contains('|'.join(alias_setlist))].copy()
	#need to use the str.contains() method
	#because the names of the proteins are sometimes part of a longer name
	
	
	filt_ref_df.to_csv(output_db, sep = '\t', index=False)
	#results will be written out to a tab-separated text file

else:
	#in a case where the T. vaginalis reference file isn't being used

	#search the appropriate column for all to extract the rows of the dataframe where the IDs are found
	filt_ref_df = ref_df[ref_df[query_col].str.contains('|'.join(query_list))].copy()
	#need to use the str.contains() method
	#because the names of the proteins are sometimes part of a longer name
	
	
	filt_ref_df.to_csv(output_db, sep = '\t', index=False)
	#results will be written out to a tab-separated text file

```

Using it: 

```bash
python query_prot_ids__v2.py -h
# usage: query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS
# This program parses the protein ID encoding reference file in order to extract queried portions of the dataframe (ie.
# encoded and unencoded value pairs). It can accomadate the name variation of the Trichomonas vaginalis proteins.
# positional arguments:
#   REF_DB                The reference file should have encoded protein names in the first column and original protein
#                         names in the second column.
#   QUERY_TYPE            The query type must be specified as either: "encoded" OR "unencoded".
#   QUERY_IDS             Here specify the query protein IDs in one of the following formats: single protein ID OR
#                         comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) OR file with protein IDs separated
#                         by newlines When using unencoded query IDs, a key portion of the protein name is sufficient -
#                         The entirety of the protein header does not need to be used.
# optional arguments:
#   -h, --help            show this help message and exit
#   -tvag TVAG_FILE, --Tvag_ref TVAG_FILE
#                         The T. vaginalis protein name aliases file from TrichDB should be used as input.
#   -out OUT_NAME, --outname OUT_NAME
#                         This argument allows the user to define an output file name.
#   -v, --version         show program's version number and exit
###
#preliminary testing
python ../query_prot_ids__v2.py ../encoding_summary_ref.txt unencoded Tvag_ctrl_deMiguel2010_secretome_IDs.txt
# Query type: unencoded
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out out_test.txt ../encoding_summary_ref.txt unencoded Tvag_ctrl_deMiguel2010_secretome_IDs.txt
# Query type: unencoded
#this worked!!! and it got the correct number of proteins - all of them matched! 
#so now let's try using it on the actual data


```

Scripts to add a column to the Metamonad database with information on whether a protein sequence begins with either Methionine or Leucine, which are the two posible start codon-derived amino acids (script to extract information from FASTA files saved to assess_startAA.py; script to add data into the Metamonad database saved to add_startAA.py). 
 - This is done to add some level of quality checking to the protein sequences
 - Assume that proteins not predicted to start with Met or Leu are lower-quality

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: assess_startAA.py
Date: 2022.04.28
Author: Virág Varga

Description:
	This program determines the first amino acid of each protein sequence in a protein
		FASTA file, and produces an output file in the format:
			Query\tStartAA
		Where the options for "StartAA" are: Met OR Leu OR ELSE

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	re
	pandas

Procedure:
	1. Loading required modules & assigning command line argument.
    2. Iterating over the file to collect the data on the first amino acid for each
		FASTA sequence, which is saved to a dictionary.
	3. Converting the dictionary to a Pandas dataframe, and writing out the results
		to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not defined by the user, but based instead on the
		input file name.

Usage
	./assess_startAA.py input_fasta
	OR
	python assess_startAA.py input_fasta

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules & command line argument

#import necessary modules
import sys #allows execution of script from command line
import re #enables regex pattern matching
import pandas as pd #allows manipulation of dataframes


#load input and output files
input_fasta = sys.argv[1]
#input_fasta = "EP00771_Trimastix_marina_edit_50.fasta"
output_db = ".".join(input_fasta.split('.')[:-1]) + '_startAA.txt'


#Part 2: Iterate over the file to collect the data

#create an empty dictionary to hold the association data
startAA_dict = {}

with open(input_fasta, "r") as infile:
	#open the input file for reading
	for line in infile:
		#iterate through the input file line by line
		if line.startswith(">"):
			#identify the header lines
			header = line.strip()
			#save the contents of the line, without the endline character, to a variable
			header = re.sub(">", "", header)
			#remove the ">" character at the front of the string
			#now we have the actual protein ID saved to a variable
			#so skip on to the next line
			sequence = next(infile)
			#and now determine the first amino acid in each sequence
			if sequence.startswith("M"):
				#if Methionine is the first amino acid in the protein sequence
				#add the protein ID and to the dictionary associated with Methionine
				startAA_dict[header] = "Met"
			elif sequence.startswith("L"):
				#if Leucine is the first amino acid in the protein sequence
				#add the protein ID and to the dictionary associated with Leucine
				startAA_dict[header] = "Leu"
			else:
				#if neither Methionine not Leucine is the first amino acid in the protein sequence
				#add the protein ID and to the dictionary without an amino acid association
				startAA_dict[header] = "ELSE"


#Part 3: Convert the dictionary to a Pandas dataframe and write out

#convert the dictionary to a dataframe using the keys as the row indexes
startAA_df = pd.DataFrame.from_dict(startAA_dict, orient='index')
#then pull the query ID information out of the index
startAA_df.reset_index(inplace=True)
#and rename the columns
startAA_df.rename(columns={"index": "Query", 0: "StartAA"}, inplace=True)


#write out results to tab-separated text file
startAA_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/StartAA_Data/ directory
#model: 
python assess_startAA.py input_fasta
#applying it: 
python ../../Scripts/assess_startAA.py ../../DataFiles/EncodedData/BM_newprots_may21.anaeromoeba_edit.fasta
#that worked! tho it deposited the results file in the EncodedData/ directory
#oh well, simple enough to move
#looping it: 
ls /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta | while read file; do
	python ../../Scripts/assess_startAA.py $file;
done
mv ../../DataFiles/EncodedData/*_startAA.txt .
###
#editing the files and compiling into one large dataframe
ls *.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 BM_newprots_may21.anaeromoeba_edit_startAA.txt > StartAA_columns.txt
#the above has to go after the loop because of the naming convention
cat StartAA_columns.txt *_temp.txt > Metamonada_StartAA.txt
#quick quality check
wc -l Metamonada_StartAA.txt
# 541206 Metamonada_StartAA.txt
wc -l Metamonada_pred_OG_DB.txt
# 541206 Metamonada_pred_OG_DB.txt
#ok, moving on! 
###
#A. lanta integration
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/StartAA_Data/ directory
ls /home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/*.fasta | while read file; do
	python ../../Scripts/assess_startAA.py $file;
done
mv ../../DataFiles/EncodedData/*_startAA.txt .
#editing the files and compiling into one large dataframe
ls *.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 BM_newprots_may21.anaeromoeba_edit_startAA.txt > StartAA_columns.txt
#the above has to go after the loop because of the naming convention
cat StartAA_columns.txt *_temp.txt > Metamonada_Alanta_StartAA.txt
#quick quality check
wc -l Metamonada_Alanta_StartAA.txt
# 556048 Metamonada_Alanta_StartAA.txt
wc -l ../Metamonada_Alanta_pred_OG_DB.txt
# 556048 ../Metamonada_Alanta_pred_OG_DB.txt
#ok, it matches, looks good!

```

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: add_startAA.py
Date: 2022-04-28
Author: Virág Varga

Description:
	This program merges the dataframe containing information on the first amino acid
		in each protein sequence into the larger Metamonad database.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Importing modules and assigning command-line arguments.
	2. Importing input data into Pandas dataframes.
	3. Merging the Metamonad and start codon dataframes and writing out the resulting
		dataframe to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but instead based on the input
		file name.

Usage
	./add_startAA.py input_db startAA_db
	OR
	python add_startAA.py input_db startAA_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import modules and assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line argument; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__200prots.txt"
startAA_db = sys.argv[2]
#startAA_db = "EP00771_Trimastix_marina_edit_50_startAA.txt"
output_db = ".".join(input_db.split('.')[:-1]) + '_startAA.txt'


#Part 2: Import data into Pandas dataframes

#import Metamonad database into a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.

#import start codon database into Pandas
startAA_df = pd.read_csv(startAA_db, sep = '\t', header=0)


#Part 3: Merge dataframes and write out

merged_df = input_df.merge(startAA_df, how='outer', on='Query')
#since the input_df is given the merge command, the data from it will be on the left,
#while the data from startAA_df will be on the right
#the `how='outer'` argument ensures that all queries are kept, even the ones without overlap
#it isn't particularly important here, though, since the dataframes are the same length


#write out the resulting dataframe
merged_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python add_startAA.py input_db startAA_db
#applying it: 
python ../Scripts/add_startAA.py Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria.txt StartAA_Data/Metamonada_StartAA.txt
#quick check that this worked correctly 
wc -l 
# 541206 Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt
awk -F'\t' '{ print $40 }' Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt | sort | uniq
# ELSE
# Leu
# Met
# StartAA
#good! these are correct! 
#Methionine is easy to spot-check with `tail`, but wanted to do a quick check on some random ones...
awk '{if ($40 == "Met") print $0;}' Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt | head
#from 1 of the resulting lines...
grep -A 1 "NbRGeYsJo2JR973r" BM_newprots_may21.anaeromoeba_edit.fasta
# >NbRGeYsJo2JR973r
# MEKQTLRQAKLFYNSFIILDSKEEGTKILLTKDEYLSGLSDFERSAKLQTSRPVSSEELMEFQAKQVIEWNEKEKENWENIIEELRTEFEKYEALTIPEQIFLVKTTGKDEGEAAYCRGMNGIFYPQSMDIKKSTLAHELWHIISRNMPTEKRDEIYSIFGFHSFGTPIDYPHQLLEYKISNPDAVNISHYLPIDLNATDSINITPIMYSKSKIFNPSFGQTFFNYLELGFLIVHKNDKNEWVPKLKPNSDENDFNSLELIPPQELPFQFWEKIGTNTNYIHHNEEITAENFMMMIVKDEPRTPKLIENLAKALKKKD*
awk '{if ($40 == "Leu") print $0;}' Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt | head
#from 1 of the resulting lines...
grep -A 1 "EtT98weHx2ifaGVM" Dientamoeba_fragilis.43352.aa_edit.fasta
# >EtT98weHx2ifaGVM
# LLIALFHPSCLKERESFDYWRNLIQLIDILNAYSILAQWISVLNASDLKIILQSLKDFLTAQAIESQRLYLPIMAKIVKAIEIVWFASTRTKKLSFEMFYHDTINKVIDIQVDYQIWASAEDNWCYARNAPWLLNADTKTRFLRANSRQLMNQLQMDAMRTATRYWGVTPVVTPTDCF*
awk '{if ($40 == "ELSE") print $0;}' Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt | head
#from 1 of the resulting lines...
grep -A 1 "NydqbtAYOPZvec3l" Dientamoeba_fragilis.43352.aa_edit.fasta
# >NydqbtAYOPZvec3l
# AFSPASVRLAAGMIGRPDSLRSFLPASTFVPSRRTTSGSFNPVFLTAAITPLAIISHLTIPPKMLTRIALTFGFDEMMRKPSVTAFSEAAPPRSRKLAGSPPESLITSIVPIARPAPLTRHAISPSS*
#yup, we're good to go! 
###
#and now extract a smaller portion of this dataframe containing: 
#query IDs, PFams, OF & SP OGs, scoring columns, start codons
head -1 Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt
# Query   Species_Id      pfamEN_hit      pfamEN_evalue   pfamEN_sum_score        EN_seed_ortholog    EN_evalue       EN_score        EN_eggNOG_OGs   EN_Preferred_name       EN_GOs  EN_KEGG_ko  EN_KEGG_Pathway EN_KEGG_Reaction        EN_PFAMs        iprS_Signature_accession   iprS_Signature_description       iprS_Score      iprS_InterPro_annotations-accession     iprS_InterPro_annotations-description       iprS_GO_annotations     iprS_Pathways_annotations  SigP_Prediction  SigP_SP_Probability     TarP_Prediction TarP_Probability        DeepL_Location      DeepL_Probability       MitoF_Prediction        MitoF_Probability_of_presequence   YLoc_Prediction  YLoc_Probability        Br_Grouped_OGs  Br_Single_OGs   ProteinOrtho_OG OrthoFinder_OG      SonicParanoid_OG        Secretion_Score Mitochondria_Score      StartAA
awk -F'\t' '{ print $1"\t"$2"\t"$3"\t"$4"\t"$36"\t"$37"\t"$38"\t"$39"\t"$40 }' Metamonada_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt > Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt
###
#A. lanta integration
python ../Scripts/add_startAA.py Metamonada_Alanta_pred_OG_DB__scoreSecretion__scoreMitochondria.txt StartAA_Data/Metamonada_Alanta_StartAA.txt
awk -F'\t' '{ print $1"\t"$2"\t"$3"\t"$4"\t"$9"\t"$12"\t"$19"\t"$36"\t"$37"\t"$38"\t"$39"\t"$40 }' Metamonada_Alanta_pred_OG_DB__scoreSecretion__scoreMitochondria_startAA.txt > Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt

```

Script to score OGs based on sequence quality, as determined by the assess_startAA.py program, or protein pathway prediction reliability scores (script saved to assess_OG_startAA_scores.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: assess_OG_startAA_scores.py
Date: 2022-04-28
Author: Virág Varga

Description:
	This program assesses the quality of OGs in the Metamonad database (or filtered 
		version of the same, including at minimum the columns 'Query', 'Secretion_Score', 
		'Mitochondria_Score', 'StartAA' and the OG information column of the OG program 
		desired to be used as input for the search), based on the desired representation 
		criteria of the user.
	The following can be used to filter for representation:
		- Percent of proteins in a given OG which meet a given minimum prediction
			score for either the mitochondrial or secretory pathway
		- Percent of proteins in a given OG which start with Methionine, or with
			either Methionine or Leucine

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	sys
	os
	datetime.datetime
	statistics

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Parsing arguments.
		- Importing modules, determining inputs & outputs, importing data
		- Determining filtration category and performing dataframe querying

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output files are only partially user-defined.
	- The input file used for the program must be the large Metamonad database or filtered version 
		of the same, including at minimum the columns 'Query', 'Secretion_Score', 
		'Mitochondria_Score', 'StartAA' and the OG information column of the OG program 
		desired to be used as input for the search.
	- The program cannot carry out multiple analyses simultaneously with one command.

Inputs & Outputs:
	- Inputs:
		+ The mandatory input file is the large Metamonad database or filtered version 
			of the same, including at minimum the columns 'Query', 'Secretion_Score', 
			'Mitochondria_Score', 'StartAA' and the OG information column of the OG program 
			desired to be used as input for the search.
		+ This program accepts input query OG ID lists in the following formats:
			- Singular OG ID provided on the command line
			- Comma-separated list of OG IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
			- File containing list of OG IDs in format: ID1\nID2 etc.
	- Outputs:
		The program will output two text files containing the following: 
			- [BASENAME]_filt[THRESHOLD]_stats.txt: Summary statistics file \n \
			- [BASENAME]_filt[THRESHOLD]_OGs.txt: File containing OGs meeting the search criteria in format:
				OG_ID1\nOG_ID2\nOG_ID3 etc.

Usage:
	./assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM 
		[-val THRESHOLD] [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE
	OR
	python assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM 
		[-val THRESHOLD] [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#################################   ARGPARSE   #######################################

import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


#The most general description of what this program can do is defined here
parser = argparse.ArgumentParser(description =
								 'This program asseses the quality of OGs in the Metamonad database (or filtered \
									version of the same, including at minimum the columns "Query", "Secretion_Score", \
									"Mitochondria_Score", "StartAA" and the OG information column of the OG program \
									desired to be used as input for the search), based on the desired representation \
									criteria of the user. \n \
								The following can be used to filter for representation: \n \
									- Percent of proteins in a given OG which meet a given minimum prediction \
										score for either the mitochondrial or secretory pathway \n \
									- Percent of proteins in a given OG which start with Methionine, or with \
										either Methionine or Leucine')

#create a group of arguments which will be required
requiredArgNames = parser.add_argument_group('required arguments')
#ref: https://stackoverflow.com/questions/24180527/argparse-required-arguments-listed-under-optional-arguments

#adding the arguments that the program can use
requiredArgNames.add_argument(
	'-cat', '--filtration_category',
	metavar='FILTRATION_CATEGORY',
	dest='filt_cat',
	choices=['Secretion_Score', 'Mitochondria_Score', 'StartAA'],
	#ref: https://stackoverflow.com/questions/15836713/allowing-specific-values-for-an-argparse-argument
	help = 'This argument requires the user to specify the type of data the filtration should be performed on. \
		The options are: "Secretion_Score" OR "Mitochondria_Score" OR "StartAA".',
	required=True
	)
	#the '-cat' flag is used to tell the program which column will be used for scoring
requiredArgNames.add_argument(
	'-query', '--query_ids',
	dest='query_ids',
	metavar='QUERY_IDS',
	help = 'Here specify the query protein IDs in one of the following formats: \n \
		single protein ID OR comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) \n \
			OR file with protein IDs separated by newlines \n \
			When using unencoded query IDs, a key portion of the protein name is sufficient - \n \
				The entirety of the protein header does not need to be used.',
	required=True
	)
	#the '-query' flag specifies that a query protein ID (or list of them) is required,
	#as well as the formats that can be used
requiredArgNames.add_argument(
	'-prog', '--og_program',
	metavar='OG_PROGRAM',
	dest='og_program',
	choices=['Br_Grouped_OGs', 'Br_Single_OGs', 'ProteinOrtho_OG', 'OrthoFinder_OG', 'SonicParanoid_OG'],
	#ref: https://stackoverflow.com/questions/15836713/allowing-specific-values-for-an-argparse-argument
	help = 'This argument requires the user to specify the OG program whose data the filtration should be performed on. \n \
		The options are: "Br_Grouped_OGs" OR "Br_Single_OGs" OR "ProteinOrtho_OG" OR "OrthoFinder_OG" OR "SonicParanoid_OG".',
	required=True
	)
	#the '-cat' flag is used to tell the program which column will be used for scoring

parser.add_argument(
	'-val', '--threshold_value',
	dest='threshold',
	metavar='THRESHOLD',
	type=int,
	default=80,
	help = "Integer value of minimum percent of proteins to meet the given test criteria in OGs. \n \
		Test criteria are: \n \
		- Percent proteins in OG predicted to score at or above given score threshold for given pathway \n \
		- Percent proteins in OG achieving given completion level based on first amino acid in sequence \n \
		(Default = 80)"
	)
	#the `type=int` argument allows argparse to accept the input as an integer
	#the `default=80` gives a default minimum membership filtration value
	#ref: https://stackoverflow.com/questions/44011031/how-to-pass-a-string-as-an-argument-in-python-without-namespace
	#ref: https://stackoverflow.com/questions/14117415/in-python-using-argparse-allow-only-positive-integers
parser.add_argument(
	'-score', '--filt_score',
	metavar='SCORE',
	dest='filt_score',
	type=float,
	default=4.0,
	help = 'This argument allows the user to define he scoring threshold that should be used \n \
		for the given pathway scoring filtration. (Default = 4.0) \n \
		This argument is automatically called for mitochondrial or sectretory pathway testing.'
	)
	#the '-score' flag will enable the user to provide the desired score threshold to be tested
parser.add_argument(
	'-incl', '--includedAA',
	metavar='INCLUDED_AA',
	dest='included_aa',
	choices=['Met', 'Leu'],
	default='Met',
	help = 'This argument allows the user to define the sequence quality checking to be used. \n \
		The options are: "Met" OR "Leu" \n \
		Where "Met" will filter for percent of proteins that start with Methionine; \n \
		while "Leu" will filter for percent of proteins that start with Methionine or Leucine. \n \
		This argument is automatically called for completion quality testing. The default is "Met".'
	)
	#the '-incl' flag will enable the user to provide the desired sequence completion quality
	#based on the starting amino acid in the protein sequence

parser.add_argument(
	'-out', '--outname',
	metavar='OUT_NAME',
	dest='out_name',
	help = 'This argument allows the user to define an output file basename. \n \
		The default basename is "Assessed_[FILTRATION_CATEGORY]_[SCORE/INCLUDED_AA]_[datetime]", \n \
			and the output files are: \n \
				- [BASENAME]_filt[THRESHOLD]_stats.txt: Summary statistics file \n \
				- [BASENAME]_filt[THRESHOLD]_OGs.txt: File containing OGs meeting threshold in format OG1\nOG2 etc.'
	)
	#the '-out' flag allows the user to define a the output file basename
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 1.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files

requiredArgNames.add_argument(
	'-i', '--input',
	dest='input_file',
	metavar='INPUT_FILE',
	help = "The input file should be the Metamonad database (or filtered version of the same, \
		including at minimum the columns 'Query', 'Secretion_Score', 'Mitochondria_Score', \
			'StartAA' and the OG information column of the OG program desired to be used as input for the search).",
	type=argparse.FileType('r'),
	required=True
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#Part 1: Import modules, determine inputs & outputs, import data

#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import sys #allows access to command-line
import os #allow access to computer files
from datetime import datetime #access data from system regarding date & time
import statistics #enables calculation of statistics in Python


#designate input file name as variable
infile = args.input_file.name
#import the dataframe into Pandas
ortho_df = pd.read_csv(infile, sep = '\t', header = 0, low_memory=False)

#identify key columns of dataframe
filt_col = args.filt_cat
#filt_col will contain the name of the column to be searched
og_col = args.og_program
#og_col will contain the name of the OG program whose data is to be searched


#identify filtration thresholds & criteria
filt_threshold = args.threshold
#determine the filtration threshold for percent inclusion of any given test criteria

if filt_col == 'Secretion_Score':
	#if the user is filtering based on prediction probability of secretion
	#save the value of the score threshold to a variable
	score_threshold = args.filt_score
elif filt_col == 'Mitochondria_Score':
	#if the user is filtering based on the prediction probability of secretion
	#save the value of the score threshold to a variable
	score_threshold = args.filt_score
elif filt_col == 'StartAA':
	#if the user is filtering based on the identity of the first amino acid in each protein
	#save the level of completion to search for to a variable
	complete_aa = args.included_aa
else:
	#if the user does not determine the input query type as encoded or unencoded
	#display this error message
	print("Please select query type: 'Secretion_Score' OR 'Mitochondria_Score' OR 'StartAA'")
	#and exit the program
	sys.exit(1)


#define the output files
if args.out_name:
	#if the user provides an output file name
	#use that file name as the output file basename
	out_base = args.out_name
else:
	#if no output file name is provided by the user
	if 'score_threshold' in globals():
		#check to see if the score_threshold variable exists
		#if it does, save the contents to a new variable
		name_threshold = str(score_threshold)
	else:
		#if score_theshold doesn't exist, then by definition variable complete_aa does
		#so do the same as before with this variable
		name_threshold = complete_aa
	out_start = "Assessed_" + filt_col + str(filt_threshold) + "_" + name_threshold + "_"
	#first extract base file name
	#then determine date & time of query
	now = datetime.now()
	time_now = now.strftime("%d-%m-%Y--%H%M%S")
	#and create the resulting outfile name
	out_base = out_start + time_now
#define specific output files
output_stats = out_base + "_stats.txt"
output_ogs = out_base + "_OGs.txt"


#identify the list of query OG IDs
og_ids = args.query_ids

#import the query list
if os.path.isfile(og_ids):
	#if the input selection of OGs is a file
	with open(og_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
		#eliminate duplicates
		query_list = list(set(query_list))
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = og_ids.split(",")
	#eliminate duplicates
	query_list = list(set(query_list))


#Part 2: Create dictionary to hold information on proteins per OG

#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_ortho_df = ortho_df.groupby(og_col)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
ortho_dict = grouped_ortho_df.set_index(og_col).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values
del ortho_dict['-']
#there will likely be proteins that do not have OG assignments, and so are "assigned" to OG '-'
#remove these from the dicitonary before proceeding


#Part 3: Perform dataframe querying & write out results

if filt_col == 'Secretion_Score' or filt_col == 'Mitochondria_Score': 
	#if the user is filtering based on prediction probability of a pathway
	
	#create empty dictionary for score information
	score_dict = {}
	
	for key in ortho_dict.keys():
		#iterate over the dictionary via its keys
		if key in query_list:
			#identify those keys (OG IDs) that are in the query list, 
			#and use only those when compiling the score dictionary
			score_list = []
			#create empty list that will be populated with score information
			og_prot_list = ortho_dict[key]
			#save list of protein query IDs associated with the given OG
			for prot in og_prot_list:
				#iterate over the list of protein query IDs
				prot_score = ortho_df.loc[ortho_df['Query'] == prot, filt_col].iloc[0]
				#with .loc, find the location where the protein query ID is found in the 'Query' column
				#then extract the contents of that cell, as well as the cell in the same row that is in the applicable score column
				#use slicing and .iloc to extract the contents of the applicable score column
				#and save the score to variable prot_score
				#append the score to the score_list
				score_list.append(prot_score)
			#save the list of scores as the value in the OG to scores dictionary
			score_dict[key] = score_list
	
	#create an empty dictionary for the scoring statistics per OG
	score_stats_dict = {}
	#the dictionary should be populated like so: 
	#score_stats_dict[OG_ID] = [number_of_prots, percent_prots_meeting_score_threshold, avg_prot_score, median_prot_score, mode_prot_score]
	
	#create an empty list to contain the good quality OGs
	good_OG_list = []
	
	for score_key in score_dict.keys(): 
		#iterate over the dictionary of scores via its keys (OG IDs)
		og_prot_scores = score_dict[score_key]
		#save list of scores for the OG to a local list for ease of manipulation
		og_length = len(og_prot_scores)
		#save the number of proteins in the OG to a variable
		numb_good_prots = sum(i >= score_threshold for i in og_prot_scores)
		#save number of proteins that meet the score threshold to a variable
		percent_good_prots = numb_good_prots/og_length*100
		#save the percent of proteins in the OG meeting the score threshold to a variable
		if percent_good_prots >= filt_threshold:
			#identify OGs with percent of proteins that meet the score threshold, 
			#that are >= the filtration threshold value for inclusion
			#and add those OGs to the list of good OGs
			good_OG_list.append(score_key)
		og_score_avg = statistics.mean(og_prot_scores)
		#save the average protein prediction score to a variable
		og_score_median = statistics.median(og_prot_scores)
		#save the median protein prediction score to a variable
		og_score_mode = statistics.mode(og_prot_scores)
		#save the most common value (mode) to a variable
		
		#save the calculated statistics to a list
		score_stats_list = [og_length, percent_good_prots, og_score_avg, og_score_median, og_score_mode]
		#and save the calculated statistics to the statistics dictionary
		score_stats_dict[score_key] = score_stats_list
		
	#convert the dictionary to a Pandas dataframe using the keys as the row indexes
	score_stats_df = pd.DataFrame.from_dict(score_stats_dict, orient='index')
	#then pull the query ID information out of the index
	score_stats_df.reset_index(inplace=True)
	#and rename the columns
	score_stats_df.rename(columns={"index": og_col, 0: "OG_Length", 1: "Percent_Good_Prots", 
							2:"OG_Avg_Score", 3: "OG_Median_Score", 4: "OG_Mode_Score"}, inplace=True)
	
	#write out results
	#first, the summary statistics file as a tab-separated dataframe
	score_stats_df.to_csv(output_stats, sep='\t', index=False)
	#and then the list of good OGs
	with open(output_ogs, "w") as outfile: 
		#open the file for writing
		#first write a header line
		outfile.write(og_col + "\n")
		for element in good_OG_list:
			#iterate over the list of good OGs 
			#and write the OG IDs out to the file separated by newline characters
			outfile.write(element + "\n")


if filt_col == 'StartAA':
	#if the user is filtering based on the identity of the first amino acid in each protein
	
	#create empty dictionary for starting amino acid information
	aa_dict = {}
	
	for key in ortho_dict.keys():
		#iterate over the dictionary via its keys
		if key in query_list:
			#identify those keys (OG IDs) that are in the query list, 
			#and use only those when compiling the score dictionary
			aa_list = []
			#create empty list that will be populated with the starting amino acids of the proteins in the OG
			og_prot_list = ortho_dict[key]
			#save list of protein query IDs associated with the given OG
			for prot in og_prot_list:
				#iterate over the list of protein query IDs
				prot_startAA = ortho_df.loc[ortho_df['Query'] == prot, filt_col].iloc[0]
				#with .loc, find the location where the protein query ID is found in the 'Query' column
				#then extract the contents of that cell, as well as the cell in the same row that is in the applicable score column
				#use slicing and .iloc to extract the contents of the applicable score column
				#and save the score to variable prot_score
				#append the score to the score_list
				aa_list.append(prot_startAA)
			#save the list of amino acids as the value in the OG to scores dictionary
			aa_dict[key] = aa_list
	
	#create an empty dictionary for the starting amino acids statistics per OG
	aa_stats_dict = {}
	#the dictionary should be populated like so: 
	#score_stats_dict[OG_ID] = [number_of_prots, percent_prots_meeting_score_threshold, avg_prot_score, median_prot_score, mode_prot_score]
	
	#create an empty list to contain the good quality OGs
	good_OG_list = []
	
	for aa_key in aa_dict.keys(): 
		#iterate over the dictionary of starting amino acids via its keys (OG IDs)
		og_aa_list = aa_dict[aa_key]
		#save list of starting amino acids for the OG to a local list for ease of manipulation
		og_length = len(og_aa_list)
		#save the number of proteins in the OG to a variable
		if complete_aa == "Met":
			#if the user specified that only Methionine presence should be used to assess completion
			numb_good_prots = og_aa_list.count("Met")
			#save number of proteins that start with Methionine to a variable
		if complete_aa == "Leu":
			#if the user specified that only Methionine presence should be used to assess completion
			numb_bad_prots = og_aa_list.count("ELSE")
			#save number of proteins that start with something other than Methionine or Leucine to a variable
			numb_good_prots = og_length - numb_bad_prots
			#deterine the number of proteins that start with Methoinine or Leucine
		percent_good_prots = numb_good_prots/og_length*100
		#save the percent of proteins in the OG starting with Methionine to a variable
		if percent_good_prots >= filt_threshold:
			#identify OGs with percent of proteins that start with Methionine, 
			#that are >= the filtration threshold value for inclusion
			#and add those OGs to the list of good OGs
			good_OG_list.append(aa_key)
		og_score_mode = statistics.mode(og_aa_list)
		#save the most common value (mode) to a variable
		
		#save the calculated statistics to a list
		aa_stats_list = [og_length, percent_good_prots, og_score_mode]
		#and save the calculated statistics to the statistics dictionary
		aa_stats_dict[aa_key] = aa_stats_list
		
		#convert the dictionary to a Pandas dataframe using the keys as the row indexes
		aa_stats_df = pd.DataFrame.from_dict(aa_stats_dict, orient='index')
		#then pull the query ID information out of the index
		aa_stats_df.reset_index(inplace=True)
		#and rename the columns
		aa_stats_df.rename(columns={"index": og_col, 0: "OG_Length", 1: "Percent_Good_Prots", 
								2: "OG_Mode_Score"}, inplace=True)
		
		#write out results
		#first, the summary statistics file as a tab-separated dataframe
		aa_stats_df.to_csv(output_stats, sep='\t', index=False)
		#and then the list of good OGs
		with open(output_ogs, "w") as outfile: 
			#open the file for writing
			#first write a header line
			outfile.write(og_col + "\n")
			for element in good_OG_list:
				#iterate over the list of good OGs 
				#and write the OG IDs out to the file separated by newline characters
				outfile.write(element + "\n")

```

Using it: 

```bash
python assess_OG_startAA_scores.py -h
# usage: assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM [-val THRESHOLD]
#                                    [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE
# This program asseses the quality of OGs in the Metamonad database (or filtered version of the same, including at
# minimum the columns "Query", "Secretion_Score", "Mitochondria_Score", "StartAA" and the OG information column of the
# OG program desired to be used as input for the search), based on the desired representation criteria of the user. The
# following can be used to filter for representation: - Percent of proteins in a given OG which meet a given minimum
# prediction score for either the mitochondrial or secretory pathway - Percent of proteins in a given OG which start
# with Methionine, or with either Methionine or Leucine
# optional arguments:
#   -h, --help            show this help message and exit
#   -val THRESHOLD, --threshold_value THRESHOLD
#                         Integer value of minimum percent of proteins to meet the given test criteria in OGs. Test
#                         criteria are: - Percent proteins in OG predicted to score at or above given score threshold
#                         for given pathway - Percent proteins in OG achieving given completion level based on first
#                         amino acid in sequence (Default = 80)
#   -score SCORE, --filt_score SCORE
#                         This argument allows the user to define he scoring threshold that should be used for the given
#                         pathway scoring filtration. (Default = 4.0) This argument is automatically called for
#                         mitochondrial or sectretory pathway testing.
#   -incl INCLUDED_AA, --includedAA INCLUDED_AA
#                         This argument allows the user to define the sequence quality checking to be used. The options
#                         are: "Met" OR "Leu" Where "Met" will filter for percent of proteins that start with
#                         Methionine; while "Leu" will filter for percent of proteins that start with Methionine or
#                         Leucine. This argument is automatically called for completion quality testing. The default is
#                         "Met".
#   -out OUT_NAME, --outname OUT_NAME
#                         This argument allows the user to define an output file basename. The default basename is
#                         "Assessed_[FILTRATION_CATEGORY]_[SCORE/INCLUDED_AA]_[datetime]", and the output files are: -
#                         [BASENAME]_filt[THRESHOLD]_stats.txt: Summary statistics file -
#                         [BASENAME]_filt[THRESHOLD]_OGs.txt: File containing OGs meeting threshold in format OG1 OG2
#                         etc.
#   -v, --version         show program's version number and exit
# required arguments:
#   -cat FILTRATION_CATEGORY, --filtration_category FILTRATION_CATEGORY
#                         This argument requires the user to specify the type of data the filtration should be performed
#                         on. The options are: "Secretion_Score" OR "Mitochondria_Score" OR "StartAA".
#   -query QUERY_IDS, --query_ids QUERY_IDS
#                         Here specify the query protein IDs in one of the following formats: single protein ID OR
#                         comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) OR file with protein IDs separated
#                         by newlines When using unencoded query IDs, a key portion of the protein name is sufficient -
#                         The entirety of the protein header does not need to be used.
#   -prog OG_PROGRAM, --og_program OG_PROGRAM
#                         This argument requires the user to specify the OG program whose data the filtration should be
#                         performed on. The options are: "Br_Grouped_OGs" OR "Br_Single_OGs" OR "ProteinOrtho_OG" OR
#                         "OrthoFinder_OG" OR "SonicParanoid_OG".
#   -i INPUT_FILE, --input INPUT_FILE
#                         The input file should be the Metamonad database (or filtered version of the same, including at
#                         minimum the columns 'Query', 'Secretion_Score', 'Mitochondria_Score', 'StartAA' and the OG
#                         information column of the OG program desired to be used as input for the search).
#help message above
#and now, to use it: 
#model: 
python assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM [-val THRESHOLD] [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE
#applying it: 
python assess_OG_startAA_scores.py -cat Secretion_Score -query OG_211,OG_266 -prog SonicParanoid_OG -val 50 -score 3.5 -out test_assess_filt -i Metamonada_pred_OG_DB__filt_scores-startAA-pfam__200.txt
python assess_OG_startAA_scores.py -cat StartAA -query OG_211,OG_266 -prog SonicParanoid_OG -val 50 -incl Met -out test_assess_filt2 -i Metamonada_pred_OG_DB__filt_scores-startAA-pfam__200.txt
#ok the program is working, now we can apply it
###
#working in the / directory
#model: 
python assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM [-val THRESHOLD] [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE

```

With the above, rerunning the identification (and subsequently scoring) the OGs associated with proteins of _T. vaginalis_ predicted to be secreted in the literature. The following articles are used as sources for the control datasets: 
 - Secretome: 
   - de Miguel et al. 2010
   - Štáfková et al. 2018
   - Twu et al. 2013
 - Hydrogenosome: 
   - ((3 papers listed in Tachezy's article/chapter))
   - Rada et al. 2011 
   - Schneider et al. 2011 
   - Beltrán et al. 2013

```bash
#extracting key proteins
#working in the Trich_Parab/Thesis_Work/Data/Control_data/ directory 
#extracting IDs from the control data files
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt > Ctrl_data_mito_IDs.txt
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_secretome.txt > Ctrl_data_sec_IDs.txt
#getting the encoded protein lists
python ../query_prot_ids__v2.py -h
# usage: query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS
# This program parses the protein ID encoding reference file in order to extract queried portions of the dataframe (ie.
# encoded and unencoded value pairs). It can accomadate the name variation of the Trichomonas vaginalis proteins.
# positional arguments:
#   REF_DB                The reference file should have encoded protein names in the first column and original protein
#                         names in the second column.
#   QUERY_TYPE            The query type must be specified as either: "encoded" OR "unencoded".
#   QUERY_IDS             Here specify the query protein IDs in one of the following formats: single protein ID OR
#                         comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) OR file with protein IDs separated
#                         by newlines When using unencoded query IDs, a key portion of the protein name is sufficient -
#                         The entirety of the protein header does not need to be used.
# optional arguments:
#   -h, --help            show this help message and exit
#   -tvag TVAG_FILE, --Tvag_ref TVAG_FILE
#                         The T. vaginalis protein name aliases file from TrichDB should be used as input.
#   -out OUT_NAME, --outname OUT_NAME
#                         This argument allows the user to define an output file name.
#   -v, --version         show program's version number and exit
#model: 
python query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS
#applying it: 
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_mito.txt ../encoding_summary_ref.txt unencoded Ctrl_data_mito_IDs.txt
# Query type: unencoded
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_sec.txt ../encoding_summary_ref.txt unencoded Ctrl_data_sec_IDs.txt
# Query type: unencoded
#quick quality checking of outputs...
#checking hydrogenosome/mitochondria:
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt | wc -l
# 1006
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt | sort | uniq | wc -l
# 684
wc -l encodingSummary_Ctrl_data_mito.txt
# 684 encodingSummary_Ctrl_data_mito.txt
#hmmm... it looks like there's one protein missing, one that it couldn't find a match to
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_mito_TEST.txt ../encoding_summary_ref.txt unencoded Ctrl_data_mito_IDs.txt
# Query type: unencoded
wc -l encodingSummary_Ctrl_data_mito_TEST.txt
# 684 encodingSummary_Ctrl_data_mito_TEST.txt
#ok so removing spaces in list elements didn't fix things
#went through the file & manually check 461 proteins, & didn't find the issue
#so we're going to move the fuck on
#but rerun the python because I edited the script a bit
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_mito.txt ../encoding_summary_ref.txt unencoded Ctrl_data_mito_IDs.txt
# Query type: unencoded
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_sec.txt ../encoding_summary_ref.txt unencoded Ctrl_data_sec_IDs.txt
# Query type: unencoded
#rechecking mitochondria file:
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt | wc -l
# 1006
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt | sort | uniq | wc -l
# 684
wc -l encodingSummary_Ctrl_data_mito.txt
# 684 encodingSummary_Ctrl_data_mito.txt
#checking secretome: 
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_secretome.txt | wc -l
# 2786
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_secretome.txt | sort | uniq | wc -l
# 2343
wc -l encodingSummary_Ctrl_data_sec.txt
# 2344 encodingSummary_Ctrl_data_sec.txt
#create input files for protein extraction from Metamonad database
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Ctrl_data_mito.txt > encodingSummary_Ctrl_data_mito_encoded.txt
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Ctrl_data_sec.txt > encodingSummary_Ctrl_data_sec_encoded.txt
#transfer files to the server
#extract these rows from a larger version of the Metamonad database
#model:
python extract_prot_db.py input_db input_prots [search_col]
#applying it: 
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/Ctrl_Data/ directory
python ../../Scripts/extract_prot_db.py ../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt encodingSummary_Ctrl_data_mito_encoded.txt Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt
python ../../Scripts/extract_prot_db.py ../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt encodingSummary_Ctrl_data_sec_encoded.txt Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt
wc -l *.txt
#    683 encodingSummary_Ctrl_data_mito_encoded.txt
#   2343 encodingSummary_Ctrl_data_sec_encoded.txt
#    684 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt
#   2344 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt
#   6054 total
#that looks right
#get list of OGs associated with these proteins
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq > Tvag_mito_ctrl_OF_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq > Tvag_mito_ctrl_SP_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort | uniq > Tvag_sec_ctrl_SP_OGs.txt
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort| uniq > Tvag_sec_ctrl_OG_OGs.txt
#checking the predictions
#secreted proteins
awk -F '\t' '(NR>1) {print $7}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort | uniq -c
#    2001 0.0
#      59 0.5
#     185 1.0
#       8 1.5
#      51 2.0
#      11 2.5
#      21 3.0
#       3 3.5
#       4 4.0
#which is... NOT GREAT!!1!
#mitochondrial/hydrogenosome proteins
awk -F '\t' '(NR>1) {print $8}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq -c
#    481 0.0
#     61 0.5
#     61 1.0
#     17 1.5
#     24 2.0
#      5 2.5
#     21 3.0
#      3 3.5
#     10 4.0
#and again,,, not great! 



```

The scores of the proteins that have in studies been predicted to be secreted or mitochondrial are very often not being caught by most (or all) of the prediction software we used. 

I am choosing to use a threshold of 3.0 as a starting point for filtration, somewhat arbitrarily. Any lower would include far too many proteins, and in fairness, both the mitochondrial and secreted control data does see a slight bump at a score of 3.0. 

There is one mitochondrial protein that cannot seem to be identified in the protein dataset I have from _T. vaginalis_, even after a very thorough search. Stairs suggested that this might be a situation where a protein that was initially identified was later revealed thorugh more thorough exploration of the data to be a software-based error, a duplication in the computer analysis of the genes, vs. a true duplication. As I was never able to identify the specific protein causing this issue, this will remain only a guess. 


#### Working with the PFam data

Create data tables of PFams associated with each OG (script saved to og2PFam_pivot__v2.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: og2PFam_pivot__v2.py
Date: 2022.04.18
Author: Virág Varga

Description:
	This program uses the Metamonad database, or a filtered copy of the same containing
		all PFam and desired OG information to create a table linking all OGs to their
		associated PFam domains.
		The PFam colum used is: pfamEN_hit

List of functions:
	concat (Source: IPRpivot.py, Courtney Stairs)

List of standard and non-standard modules used:
	sys
	pandas
	os
	collections.Counter

Procedure:
	1. Importing necessary modules & function; determining inputs & outputs.
	2. Importing database into Pandas and extracting relevant columns.
	3. Pivoting the dataframe and removing unnecessary data.
	4. Creating a dataframe with count data on PFams per OG. 
	5. Creating a simple dataframe showing OGs and associated PFams.
	6. Writing out the results to two tab-separated text files.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a version of the Metamonad database which
		includes both PFam and OG assignments. The PFam column used by this program
		is pfamEN_hit.
	- The output file names are not user-defined.

Version:
	This program can be considered Version 2.0 of the og2PFam_pivot.py program.
		The funcitonality of the orginal program has been expanded to account for the
		larger, more complex dataset of the Metamonad database.

Usage
	./og2PFam_pivot__v2.py input_db og_col
	OR
	python og2PFam_pivot__v2.py input_db og_col

	Where og_col should be the name of the OG column for which the PFam domains should
		be aggregated.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules & function, determine inputs & outputs

#import necessary modules
import sys #allow execution of code from the command line
import pandas as pd #facilitates manipulation of dataframes in Python
import os #allow access to computer files
from collections import Counter #enables easy counting of elements


#make function for joining strings with ','
#Source: IPRpivot.py, Courtney Stairs
def concat(str):
	return ','.join(str)


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__200prots.txt"

og_col = sys.argv[2]
#og_col = "SonicParanoid_OG"


#output_db will contain information on PFams associated with each OG
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#first extract base file name
#and then define the output file names
output_db = "OGs2PFams_" + og_col + ".txt"
output_counts = "OGs2PFams_" + og_col + "_Counts.txt"


#Part 2: Import database into Pandas, extract relevant columns

#read in the input OG database file, assigning the first row as a header row
input_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.


#select the columns that will be used to create the pivot table
pfam_og_RAW_df = input_df[['pfamEN_hit', og_col]].copy()
#copy those columns only into a new dataframe


#Part 3: Pivot the dataframe and remove unnecessary data

#pivot the table
pfam_og_WIP_df = pd.pivot_table(pfam_og_RAW_df, index = og_col, aggfunc=concat).reset_index()
#use `.reset_index()` to pull the OG ID column out of the index

#the above method leaves extraneous whitespace, which needs to be removed
pfam_og_WIP_df['pfamEN_hit'] = pfam_og_WIP_df['pfamEN_hit'].str.replace(' ', '')
#then remove the row of the dataframe containing the "OG" of "-"
pfam_og_df = pfam_og_WIP_df[pfam_og_WIP_df[og_col] != "-"].copy()
#use `.copy()` to allow manipulation of the new dataframe


#Part 4: Create dataframe with count data on PFams

#create version of dataframe with count data (ie. non-set)
pfam_count_df = pfam_og_df.copy()
pfam_count_df['pfamEN_hit'] = pfam_count_df['pfamEN_hit'].apply(lambda x: ','.join(sorted(x.split(','))))
#now remove non-hits from the dataframe
pfam_count_df['pfamEN_hit'] = pfam_count_df['pfamEN_hit'].str.replace('-,', '')
#also remove OGs without PFam hits
pfam_count_prep_df = pfam_count_df[pfam_count_df['pfamEN_hit'] != "-"].copy()
pfam_count_prep_df['pfamEN_hit'] = pfam_count_prep_df['pfamEN_hit'].apply(lambda x: x.split(','))
pfam_count_prep_df.set_index(og_col, inplace=True)


#create an empty dictionary to store the data
pfam_count_dict = {}

for index, row in pfam_count_prep_df.iterrows(): 
	#iterate over the dataframe row by row
	og_pfam_list = row[0]
	#save the list of PFam hits to a variable
	og_pfam_counts = str(dict(Counter(og_pfam_list)))
	#save the results of the PFam counts to a variable as a string
	og_pfam_counts = og_pfam_counts.replace('{', '')
	#remove the {braces} currently in the string
	og_pfam_counts = og_pfam_counts.replace('}', '')
	#remove spaces from the string
	og_pfam_counts = og_pfam_counts.replace(' ', '')
	#remove the single quotes from inside the strin
	#need to use "\" character to prevent EOF parsing error
	og_pfam_counts = og_pfam_counts.replace('\'', '')
	#now populate the dictionary
	pfam_count_dict[index] = og_pfam_counts

#convert dictionary back to dataframe
final_pfam_count_df = pd.DataFrame.from_dict(pfam_count_dict, orient='index', columns=['pfamEN_hits_Counts'])
#and pull the OG data out of the index
final_pfam_count_df.reset_index(inplace=True)
#before renaming it according to the OG program being used
final_pfam_count_df.rename(columns={'index': og_col}, inplace=True)


#Part 5: Create simple dataframe showing OGs and associated PFams

#aggregate PFam IDs into a comma-separated string, and remove duplicate PFam hits
pfam_og_df['pfamEN_hit'] = pfam_og_df['pfamEN_hit'].apply(lambda x: ','.join(sorted(list(set(x.split(','))))))

#remove "-" "hits" from the pfamEN_hits column of data
pfam_og_df['pfamEN_hit'] = pfam_og_df['pfamEN_hit'].str.replace('-,', '')
#also remove OGs without PFam hits
final_pfam_og_df = pfam_og_df[pfam_og_df['pfamEN_hit'] != "-"].copy()


#Part 6: Write out results

#write out count data to tab-separated text file
final_pfam_count_df.to_csv(output_counts, sep='\t', index=False)

#write out OGs to PFams simplified data to a tab-separated text file
final_pfam_og_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python og2PFam_pivot__v2.py input_db og_col
#applying it: 
python ../Scripts/og2PFam_pivot__v2.py Metamonada_pred_OG_DB.txt Br_Grouped_OGs
python ../Scripts/og2PFam_pivot__v2.py Metamonada_pred_OG_DB.txt Br_Single_OGs
python ../Scripts/og2PFam_pivot__v2.py Metamonada_pred_OG_DB.txt ProteinOrtho_OG
python ../Scripts/og2PFam_pivot__v2.py Metamonada_pred_OG_DB__filt_scores-pfam.txt OrthoFinder_OG
python ../Scripts/og2PFam_pivot__v2.py Metamonada_pred_OG_DB__filt_scores-pfam.txt SonicParanoid_OG
###
#A. lanta integration
python ../Scripts/og2PFam_pivot__v2.py Metamonada_Alanta_pred_OG_DB.txt Br_Grouped_OGs
python ../Scripts/og2PFam_pivot__v2.py Metamonada_Alanta_pred_OG_DB.txt Br_Single_OGs
python ../Scripts/og2PFam_pivot__v2.py Metamonada_Alanta_pred_OG_DB.txt ProteinOrtho_OG
python ../Scripts/og2PFam_pivot__v2.py Metamonada_Alanta_pred_OG_DB.txt OrthoFinder_OG
python ../Scripts/og2PFam_pivot__v2.py Metamonada_Alanta_pred_OG_DB.txt SonicParanoid_OG

```


### Final Filtration

Using the control data as a source (code here copied from above): 

```bash
#extracting key proteins
#working in the Trich_Parab/Thesis_Work/Data/Control_data/ directory 
#extracting IDs from the control data files
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt > Ctrl_data_mito_IDs.txt
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_secretome.txt > Ctrl_data_sec_IDs.txt
#getting the encoded protein lists
python ../query_prot_ids__v2.py -h
# usage: query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS
# This program parses the protein ID encoding reference file in order to extract queried portions of the dataframe (ie.
# encoded and unencoded value pairs). It can accomadate the name variation of the Trichomonas vaginalis proteins.
# positional arguments:
#   REF_DB                The reference file should have encoded protein names in the first column and original protein
#                         names in the second column.
#   QUERY_TYPE            The query type must be specified as either: "encoded" OR "unencoded".
#   QUERY_IDS             Here specify the query protein IDs in one of the following formats: single protein ID OR
#                         comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) OR file with protein IDs separated
#                         by newlines When using unencoded query IDs, a key portion of the protein name is sufficient -
#                         The entirety of the protein header does not need to be used.
# optional arguments:
#   -h, --help            show this help message and exit
#   -tvag TVAG_FILE, --Tvag_ref TVAG_FILE
#                         The T. vaginalis protein name aliases file from TrichDB should be used as input.
#   -out OUT_NAME, --outname OUT_NAME
#                         This argument allows the user to define an output file name.
#   -v, --version         show program's version number and exit
#model: 
python query_prot_ids__v2.py [-h] [-tvag TVAG_FILE] [-out OUT_NAME] [-v] REF_DB QUERY_TYPE QUERY_IDS
#applying it: 
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_mito.txt ../encoding_summary_ref.txt unencoded Ctrl_data_mito_IDs.txt
# Query type: unencoded
python ../query_prot_ids__v2.py -tvag TrichDB-57_TvaginalisG3_GeneAliases.txt -out encodingSummary_Ctrl_data_sec.txt ../encoding_summary_ref.txt unencoded Ctrl_data_sec_IDs.txt
# Query type: unencoded
#checking mitochondria file:
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt | wc -l
# 1006
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_mito.txt | sort | uniq | wc -l
# 684
wc -l encodingSummary_Ctrl_data_mito.txt
# 684 encodingSummary_Ctrl_data_mito.txt
#checking secretome: 
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_secretome.txt | wc -l
# 2786
awk -F '\t' '(NR>1) {print $2}' Ctrl_data_secretome.txt | sort | uniq | wc -l
# 2343
wc -l encodingSummary_Ctrl_data_sec.txt
# 2344 encodingSummary_Ctrl_data_sec.txt
#create input files for protein extraction from Metamonad database
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Ctrl_data_mito.txt > encodingSummary_Ctrl_data_mito_encoded.txt
awk -F '\t' '(NR>1) {print $1}' encodingSummary_Ctrl_data_sec.txt > encodingSummary_Ctrl_data_sec_encoded.txt
#transfer files to the server
#extract these rows from a larger version of the Metamonad database
#model:
python extract_prot_db.py input_db input_prots [search_col]
#applying it: 
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/Ctrl_Data/ directory
python ../../Scripts/extract_prot_db.py ../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt encodingSummary_Ctrl_data_mito_encoded.txt Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt
python ../../Scripts/extract_prot_db.py ../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt encodingSummary_Ctrl_data_sec_encoded.txt Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt
wc -l *.txt
#    683 encodingSummary_Ctrl_data_mito_encoded.txt
#   2343 encodingSummary_Ctrl_data_sec_encoded.txt
#    684 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt
#   2344 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt
#   6054 total
#that looks right
#get list of OGs associated with these proteins
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq | sed "/-/d" > Tvag_mito_ctrl_OF_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq | sed "/-/d" > Tvag_mito_ctrl_SP_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort | uniq | sed "/-/d" > Tvag_sec_ctrl_SP_OGs.txt
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort| uniq | sed "/-/d" > Tvag_sec_ctrl_OF_OGs.txt
#checking the predictions
#secreted proteins
awk -F '\t' '(NR>1) {print $7}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort | uniq -c
#    2001 0.0
#      59 0.5
#     185 1.0
#       8 1.5
#      51 2.0
#      11 2.5
#      21 3.0
#       3 3.5
#       4 4.0
#which is... NOT GREAT!!1!
#mitochondrial/hydrogenosome proteins
awk -F '\t' '(NR>1) {print $8}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq -c
#    481 0.0
#     61 0.5
#     61 1.0
#     17 1.5
#     24 2.0
#      5 2.5
#     21 3.0
#      3 3.5
#     10 4.0
#and again,,, not great! 
###
#and even worse? 
#secretion scores on mito ctrl data
awk -F '\t' '(NR>1) {print $7}' Metamonada_pred_OG_DB__filt_scores-startAA-
pfam__Tvag_mito_ctrl.txt | sort | uniq -c
#    617 0.0
#     29 0.5
#     28 1.0
#      1 1.5
#      5 2.0
#      3 3.0
#mito scores on secretion ctrl data
awk -F '\t' '(NR>1) {print $8}' Metamonada_pred_OG_DB__filt_scores-startAA-
pfam__Tvag_sec_ctrl.txt | sort | uniq -c
#   2130 0.0
#     73 0.5
#     71 1.0
#     14 1.5
#     23 2.0
#      3 2.5
#     22 3.0
#      1 3.5
#      6 4.0

```

Will move forward using a score threshold of 3. 

```bash
#pulling proteins with a score threshold of 3 from the larger Metamonad database
#model: 
python filter_scored_prots.py input_db score_col score_threshold
#applying it: 
python ../Scripts/filter_scored_prots.py Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt Secretion_Score 3
python ../Scripts/filter_scored_prots.py Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt Mitochondria_Score 3
#grabbing the OGs for those 
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Mitochondria3.txt | sort | uniq | sed "/-/d" > Metamonad_mito_3_OF_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Mitochondria3.txt | sort | uniq | sed "/-/d" > Metamonad_mito_3_SP_OGs.txt
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Secretome3.txt | sort | uniq | sed "/-/d" > Metamonad_sec_3_SP_OGs.txt
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Secretome3.txt | sort| uniq | sed "/-/d" > Metamonad_sec_3_OF_OGs.txt
#need the `sed "/-/d"` to delete the "OG" of "-"
#link the data from the control directory to the filtration directory
#which is /home/inf-47-2020/ThesisTrich/DB_Construct/FinalFilt/
ln -s /home/inf-47-2020/ThesisTrich/DB_Construct/Ctrl_Data/*.txt .
#ok, now let's see how many OGs this gives us
wc -l Metamonad_*
#  1336 Metamonad_mito_3_OF_OGs.txt
#  1060 Metamonad_mito_3_SP_OGs.txt
#  3944 Metamonad_sec_3_OF_OGs.txt
#  2491 Metamonad_sec_3_SP_OGs.txt
#  8831 total
wc -l Tvag_*
#   442 Tvag_mito_ctrl_OF_OGs.txt
#   470 Tvag_mito_ctrl_SP_OGs.txt
#  1600 Tvag_sec_ctrl_OF_OGs.txt
#  1633 Tvag_sec_ctrl_SP_OGs.txt
#  4145 total
```

Now, need to evaluate the quality of the OGs. 

For this step, I had to create a modified version of the filter_OG_speciesDB.py script (saved to filter_OG_speciesDB__v2.py) that would allow for the searching of query OG IDs. 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_OG_speciesDB__v2.py
Date: 2022-05-05
Author: Virág Varga

Description:
	This program filters the species and phyla representation per OG database created by
		the og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with
		the same column structure), based on the desired representation criteria of the user.
	The following can be used to filter for representation:
		- Presence or absence of specific species (or list of species) in an OG
		- Presence or absence of specific phylum (or list of phyla) in an OG
		- Percentage of species or phyla represented in an OG
	In addition, a `-query` argument can be provided to search for the presence of specific
		OG IDs within the filtered dataset.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	argparse
	pandas
	os

Procedure:
	1. Assignment of command-line arguments with argparse.
	2. Parsing arguments.
		- Importing modules, determining inputs & outputs, importing data
		- Determining filtration category and performing dataframe querying

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The name of the output file is not user-defined.
	- The input file used for the program must be a file created by the og_db_plusSpeciesRep__v2.py
		program (or a filtered version of the same, with the same column structure).
	- The program cannot accept multiple input program files, nor can it determine the type
		of input file it was given (ie. which program's results file was used as input).

Inputs & Outputs:
	- Inputs:
		+ The mandatory input file is a file created by the og_db_plusSpeciesRep__v2.py program
			(or a filtered version of the same, with the same column structure).
		+ This program accepts input query species/phyla lists in the following formats:
			- Singular species/phylum provided on the command line
			- Comma-separated list of species/phyla provided on the command line (ex. `ID_1,ID_2,ID_3`)
			- File containing list of species/phyla in format: ID1\nID2 etc.
		+ If the threshold option is selected, an integer percentage value must be given
			(ex. 80 for 80%), otherwise the default is 80.
		+ With the `-query` option selected, this program will accept query OG IDs in the same formats 
			as it accepts species/phyla lists: single, comma-separated or newline-separated in a file.
			The input type does not need to be specified, as the program will detect it. 
	- Outputs:
		+ The program will output a text file containing the OGs that met the search criteria in
			the format: OG_ID1\nOG_ID2\nOG_ID3 etc
		+ If the -out flag is used, the program will output a filtered version of the dataframe,
			including all rows which match the query.

Version:
	This script is essentially a combination of two scripts: filter_OG_speciesDB.py and
		assess_OG_startAA_scores.py. It allows for the assessment of species representation
		within a given list of OGs, as seen in filter_OG_speciesDB.py, but allows the option to
		search for a specified list of Query OGs within the filtered results.

Usage:
	./filter_OG_speciesDB__v2.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID]
		[-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-query QUERY_IDS] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE
	OR
	python filter_OG_speciesDB__v2.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID]
		[-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-query QUERY_IDS] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""

#################################   ARGPARSE   #######################################

import argparse
#the argparse module allows for a single program script to be able to carry out a variety of specified functions
#this can be done with the specification of unique flags for each command


#The most general description of what this program can do is defined here
parser = argparse.ArgumentParser(description =
								 'This program filters the species and phyla representation per OG database created by \
									the og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with \
									the same column structure), based on the desired representation criteria of the user. \
								The following can be used to filter for representation: \
									- Presence or absence of specific species (or list of species) in an OG \
									- Presence or absence of specific phylum (or list of phyla) in an OG \
									- Percentage of species or phyla represented in an OG \
								In addition, a `-query` argument can be provided to search for the presence of specific \
									OG IDs within the filtered dataset.')

#create a group of arguments which will be required
requiredArgNames = parser.add_argument_group('required arguments')
#ref: https://stackoverflow.com/questions/24180527/argparse-required-arguments-listed-under-optional-arguments

#adding the arguments that the program can use
requiredArgNames.add_argument(
	'-type', '--filtration_type',
	metavar='FILTRATION_TYPE',
	dest='filt_type',
	choices=['include', 'exclude', 'threshold'],
	#ref: https://stackoverflow.com/questions/15836713/allowing-specific-values-for-an-argparse-argument
	help = 'This argument requires the user to specify the type of filtration that should be done. \
		Use "include" to include species/phylum IDs, "exclude" to exclude species/phylum IDs, \
			"threshold" to test for inclusion percentage >= the threshold value given',
	required=True
	)
	#the '-type' flag is used to tell the program what kind of analysis is going to be done
requiredArgNames.add_argument(
	'-cat', '--filtration_category',
	metavar='FILTRATION_CATEGORY',
	dest='filt_cat',
	choices=['species', 'phylum'],
	help = 'This argument requires the user to specify the type of data the filtration should be performed on. \
		The options are: "species" OR "phylum".',
	required=True
	)
	#the '-prog' flag is used to tell the program which pragram's data is being used as input

parser.add_argument(
	'-single', '--single_id',
	metavar='SINGLE_ID',
	dest='single_ID',
	help = 'This argument takes a single species or phylum ID as input.'
	)
	#the '-single' flag will import the input single species/phylum as a simple string
parser.add_argument(
	'-list', '--id_list',
	metavar='ID_LIST',
	dest='ID_list',
	help = 'This argument takes a comma-separated list of species or phylum IDs as input (ex. ID1,ID2,ID3).'
	)
	#the '-list' flag will import the input list of species/phyla IDs from the command line
parser.add_argument(
	'-file', '--ID_file',
	dest='ID_file',
	metavar='ID_FILE',
	help = "The file containing species or phylum IDs should be in the format: ID\nID\nID etc.",
	type=argparse.FileType('r')
	)
	#the 'file' flag will import the list of species or phyla IDs from the given file
parser.add_argument(
	'-val', '--threshold_value',
	dest='threshold',
	metavar='INTEGER',
	type=int,
	default=80,
	help = "Integer value of minimum percent of species or phyla that should be represented in OGs. (Default = 80)"
	)
	#the `type=int` argument allows argparse to accept the input as an integer
	#the `default=80` gives a default minimum membership filtration value
	#ref: https://stackoverflow.com/questions/44011031/how-to-pass-a-string-as-an-argument-in-python-without-namespace
	#ref: https://stackoverflow.com/questions/14117415/in-python-using-argparse-allow-only-positive-integers

parser.add_argument(
	'-query', '--query_ids',
	dest='query_ids',
	metavar='QUERY_IDS',
	help = 'Here specify the query OG IDs in one of the following formats: \n \
		single OG ID OR comma-separated list of OG IDs (ex. `ID_1,ID_2,ID_3`) \n \
			OR file with OG IDs separated by newlines.'
	)
	#the '-query' flag specifies that a query OG ID (or list of them) has been given,
	#as well as the formats that can be used

parser.add_argument(
	'-suf', '--suffix',
	metavar='SUFFIX',
	dest='suffix',
	help = 'This argument allows the user to define a suffix to be used for the output file name.'
	)
	#the '-suf' flag allows the user to define a suffix to be used for the output file name.
parser.add_argument(
	'-out', '--filt_db_out',
	action='store_true',
	help = 'This argument will enable the program to print the entire filtered database, not only the list of OG IDs.'
	)
	#the '-out' flag will enable the program to print the entire filtered database, not only the list of OG IDs
parser.add_argument(
	'-v', '--version',
	action='version',
	version='%(prog)s 1.0'
	)
	#This portion of the code specifies the version of the program; currently 1.0
	#The user can call this flag ('-v') without specifying input and output files
requiredArgNames.add_argument(
	'-i', '--input',
	dest='input_file',
	metavar='INPUT_FILE',
	help = "The input file should be a species and phyla representation per OG database created by \
		the og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with \
		the same column structure).",
	type=argparse.FileType('r'),
	required=True
	)
	#this portion of code specifies that the program requires an input file, and it should be opened for reading ('r')


args = parser.parse_args()
#this command allows the program to execute the arguments in the flags specified above


#################################   Parse Arguments   ######################################


#Part 1: Import modules, determine inputs & outputs, import data

#import necessary modules
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files


#designate input file name as variable
infile = args.input_file.name
#import the dataframe into Pandas
ortho_df = pd.read_csv(infile, sep = '\t', header = 0)
#identify OG column name (for use later)
og_col = ortho_df.columns[0]


#define the output file basename based on the input file name
base = os.path.basename(infile)
out_full = os.path.splitext(base)[0]


#import the query list
if args.single_ID:
	#if the option to enter only one query species/phylum on the command line was selected
	search_IDs = args.single_ID
	#save the query species/phylum to a variable
	#and save that variable to a 1-member list
	search_list = [search_IDs]

if args.ID_list:
	#if the option to give a list of species/phyla IDs was given
	search_IDs = args.ID_list
	#save the contents of the comma-separated string to a variable
	#and save the species/phyla IDs to a list based on comma placement in the input string
	search_list = search_IDs.split(",")

if args.ID_file:
	#if an input file was given containing the species/phyla IDs to search
	#save the file to a variable
	search_file = args.ID_file
	#save the contents of the file (should be a column of species/phyla IDs) to a list
	search_list = [line.rstrip('\n') for line in search_file]


#Part 2: Determine filtration category and perform dataframe querying

if args.filt_cat == 'species':
	#if the species category was selected
	#identify the relevant dataframe columns and save to variables
	percent_col = "Species_Percent"
	#species percent column
	rep_col = "Species_Represented"
	#list of species represented
if args.filt_cat == 'phylum':
	#if the phylum category was selected
	#identify the relevant dataframe columns and save to variables
	percent_col = "Phylum_Percent"
	#phylum percent column
	rep_col = "Phyla_Represented"
	#list of phylum represented


if args.filt_type == 'threshold':
	#if the threshold option was selected
	#parse the relevant argument to determine the threshold value
	threshold_value = args.threshold
	#now determine the decimal value of the threshold percent
	threshold_decimal = threshold_value/100

	#filter the relevant dataframe column on the basis of the threshold value
	filt_ortho_df = ortho_df[ortho_df[percent_col] >= threshold_decimal]
	
	#if the query option is given, test for the inclusion of specific OGs
	if args.query_ids:
		#if the argument has been specified by the user
		#identify the list of query OG IDs
		og_ids = args.query_ids

		#import the query list
		if os.path.isfile(og_ids):
			#if the input selection of OGs is a file
			with open(og_ids, 'r') as infile:
				#open the file for reading
				#and save the contents of the file (should be a column of protein query IDs) to a list
				query_list = [line.rstrip('\n') for line in infile]
				#eliminate duplicates
				query_list = list(set(query_list))
		else:
			#if the input protein query ID list is a string instead of a text file
			#save the contents of the comma-separated string to a list variable
			query_list = og_ids.split(",")
			#eliminate duplicates
			query_list = list(set(query_list))

		#now select the specific portion of the dataframe that includes the query OGs
		query_filt_ortho_df = filt_ortho_df[filt_ortho_df[og_col].isin(query_list)].copy()
		#then create the list of query OGs that meet the filtration criteria
		og_list = query_filt_ortho_df[og_col].unique()

		#write message for bad OGs
		bad_OG_message = "No query OGs have met the filtration criteria!"
	
	else:
		#if the query search argument is not provided
		#create a list of the OG IDs that met the threshold
		og_list = filt_ortho_df[og_col].unique()

		#write message for bad OGs
		bad_OG_message = "No OGs have met the filtration criteria!"

	#now create the relevant results file(s)
	if len(og_list) == 0:
		#check if any OGs have hit the threshold
		#and if not, print message to the console
		print(bad_OG_message)
	else:
		#if there are OGs to report
		if args.suffix:
			#if the user has provided a suffix
			suffix_out = args.suffix
			#save the suffix to a variable
			#and designate the outfile name
			output_file = out_full + "_" + suffix_out + "_" + str(threshold_value) + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_" + suffix_out + "_filtDB_" + str(threshold_value) + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)
		else:
			#if the user has not given a file suffix, use the default outfile name
			output_file = out_full + "_filt" + str(threshold_value) + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_filtDB_" + str(threshold_value) + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)


if args.filt_type == 'include':
	#if the inclusion option was selected
	#first, convert the relevant data colum elements into a list (vs. comma-separated string)
	ortho_df[rep_col] = ortho_df[rep_col].str.split(', ')
	#now see if the species/phyla are all represented in the list
	#ref: https://stackoverflow.com/questions/60932036/check-if-pandas-column-contains-all-elements-from-a-list
	search_set = set(search_list)
	#first create a set of the list of species/phyla to search for
	#then search the appropriate column for all OGs that include all of the species/phyla in the set
	filt_ortho_df = ortho_df[ortho_df[rep_col].map(search_set.issubset)].copy()

	#if the query option is given, test for the inclusion of specific OGs
	if args.query_ids:
		#if the argument has been specified by the user
		#identify the list of query OG IDs
		og_ids = args.query_ids

		#import the query list
		if os.path.isfile(og_ids):
			#if the input selection of OGs is a file
			with open(og_ids, 'r') as infile:
				#open the file for reading
				#and save the contents of the file (should be a column of protein query IDs) to a list
				query_list = [line.rstrip('\n') for line in infile]
				#eliminate duplicates
				query_list = list(set(query_list))
		else:
			#if the input protein query ID list is a string instead of a text file
			#save the contents of the comma-separated string to a list variable
			query_list = og_ids.split(",")
			#eliminate duplicates
			query_list = list(set(query_list))

		#now select the specific portion of the dataframe that includes the query OGs
		query_filt_ortho_df = filt_ortho_df[filt_ortho_df[og_col].isin(query_list)].copy()
		#then create the list of query OGs that meet the filtration criteria
		og_list = query_filt_ortho_df[og_col].unique()

		#write message for bad OGs
		bad_OG_message = "No query OGs have met the filtration criteria!"

	else:
		#if the query search argument is not provided
		#create a list of the OG IDs that met the threshold
		og_list = filt_ortho_df[og_col].unique()

		#write message for bad OGs
		bad_OG_message = "No OGs have met the filtration criteria!"

	#now create the relevant results file(s)
	if len(og_list) == 0:
		#check if any OGs have hit the threshold
		#and if not, print message to the console
		print(bad_OG_message)
	else:
		#if there are OGs to report
		if args.suffix:
			#if the user has provided a suffix
			suffix_out = args.suffix
			#save the suffix to a variable
			#and designate the outfile name
			output_file = out_full + "_incl_" + suffix_out + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				#turn the lists in the cells into comma-separated strings
				filt_ortho_df[rep_col] = filt_ortho_df[rep_col].apply(lambda x: ', '.join(map(str, x)))
				output_db = out_full + "_incl_" + suffix_out + "_filtDB" + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)
		else:
			#if the user has not given a file suffix, use the default outfile name
			output_file = out_full + "_include.txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				#turn the lists in the cells into comma-separated strings
				filt_ortho_df[rep_col] = filt_ortho_df[rep_col].apply(lambda x: ', '.join(map(str, x)))
				output_db = out_full + "_include_filtDB.txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)


if args.filt_type == 'exclude':
	#if the exclusion option was selected
	#then search the appropriate column for all OGs that do not include the species/phyla
	#ref: https://stackoverflow.com/questions/26577516/how-to-test-if-a-string-contains-one-of-the-substrings-in-a-list-in-pandas
	filt_ortho_df = ortho_df[~ortho_df[rep_col].str.contains('|'.join(search_list))].copy()

	#if the query option is given, test for the inclusion of specific OGs
	if args.query_ids:
		#if the argument has been specified by the user
		#identify the list of query OG IDs
		og_ids = args.query_ids

		#import the query list
		if os.path.isfile(og_ids):
			#if the input selection of OGs is a file
			with open(og_ids, 'r') as infile:
				#open the file for reading
				#and save the contents of the file (should be a column of protein query IDs) to a list
				query_list = [line.rstrip('\n') for line in infile]
				#eliminate duplicates
				query_list = list(set(query_list))
		else:
			#if the input protein query ID list is a string instead of a text file
			#save the contents of the comma-separated string to a list variable
			query_list = og_ids.split(",")
			#eliminate duplicates
			query_list = list(set(query_list))

		#now select the specific portion of the dataframe that includes the query OGs
		query_filt_ortho_df = filt_ortho_df[filt_ortho_df[og_col].isin(query_list)].copy()
		#then create the list of query OGs that meet the filtration criteria
		og_list = query_filt_ortho_df[og_col].unique()

		#write message for bad OGs
		bad_OG_message = "No query OGs have met the filtration criteria!"

	else:
		#if the query search argument is not provided
		#create a list of the OG IDs that met the threshold
		og_list = filt_ortho_df[og_col].unique()

		#write message for bad OGs
		bad_OG_message = "No OGs have met the filtration criteria!"

	#now create the relevant results file(s)
	if len(og_list) == 0:
		#check if any OGs have hit the threshold
		#and if not, print message to the console
		print(bad_OG_message)
	else:
		#if there are OGs to report
		if args.suffix:
			#if the user has provided a suffix
			suffix_out = args.suffix
			#save the suffix to a variable
			#and designate the outfile name
			output_file = out_full + "_excl_" + suffix_out + ".txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_excl_" + suffix_out + "_filtDB" + ".txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)
		else:
			#if the user has not given a file suffix, use the default outfile name
			output_file = out_full + "_exclude.txt"
			#now write out the list to a text file
			with open(output_file, "w") as outfile:
				#open the outfile for writing
				for element in og_list:
					#iterate over the elements in the list
					#and print them to the text file in the format OG_ID1\nOG_ID2\nOG_ID3 etc.
					outfile.write(element + "\n")
			if args.filt_db_out:
				#if the user has requested that the entire filtered database be written out
				output_db = out_full + "_exclude_filtDB.txt"
				#determine the output file name
				#and write out the results to a tab-separated text file
				filt_ortho_df.to_csv(output_db, sep='\t', index=False)

```


```bash
#now the big one: evaluating OG quality
#species/phyla representation first
#first, eliminate OGs without T. vaginalis
python ../../Scripts/filter_OG_speciesDB__v2.py -h
# usage: filter_OG_speciesDB__v2.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY
#                                   [-single SINGLE_ID] [-list ID_LIST] [-file ID_FILE] [-val INTEGER]
#                                   [-query QUERY_IDS] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE
# This program filters the species and phyla representation per OG database created by the
# og_db_plusSpeciesRep__v2.py program (or a filtered version of the same, with the same column
# structure), based on the desired representation criteria of the user. The following can be used to
# filter for representation: - Presence or absence of specific species (or list of species) in an OG -
# Presence or absence of specific phylum (or list of phyla) in an OG - Percentage of species or phyla
# represented in an OG In addition, a `-query` argument can be provided to search for the presence of
# specific OG IDs within the filtered dataset.
# optional arguments:
#   -h, --help            show this help message and exit
#   -single SINGLE_ID, --single_id SINGLE_ID
#                         This argument takes a single species or phylum ID as input.
#   -list ID_LIST, --id_list ID_LIST
#                         This argument takes a comma-separated list of species or phylum IDs as input
#                         (ex. ID1,ID2,ID3).
#   -file ID_FILE, --ID_file ID_FILE
#                         The file containing species or phylum IDs should be in the format: ID ID ID
#                         etc.
#   -val INTEGER, --threshold_value INTEGER
#                         Integer value of minimum percent of species or phyla that should be
#                         represented in OGs. (Default = 80)
#   -query QUERY_IDS, --query_ids QUERY_IDS
#                         Here specify the query OG IDs in one of the following formats: single OG ID
#                         OR comma-separated list of OG IDs (ex. `ID_1,ID_2,ID_3`) OR file with OG IDs
#                         separated by newlines.
#   -suf SUFFIX, --suffix SUFFIX
#                         This argument allows the user to define a suffix to be used for the output
#                         file name.
#   -out, --filt_db_out   This argument will enable the program to print the entire filtered database,
#                         not only the list of OG IDs.
#   -v, --version         show program's version number and exit

# required arguments:
#   -type FILTRATION_TYPE, --filtration_type FILTRATION_TYPE
#                         This argument requires the user to specify the type of filtration that
#                         should be done. Use "include" to include species/phylum IDs, "exclude" to
#                         exclude species/phylum IDs, "threshold" to test for inclusion percentage >=
#                         the threshold value given
#   -cat FILTRATION_CATEGORY, --filtration_category FILTRATION_CATEGORY
#                         This argument requires the user to specify the type of data the filtration
#                         should be performed on. The options are: "species" OR "phylum".
#   -i INPUT_FILE, --input INPUT_FILE
#                         The input file should be a species and phyla representation per OG database
#                         created by the og_db_plusSpeciesRep__v2.py program (or a filtered version of
#                         the same, with the same column structure).
#model: 
python filter_OG_speciesDB__v2.py [-h] -type FILTRATION_TYPE -cat FILTRATION_CATEGORY [-single SINGLE_ID] [-list ID_LIST] [-file ID_FILE] [-val INTEGER] [-query QUERY_IDS] [-suf SUFFIX] [-out] [-v] -i INPUT_FILE
#specific model needed here: 
python ../../Scripts/filter_OG_speciesDB.py -type include -cat species -single Trichomonas_vaginalis -query QUERY_IDS -suf SUFFIX -i ../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Tvag_mito_ctrl_OF_OGs.txt -suf ctrlM -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Tvag_mito_ctrl_SP_OGs.txt -suf ctrlM -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Metamonad_mito_3_OF_OGs.txt -suf filtM3 -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Metamonad_mito_3_SP_OGs.txt -suf filtM3 -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Tvag_sec_ctrl_OF_OGs.txt -suf ctrlS -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Tvag_sec_ctrl_SP_OGs.txt -suf ctrlS -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Metamonad_sec_3_OF_OGs.txt -suf filtS3 -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Metamonad_sec_3_SP_OGs.txt -suf filtS3 -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#checking results
wc -l *.txt
#     683 encodingSummary_Ctrl_data_mito_encoded.txt
#    2343 encodingSummary_Ctrl_data_sec_encoded.txt
#    3752 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Mitochondria3.txt
#   13291 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Secretome3.txt
#     684 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt
#    2344 Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt
#    1336 Metamonad_mito_3_OF_OGs.txt
#    1060 Metamonad_mito_3_SP_OGs.txt
#    3944 Metamonad_sec_3_OF_OGs.txt
#    2491 Metamonad_sec_3_SP_OGs.txt
#     442 OF_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlM.txt
#    1600 OF_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlS.txt
#     480 OF_OGs_parsed__OG-Species-PhylaPercent_incl_filtM3.txt
#     999 OF_OGs_parsed__OG-Species-PhylaPercent_incl_filtS3.txt
#     470 SP_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlM.txt
#    1633 SP_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlS.txt
#     441 SP_OGs_parsed__OG-Species-PhylaPercent_incl_filtM3.txt
#     915 SP_OGs_parsed__OG-Species-PhylaPercent_incl_filtS3.txt
#     442 Tvag_mito_ctrl_OF_OGs.txt
#     470 Tvag_mito_ctrl_SP_OGs.txt
#    1600 Tvag_sec_ctrl_OF_OGs.txt
#    1633 Tvag_sec_ctrl_SP_OGs.txt
#   43053 total
#the control numbers are the same before and after "filtration"
#which means this is reliably filtering out OGs unique to T. vaginalis
#so we can move on
#first, let's change the file names, because these are going to get LONG
#move these to a new subdirectory then copy back with better name
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlM.txt Tvag_mito_ctrl_OF_OGs_Tv.txt
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlS.txt Tvag_sec_ctrl_OF_OGs_Tv.txt
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_incl_filtM3.txt Metamonad_mito_3_OF_OGs_Tv.txt
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_incl_filtS3.txt Metamonad_sec_3_OF_OGs_Tv.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlM.txt Tvag_mito_ctrl_SP_OGs_Tv.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_incl_ctrlS.txt Tvag_sec_ctrl_SP_OGs_Tv.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_incl_filtM3.txt Metamonad_mito_3_SP_OGs_Tv.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_incl_filtS3.txt Metamonad_sec_3_SP_OGs_Tv.txt
#so now let's filter out OGs unique to T. vaginalis
#specific model: 
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query ../QUERY_IDS -suf SUFFIX -i ../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Tvag_mito_ctrl_OF_OGs_Tv.txt -suf CtrlMnonTvP -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
#where nonTvP means "non-T. vaginalis paralog OG"
###
#quick proof of concept
wc -l OF_OGs_parsed__OG-Species-PhylaPercent_nonTvP_*
#   442 OF_OGs_parsed__OG-Species-PhylaPercent_nonTvP_3.txt
#   348 OF_OGs_parsed__OG-Species-PhylaPercent_nonTvP_4.txt
#   348 OF_OGs_parsed__OG-Species-PhylaPercent_nonTvP_6.txt
#   348 OF_OGs_parsed__OG-Species-PhylaPercent_nonTvP_7.txt
#   327 OF_OGs_parsed__OG-Species-PhylaPercent_nonTvP_8.txt
#  1813 total
#given that 2/26=0.077 while 1/26=0.038 
#the filtration is happening at the correct draw lines
###
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Tvag_mito_ctrl_SP_OGs_Tv.txt -suf CtrlMnonTvP -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Metamonad_mito_3_OF_OGs_Tv.txt -suf M3nonTvP -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Metamonad_mito_3_SP_OGs_Tv.txt -suf M3nonTvP -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Tvag_sec_ctrl_OF_OGs_Tv.txt -suf CtrlSnonTvP -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Tvag_sec_ctrl_SP_OGs_Tv.txt -suf CtrlSnonTvP -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Metamonad_sec_3_OF_OGs_Tv.txt -suf S3nonTvP -i ../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Metamonad_sec_3_SP_OGs_Tv.txt -suf S3nonTvP -i ../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *_6.txt
#   348 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMnonTvP_6.txt
#  1311 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSnonTvP_6.txt
#   376 OF_OGs_parsed__OG-Species-PhylaPercent_M3nonTvP_6.txt
#   714 OF_OGs_parsed__OG-Species-PhylaPercent_S3nonTvP_6.txt
#   379 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMnonTvP_6.txt
#  1366 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSnonTvP_6.txt
#   345 SP_OGs_parsed__OG-Species-PhylaPercent_M3nonTvP_6.txt
#   669 SP_OGs_parsed__OG-Species-PhylaPercent_S3nonTvP_6.txt
#  5508 total
#move them to the results file and copy out the new ones
mv *_6.txt OGs_parsed_FiltVers
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMnonTvP_6.txt Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSnonTvP_6.txt Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_M3nonTvP_6.txt Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/OF_OGs_parsed__OG-Species-PhylaPercent_S3nonTvP_6.txt Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMnonTvP_6.txt Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSnonTvP_6.txt Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_M3nonTvP_6.txt Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt
cp OGs_parsed_FiltVers/SP_OGs_parsed__OG-Species-PhylaPercent_S3nonTvP_6.txt Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt
#quick check this worked correctly
wc -l *_nonTvP.txt
#   376 Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt
#   345 Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt
#   714 Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt
#   669 Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt
#   348 Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt
#   379 Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt
#  1311 Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt
#  1366 Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt
#  5508 total
#ok, yes, we're good to go
#quick checks on number of OGs left for each category
cat Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 609
cat Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 626
cat Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 1807
cat Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 1823
#reminder that the goal is 500-1000 OGs
#so the mitochondria/hydrogensome OG count is close to our lowball goal
#which is concerning given the filtration isn't complete yet
#but the secretome looks to be doing ok
#so let's move on, and try a few different species/phyla filtration methods
#let's do this work in the /home/inf-47-2020/ThesisTrich/DB_Construct/FinalFilt_S3M3/OGs_parsed_FiltVers/ directory
#and then copy up the filtration I decide to use
#Phyla: Parabasalia & Anaeramoebidae
#specific model needed here: 
python ../../../Scripts/filter_OG_speciesDB.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../QUERY_IDS -suf SUFFIX -i ../../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt -suf PActrlM -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt -suf PActrlM -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt -suf PAfiltM3 -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt -suf PAfiltM3 -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt -suf PActrlS -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt -suf PActrlS -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt -suf PAfiltS3 -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type include -cat phylum -list Parabasalia,Anaeramoebidae -query ../Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt -suf PAfiltS3 -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *.txt
#   190 OF_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlM.txt
#   648 OF_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlS.txt
#   267 OF_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltM3.txt
#   297 OF_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltS3.txt
#   216 SP_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlM.txt
#   763 SP_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlS.txt
#   261 SP_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltM3.txt
#   320 SP_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltS3.txt
#  2962 total
cat OF_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlM.txt OF_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltM3.txt | sort | uniq | wc -l
# 347
cat OF_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlS.txt OF_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltS3.txt | sort | uniq | wc -l
# 795
cat SP_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlM.txt SP_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltM3.txt | sort | uniq | wc -l
# 381
cat SP_OGs_parsed__OG-Species-PhylaPercent_incl_PActrlS.txt SP_OGs_parsed__OG-Species-PhylaPercent_incl_PAfiltS3.txt | sort | uniq | wc -l
# 943
#ok, so that definitely dropped the numbers
#secretomes still look ok for both, but mitochondria/hydrogenosome are below 500

#Phyla: 2/5 (40%)
#specific model: 
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../QUERY_IDS -suf SUFFIX -i ../../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlMphyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlMphyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt -suf M3phyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt -suf M3phyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlSphyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlSphyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt -suf S3phyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 40 -query ../Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt -suf S3phyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *_40.txt
#   218 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt
#   783 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_40.txt
#   296 OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt
#   384 OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_40.txt
#   245 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt
#   892 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_40.txt
#   291 SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt
#   412 SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_40.txt
#  3521 total
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt| sort | uniq | wc -l
# 403
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_40.txt OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_40.txt | sort | uniq | wc -l
# 999
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt | sort | uniq | wc -l
# 440
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_40.txt SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_40.txt | sort | uniq | wc -l
# 1138
#even at this low of a stage, mitochondria/hydrogenosome is still not clearing the minimum target of 500 OGs
#secretome is fine, obviously
#Phyla: 3/5 (60%)
#specific model: 
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../QUERY_IDS -suf SUFFIX -i ../../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlMphyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlMphyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt -suf M3phyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt -suf M3phyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlSphyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlSphyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt -suf S3phyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 60 -query ../Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt -suf S3phyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *_60.txt
#   201 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_60.txt
#   689 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt
#   277 OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_60.txt
#   321 OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt
#   226 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_60.txt
#   814 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt
#   271 SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_60.txt
#   348 SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt
#  3147 total
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_60.txt OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_60.txt| sort | uniq | wc -l
# 370
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt | sort | uniq | wc -l
# 853
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_60.txt SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_60.txt | sort | uniq | wc -l
# 403
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt | sort | uniq | wc -l
# 1012
#the mitochondria/hydrogenosome is low again, while the secretome remains ok
#we definitely can't do higher for mitochondria/hydrogenosome
#unwise to do higher for secretome, too, since have to filter further
#Phyla: 5/5 (100%) [for curiosity's sake/comparative data]
#specific model: 
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../QUERY_IDS -suf SUFFIX -i ../../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlMphyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlMphyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt -suf M3phyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt -suf M3phyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlSphyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlSphyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt -suf S3phyla -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat phylum -val 100 -query ../Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt -suf S3phyla -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *_100.txt
#   147 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_100.txt
#   471 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_100.txt
#   202 OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_100.txt
#   224 OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_100.txt
#   168 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_100.txt
#   578 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_100.txt
#   201 SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_100.txt
#   237 SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_100.txt
#  2228 total
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_100.txt OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_100.txt| sort | uniq | wc -l
# 269
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_100.txt OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_100.txt | sort | uniq | wc -l
# 567
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_100.txt SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_100.txt | sort | uniq | wc -l
# 297
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_100.txt SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_100.txt | sort | uniq | wc -l
# 694
#apparently the secretome barely manages to pass
#tho of course the hydrogenosome/mitochondria is way under the target of 500
#Species: 50% (13/26)
#specific model: 
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../QUERY_IDS -suf SUFFIX -i ../../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlMspp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlMspp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt -suf M3spp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt -suf M3spp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlSspp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlSspp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt -suf S3spp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 50 -query ../Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt -suf S3spp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *spp_50.txt
#   179 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_50.txt
#   587 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_50.txt
#   249 OF_OGs_parsed__OG-Species-PhylaPercent_M3spp_50.txt
#   271 OF_OGs_parsed__OG-Species-PhylaPercent_S3spp_50.txt
#   202 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_50.txt
#   695 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_50.txt
#   245 SP_OGs_parsed__OG-Species-PhylaPercent_M3spp_50.txt
#   271 SP_OGs_parsed__OG-Species-PhylaPercent_S3spp_50.txt
#  2699 total
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_50.txt OF_OGs_parsed__OG-Species-PhylaPercent_M3spp_50.txt| sort | uniq | wc -l
# 328
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_50.txt OF_OGs_parsed__OG-Species-PhylaPercent_S3spp_50.txt | sort | uniq | wc -l
# 713
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_50.txt SP_OGs_parsed__OG-Species-PhylaPercent_M3spp_50.txt | sort | uniq | wc -l
# 359
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_50.txt SP_OGs_parsed__OG-Species-PhylaPercent_S3spp_50.txt | sort | uniq | wc -l
# 832
#mitochondria & hydrogenosome not clearing the minimum, even here
#secretome is ok
#Species: 100% (26/26) [only for the sake of comparative statistics]
#specific model: 
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../QUERY_IDS -suf SUFFIX -i ../../OG_Data/INPUT_FILE
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlMspp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlMspp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt -suf M3spp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt -suf M3spp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#secretome
#control
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt -suf CtrlSspp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt -suf CtrlSspp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
#metamonad
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt -suf S3spp -i ../../OG_Data/OF_OGs_parsed__OG-Species-PhylaPercent.txt
python ../../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 100 -query ../Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt -suf S3spp -i ../../OG_Data/SP_OGs_parsed__OG-Species-PhylaPercent.txt
wc -l *spp_100.txt
#   62 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_100.txt
#  151 OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_100.txt
#   71 OF_OGs_parsed__OG-Species-PhylaPercent_M3spp_100.txt
#   59 OF_OGs_parsed__OG-Species-PhylaPercent_S3spp_100.txt
#   61 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_100.txt
#  157 SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_100.txt
#   52 SP_OGs_parsed__OG-Species-PhylaPercent_M3spp_100.txt
#   41 SP_OGs_parsed__OG-Species-PhylaPercent_S3spp_100.txt
#  654 total
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_100.txt OF_OGs_parsed__OG-Species-PhylaPercent_M3spp_100.txt| sort | uniq | wc -l
# 97
cat OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_100.txt OF_OGs_parsed__OG-Species-PhylaPercent_S3spp_100.txt | sort | uniq | wc -l
# 166
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMspp_100.txt SP_OGs_parsed__OG-Species-PhylaPercent_M3spp_100.txt | sort | uniq | wc -l
# 85
cat SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSspp_100.txt SP_OGs_parsed__OG-Species-PhylaPercent_S3spp_100.txt | sort | uniq | wc -l
# 160
#wow those numbers are low, lol
#I think the best way to proceed is with phyla filtration, vs. with species numbers
#that also accomadates better for incomplete genomes & gene losses
#use 40% (2/5) phyla representation threshold for mitochondria/hydrogenosome
#use 60% (3/5) phyla representation threshold for secretome
#even like this, the number of OGs for the mitochondria/hydrogenosome is below the 500 minimum target
#but going lower would remove most useful information on differences between phyla
#so now...
#evaluate OGs based on score threshold of 2
#which would mean half of the programs caught it
#this allows for a wider net, which seems rational in light of how poorly the scoring seems to match the control data
python ../../../Scripts/assess_OG_startAA_scores.py -h
# usage: assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM
#                                    [-val THRESHOLD] [-score SCORE] [-incl INCLUDED_AA]
#                                    [-out OUT_NAME] [-v] -i INPUT_FILE

# This program asseses the quality of OGs in the Metamonad database (or filtered version of the same,
# including at minimum the columns "Query", "Secretion_Score", "Mitochondria_Score", "StartAA" and
# the OG information column of the OG program desired to be used as input for the search), based on
# the desired representation criteria of the user. The following can be used to filter for
# representation: - Percent of proteins in a given OG which meet a given minimum prediction score for
# either the mitochondrial or secretory pathway - Percent of proteins in a given OG which start with
# Methionine, or with either Methionine or Leucine

# optional arguments:
#   -h, --help            show this help message and exit
#   -val THRESHOLD, --threshold_value THRESHOLD
#                         Integer value of minimum percent of proteins to meet the given test
#                         criteria in OGs. Test criteria are: - Percent proteins in OG predicted to
#                         score at or above given score threshold for given pathway - Percent
#                         proteins in OG achieving given completion level based on first amino acid
#                         in sequence (Default = 80)
#   -score SCORE, --filt_score SCORE
#                         This argument allows the user to define he scoring threshold that should be
#                         used for the given pathway scoring filtration. (Default = 4.0) This
#                         argument is automatically called for mitochondrial or sectretory pathway
#                         testing.
#   -incl INCLUDED_AA, --includedAA INCLUDED_AA
#                         This argument allows the user to define the sequence quality checking to be
#                         used. The options are: "Met" OR "Leu" Where "Met" will filter for percent
#                         of proteins that start with Methionine; while "Leu" will filter for percent
#                         of proteins that start with Methionine or Leucine. This argument is
#                         automatically called for completion quality testing. The default is "Met".
#   -out OUT_NAME, --outname OUT_NAME
#                         This argument allows the user to define an output file basename. The
#                         default basename is
#                         "Assessed_[FILTRATION_CATEGORY]_[SCORE/INCLUDED_AA]_[datetime]", and the
#                         output files are: - [BASENAME]_filt[THRESHOLD]_stats.txt: Summary
#                         statistics file - [BASENAME]_filt[THRESHOLD]_OGs.txt: File containing OGs
#                         meeting threshold in format OG1 OG2 etc.
#   -v, --version         show program's version number and exit

# required arguments:
#   -cat FILTRATION_CATEGORY, --filtration_category FILTRATION_CATEGORY
#                         This argument requires the user to specify the type of data the filtration
#                         should be performed on. The options are: "Secretion_Score" OR
#                         "Mitochondria_Score" OR "StartAA".
#   -query QUERY_IDS, --query_ids QUERY_IDS
#                         Here specify the query protein IDs in one of the following formats: single
#                         protein ID OR comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) OR
#                         file with protein IDs separated by newlines When using unencoded query IDs,
#                         a key portion of the protein name is sufficient - The entirety of the
#                         protein header does not need to be used.
#   -prog OG_PROGRAM, --og_program OG_PROGRAM
#                         This argument requires the user to specify the OG program whose data the
#                         filtration should be performed on. The options are: "Br_Grouped_OGs" OR
#                         "Br_Single_OGs" OR "ProteinOrtho_OG" OR "OrthoFinder_OG" OR
#                         "SonicParanoid_OG".
#   -i INPUT_FILE, --input INPUT_FILE
#                         The input file should be the Metamonad database (or filtered version of the
#                         same, including at minimum the columns 'Query', 'Secretion_Score',
#                         'Mitochondria_Score', 'StartAA' and the OG information column of the OG
#                         program desired to be used as input for the search).
#model: 
python assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM [-val THRESHOLD] [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE
#more specific model for use here: 
python ../../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score/Mitochondria_Score -query QUERY_IDS -prog OrthoFinder_OG/SonicParanoid_OG -val THRESHOLD -score 2 -out OUT_NAME -i ../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/assess_OG_startAA_scores.py -cat Mitochondria_Score -query OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt -prog OrthoFinder_OG -val 50 -score 2 -out Tvag_mito_ctrl_OF_OGs_Tv_nonTvP_phy40 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3578044
python ../../../Scripts/assess_OG_startAA_scores.py -cat Mitochondria_Score -query SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt -prog SonicParanoid_OG -val 50 -score 2 -out Tvag_mito_ctrl_SP_OGs_Tv_nonTvP_phy40 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3578176
#metamonada
python ../../../Scripts/assess_OG_startAA_scores.py -cat Mitochondria_Score -query OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt -prog OrthoFinder_OG -val 50 -score 2 -out Metamonad_mito_3_OF_OGs_Tv_nonTvP_phy40 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3578316
python ../../../Scripts/assess_OG_startAA_scores.py -cat Mitochondria_Score -query SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt -prog SonicParanoid_OG -val 50 -score 2 -out Metamonad_mito_3_SP_OGs_Tv_nonTvP_phy40 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3578509
#secretome
#control
python ../../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score -query OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt -prog OrthoFinder_OG -val 50 -score 2 -out Tvag_sec_ctrl_OF_OGs_Tv_nonTvP_phy60 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3579542
python ../../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score -query SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt -prog SonicParanoid_OG -val 50 -score 2 -out Tvag_sec_ctrl_SP_OGs_Tv_nonTvP_phy60 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3579676
#metamonada
python ../../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score -query OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt -prog OrthoFinder_OG -val 50 -score 2 -out Metamonad_sec_3_OF_OGs_Tv_nonTvP_phy60 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3579814
python ../../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score -query SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt -prog SonicParanoid_OG -val 50 -score 2 -out Metamonad_sec_3_SP_OGs_Tv_nonTvP_phy60 -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3579943
wc -l *_OGs.txt
#    9 Metamonad_mito_3_OF_OGs_Tv_nonTvP_phy40_OGs.txt
#   12 Metamonad_mito_3_SP_OGs_Tv_nonTvP_phy40_OGs.txt
#   17 Metamonad_sec_3_OF_OGs_Tv_nonTvP_phy60_OGs.txt
#   33 Metamonad_sec_3_SP_OGs_Tv_nonTvP_phy60_OGs.txt
#    7 Tvag_mito_ctrl_OF_OGs_Tv_nonTvP_phy40_OGs.txt
#   10 Tvag_mito_ctrl_SP_OGs_Tv_nonTvP_phy40_OGs.txt
#   10 Tvag_sec_ctrl_OF_OGs_Tv_nonTvP_phy60_OGs.txt
#   15 Tvag_sec_ctrl_SP_OGs_Tv_nonTvP_phy60_OGs.txt
#  113 total
#... HOO BOY THAT'S BAD
#I think... I might end up just,,, ignoring the scores :/
#finally, check completion of OGs
#more specific model for use here: 
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query QUERY_IDS -prog OrthoFinder_OG/SonicParanoid_OG -val THRESHOLD -out OUT_NAME -i ../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt
#actually applying it: 
#mitochondria/hydrogenosome
#control
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OF_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt -prog OrthoFinder_OG -val 50 -out Tvag_mito_ctrl_OF_OGs_Tv_nonTvP_phy40_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3581526
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query SP_OGs_parsed__OG-Species-PhylaPercent_CtrlMphyla_40.txt -prog SonicParanoid_OG -val 50 -out Tvag_mito_ctrl_SP_OGs_Tv_nonTvP_phy40_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3581655
#metamonada
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OF_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt -prog OrthoFinder_OG -val 50 -out Metamonad_mito_3_OF_OGs_Tv_nonTvP_phy40_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3581792
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query SP_OGs_parsed__OG-Species-PhylaPercent_M3phyla_40.txt -prog SonicParanoid_OG -val 50 -out Metamonad_mito_3_SP_OGs_Tv_nonTvP_phy40_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3581928
#secretome
#control
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OF_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt -prog OrthoFinder_OG -val 50 -out Tvag_sec_ctrl_OF_OGs_Tv_nonTvP_phy60_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3583216
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query SP_OGs_parsed__OG-Species-PhylaPercent_CtrlSphyla_60.txt -prog SonicParanoid_OG -val 50 -out Tvag_sec_ctrl_SP_OGs_Tv_nonTvP_phy60_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3583345
#metamonada
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OF_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt -prog OrthoFinder_OG -val 50 -out Metamonad_sec_3_OF_OGs_Tv_nonTvP_phy60_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3583475
python ../../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query SP_OGs_parsed__OG-Species-PhylaPercent_S3phyla_60.txt -prog SonicParanoid_OG -val 50 -out Metamonad_sec_3_SP_OGs_Tv_nonTvP_phy60_AA -i ../../Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt &
# [1] 3583604
wc -l *AA_OGs.txt
#   291 Metamonad_mito_3_OF_OGs_Tv_nonTvP_phy40_AA_OGs.txt
#   289 Metamonad_mito_3_SP_OGs_Tv_nonTvP_phy40_AA_OGs.txt
#   308 Metamonad_sec_3_OF_OGs_Tv_nonTvP_phy60_AA_OGs.txt
#   342 Metamonad_sec_3_SP_OGs_Tv_nonTvP_phy60_AA_OGs.txt
#   212 Tvag_mito_ctrl_OF_OGs_Tv_nonTvP_phy40_AA_OGs.txt
#   243 Tvag_mito_ctrl_SP_OGs_Tv_nonTvP_phy40_AA_OGs.txt
#   681 Tvag_sec_ctrl_OF_OGs_Tv_nonTvP_phy60_AA_OGs.txt
#   810 Tvag_sec_ctrl_SP_OGs_Tv_nonTvP_phy60_AA_OGs.txt
#  3176 total
cat Metamonad_mito_3_OF_OGs_Tv_nonTvP_phy40_AA_OGs.txt Tvag_mito_ctrl_OF_OGs_Tv_nonTvP_phy40_AA_OGs.txt | sort | uniq | wc -l
# 392
cat Metamonad_mito_3_SP_OGs_Tv_nonTvP_phy40_AA_OGs.txt Tvag_mito_ctrl_SP_OGs_Tv_nonTvP_phy40_AA_OGs.txt | sort | uniq | wc -l
# 435
cat Metamonad_sec_3_OF_OGs_Tv_nonTvP_phy60_AA_OGs.txt Tvag_sec_ctrl_OF_OGs_Tv_nonTvP_phy60_AA_OGs.txt | sort | uniq | wc -l
# 833
cat Metamonad_sec_3_SP_OGs_Tv_nonTvP_phy60_AA_OGs.txt Tvag_sec_ctrl_SP_OGs_Tv_nonTvP_phy60_AA_OGs.txt | sort | uniq | wc -l
# 1002
#as per usual at this point: 
#the mitochondria/hydrogensome is too low, while the secretome is pretty much on target

```

Overall: 
 - More secreted proteins identified both in control data, and via filtration. 
 - The scores of the control proteins were not what I hoped they would be: most of the prediction software I used appaerntly wasn't able to predict secreted or hydrogensomal proteins in _T. vaginalis_. 
 - Any type of filtration tends to reduce the number of mitochondrial/hydrogenosomal OGs to below the target minimum of 500. 
 - The secreted proteins perform better. 
 - But neither pathway seems to have consistently high scores within an OG. 

Will proceed using all OGs from control & initial filtration (score of at least 3), as long as they a) have a protein member present in Trichomonas vaginalis, and b) are not an OG consisting solely of paralogs. 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/FinalFilt_S3M3/ directory
wc -l *_nonTvP.txt
#   376 Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt
#   345 Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt
#   714 Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt
#   669 Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt
#   348 Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt
#   379 Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt
#  1311 Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt
#  1366 Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt
#  5508 total
cat Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 609
cat Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 626
cat Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 1807
cat Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt | sort | uniq | wc -l
# 1823
#creating the necessary files
cat Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt > MetamonadCtrl_mito_3_OF_OGs_Tv_nonTvP.txt
cat Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt > MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt
cat Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt > MetamonadCtrl_sec_3_OF_OGs_Tv_nonTvP.txt
cat Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt > MetamonadCtrl_sec_3_SP_OGs_Tv_nonTvP.txt
wc -l MetamonadCtrl_*
#   724 MetamonadCtrl_mito_3_OF_OGs_Tv_nonTvP.txt
#   724 MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt
#  2025 MetamonadCtrl_sec_3_OF_OGs_Tv_nonTvP.txt
#  2035 MetamonadCtrl_sec_3_SP_OGs_Tv_nonTvP.txt
#  5508 total
#ah wait made the wrong files
#but this is still good control data in one way
#it shows that there is overlap between the control and filtered OG data
cat Metamonad_mito_3_OF_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_OF_OGs_Tv_nonTvP.txt | sort | uniq > MetamonadCtrl_mito_3_OF_OGs_Tv_nonTvP.txt
cat Metamonad_mito_3_SP_OGs_Tv_nonTvP.txt Tvag_mito_ctrl_SP_OGs_Tv_nonTvP.txt | sort | uniq > MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt
cat Metamonad_sec_3_OF_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_OF_OGs_Tv_nonTvP.txt | sort | uniq > MetamonadCtrl_sec_3_OF_OGs_Tv_nonTvP.txt
cat Metamonad_sec_3_SP_OGs_Tv_nonTvP.txt Tvag_sec_ctrl_SP_OGs_Tv_nonTvP.txt | sort | uniq > MetamonadCtrl_sec_3_SP_OGs_Tv_nonTvP.txt
wc -l MetamonadCtrl_*
#   609 MetamonadCtrl_mito_3_OF_OGs_Tv_nonTvP.txt
#   626 MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt
#  1807 MetamonadCtrl_sec_3_OF_OGs_Tv_nonTvP.txt
#  1823 MetamonadCtrl_sec_3_SP_OGs_Tv_nonTvP.txt
#  4865 total
#now copy out these files up 1 directory, into /home/inf-47-2020/ThesisTrich/DB_Construct/
cp MetamonadCtrl_* ..

```

Note from Courtney: Use the start codon of the sequences to gauge whether the scores should even be checked. 


## _A. lanta_ inclusion

The _Anaeramoeba lanta_ genome was added to the analysis during very late stages of the project. I recieved the genome on 16.May.2022. 

In the interest of time (while still desiring to include the proteome), only a very limited analysis was run on _A. lanta_ prior to its inclusion in the Count, Mesquite and ALE analyses. Namely, the proteome was used in re-clustering with OrthoFinder and SonicParanoid, and run through the EggNOG pipeline (with _de novo_ PFam searches). Afterwards, a small version of the Metamonad database was constructed for use in the gene flux and ancestral state reconstruction analyses. The other analysis programs were used, and the full database constructed with _A. lanta_'s inclusion, at a later date, and so all of that additional data was not utilized for filtration. 

Below, the workflow of the creation of the limited Metamonad database including _A. lanta_. 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_ignava/Prelim_DB_ver/temp_dir/ directory
#conglomerating the relevant portions of the EggNOG parsed databases into one lage database
#essentially, only the hits column
cp /home/inf-47-2020/ThesisTrich/PARSED_Results/EggNOG_dn_PFam/*.txt .
nano Header_line.txt
#includes: Query\tpfamEN_hit
#create temp versions of the files without column headers
ls *_PFam.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
#create minimized versions of these files with only two columns
ls *_temp.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	awk -F '\t' '{print $1"\t"$2}' $file > ${file_base}_temp2.txt;
done
#concatenate the files together
cat Header_line.txt *_temp2.txt > Metamonada_AlantaPFam.txt
#now move up the large file and get rid of the rest
mv Metamonada_AlantaPFam.txt ..
cd ..
rm -r temp_dir
#ok
#so now time to create the database
python
```

```python
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_ignava/ directory, using python on the command line
#import necessary modules
import pandas as pd
#import the EggNOG Metamonad database
eggnog_db = "Prelim_DB_ver/Metamonada_AlantaPFam.txt"
eggnog_df = pd.read_csv(eggnog_db, sep = '\t', header=0)
#import the OF & SP clustering data
#and edit Species column name & content for ease of use
orthofinder_db = "OG_Data/OF_OGs_parsed_Alanta.txt"
orthofinder_df = pd.read_csv(orthofinder_db, sep = '\t', header=0)
orthofinder_df["Species"] = orthofinder_df["Species"].str.replace("_edit", "")
orthofinder_df.rename(columns={'Species': 'Species_Id'}, inplace=True)
sonicparanoid_db = "OG_Data/SP_OGs_parsed_Alanta.txt"
sonicparanoid_df = pd.read_csv(sonicparanoid_db, sep = '\t', header=0)
sonicparanoid_df["Species"] = sonicparanoid_df["Species"].str.replace("_edit.fasta", "")
sonicparanoid_df.rename(columns={'Species': 'Species_Id'}, inplace=True)
#merging the OF & SP dataframes
merged_og_df = orthofinder_df.merge(sonicparanoid_df, how='outer', on=['Query', 'Species_Id']).fillna('-')
#quick checks before moving on...
merged_og_df.Species_Id.unique()
# array(['Anaeramoeba_lanta_160522', 'BM_newprots_may21.anaeromoeba',
#        'BS_newprots_may21.anaeromoeba',
#        'Carpediemonas_membranifera.PRJNA719540',
#        'Dientamoeba_fragilis.43352.aa', 'EP00701_Giardia_intestinalis',
#        'EP00703_Trepomonas_sp_PC1', 'EP00708_Paratrimastix_pyriformis',
#        'EP00764_Aduncisulcus_paluster', 'EP00766_Chilomastix_caulleryi',
#        'EP00767_Chilomastix_cuspidata', 'EP00768_Dysnectes_brevis',
#        'EP00769_Ergobibamus_cyprinoides',
#        'EP00770_Monocercomonoides_exilis', 'EP00771_Trimastix_marina',
#        'EP00792_Barthelona_sp_PAP020', 'GiardiaDB_GintestinalisADH',
#        'GiardiaDB_GintestinalisBGS_B', 'GiardiaDB_GintestinalisBGS',
#        'GiardiaDB_GintestinalisEP15', 'Giardia_intestinalis.PRJNA1439',
#        'Giardia_muris.PRJNA524057', 'Histomonas_meleagridis.135588.aa',
#        'Histomonas_meleagridis.PRJNA594289',
#        'Kipferlia_bialata.PRJDB5223', 'Pentatrichomonas_hominis.5728.aa',
#        'SC_newprots_may21.anaeromoeba',
#        'Spironucleus_salmonicida.PRJNA60811',
#        'Tetratrichomonas_gallinarum.5730.aa',
#        'Trichomonas_foetus.PRJNA345179',
#        'Trichomonas_vaginalis_GenBank.PRJNA16084',
#        'Trichomonas_vaginalis_RefSeq.G3'], dtype=object)
len(merged_og_df.Species_Id.unique().tolist())
# 32
#adding in the species designations using the file names as the basis for the designations
merged_full_df = merged_og_df.merge(eggnog_df, how='left', on=['Query']).fillna('-')
# merged_og_df
#                    Query                       Species_Id OrthoFinder_OG SonicParanoid_OG
# 0       19PjcvaOBGOMWk0V         Anaeramoeba_lanta_160522      OG0000000            OG_54
# 1       1pojXveWXkF3RbEs         Anaeramoeba_lanta_160522      OG0000000           OG_715
# 2       3vaqvuwE8w510we6         Anaeramoeba_lanta_160522      OG0000000                -
# 3       4DGzn5UroA6Swm6T         Anaeramoeba_lanta_160522      OG0000000            OG_54
# 4       8NgVzAlKtWBW9nVx         Anaeramoeba_lanta_160522      OG0000000           OG_401
# ...                  ...                              ...            ...              ...
# 483516  fX59GGG0xq6xzNST  Trichomonas_vaginalis_RefSeq.G3              -         OG_64955
# 483517  0ZZnasJYPxisgQQq  Trichomonas_vaginalis_RefSeq.G3              -         OG_65352
# 483518  p1skDVmnFphH9mPw  Trichomonas_vaginalis_RefSeq.G3              -         OG_65533
# 483519  JNv2EPU9fVvxBKlw  Trichomonas_vaginalis_RefSeq.G3              -         OG_65713
# 483520  iyCNfS0TNriXol6D  Trichomonas_vaginalis_RefSeq.G3              -         OG_66483
# [483521 rows x 4 columns]
merged_full_df = merged_og_df.merge(eggnog_df, how='left', on=['Query']).fillna('-')
merged_full_df
#                    Query                       Species_Id  ... SonicParanoid_OG           pfamEN_hit
# 0       19PjcvaOBGOMWk0V         Anaeramoeba_lanta_160522  ...            OG_54                  Ras
# 1       1pojXveWXkF3RbEs         Anaeramoeba_lanta_160522  ...           OG_715                  Ras
# 2       3vaqvuwE8w510we6         Anaeramoeba_lanta_160522  ...                -                  Ras
# 3       4DGzn5UroA6Swm6T         Anaeramoeba_lanta_160522  ...            OG_54                  Ras
# 4       8NgVzAlKtWBW9nVx         Anaeramoeba_lanta_160522  ...           OG_401                  Ras
# ...                  ...                              ...  ...              ...                  ...
# 483516  fX59GGG0xq6xzNST  Trichomonas_vaginalis_RefSeq.G3  ...         OG_64955              DUF3839
# 483517  0ZZnasJYPxisgQQq  Trichomonas_vaginalis_RefSeq.G3  ...         OG_65352                    -
# 483518  p1skDVmnFphH9mPw  Trichomonas_vaginalis_RefSeq.G3  ...         OG_65533                    -
# 483519  JNv2EPU9fVvxBKlw  Trichomonas_vaginalis_RefSeq.G3  ...         OG_65713         Ank_4, Ank_2
# 483520  iyCNfS0TNriXol6D  Trichomonas_vaginalis_RefSeq.G3  ...         OG_66483  Ank_2, Ank_4, Ank_2
# [483521 rows x 5 columns]
#ok, that looks good
#finally, rearrange column order - move PFam IDs to column 3 (Pythonic index 2)
cols = merged_full_df.columns.tolist()
cols
# ['Query', 'Species_Id', 'OrthoFinder_OG', 'SonicParanoid_OG', 'pfamEN_hit']
cols = ['Query', 'Species_Id', 'pfamEN_hit', 'OrthoFinder_OG', 'SonicParanoid_OG']
merged_full_df = merged_full_df[cols]
merged_full_df
#                    Query                       Species_Id  ... OrthoFinder_OG SonicParanoid_OG
# 0       19PjcvaOBGOMWk0V         Anaeramoeba_lanta_160522  ...      OG0000000            OG_54
# 1       1pojXveWXkF3RbEs         Anaeramoeba_lanta_160522  ...      OG0000000           OG_715
# 2       3vaqvuwE8w510we6         Anaeramoeba_lanta_160522  ...      OG0000000                -
# 3       4DGzn5UroA6Swm6T         Anaeramoeba_lanta_160522  ...      OG0000000            OG_54
# 4       8NgVzAlKtWBW9nVx         Anaeramoeba_lanta_160522  ...      OG0000000           OG_401
# ...                  ...                              ...  ...            ...              ...
# 483516  fX59GGG0xq6xzNST  Trichomonas_vaginalis_RefSeq.G3  ...              -         OG_64955
# 483517  0ZZnasJYPxisgQQq  Trichomonas_vaginalis_RefSeq.G3  ...              -         OG_65352
# 483518  p1skDVmnFphH9mPw  Trichomonas_vaginalis_RefSeq.G3  ...              -         OG_65533
# 483519  JNv2EPU9fVvxBKlw  Trichomonas_vaginalis_RefSeq.G3  ...              -         OG_65713
# 483520  iyCNfS0TNriXol6D  Trichomonas_vaginalis_RefSeq.G3  ...              -         OG_66483
# [483521 rows x 5 columns]
merged_full_df.columns
# Index(['Query', 'Species_Id', 'pfamEN_hit', 'OrthoFinder_OG',
#        'SonicParanoid_OG'],
#       dtype='object')
merged_full_df.pfamEN_hit
# 0                         Ras
# 1                         Ras
# 2                         Ras
# 3                         Ras
# 4                         Ras
#                  ...
# 483516                DUF3839
# 483517                      -
# 483518                      -
# 483519           Ank_4, Ank_2
# 483520    Ank_2, Ank_4, Ank_2
# Name: pfamEN_hit, Length: 483521, dtype: object
#ok, looks good to go
#writing out results to a tab-separated text file
merged_full_df.to_csv("Metamonada_PFamAignava.txt", sep = '\t', index=False)
#ok, we're done here
exit()
```

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/FinalFilt_S3M3/ directory
#extracting all of the protein IDs predicted to be secreted 
#(control or filtration from original large Metamonad database)
awk -F '\t' '(NR>1) {print $1}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Mitochondria3.txt > EncodedProts_Mito3.txt
awk -F '\t' '(NR>1) {print $1}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__filt-Secretome3.txt > EncodedProts_Sec3.txt
awk -F '\t' '(NR>1) {print $1}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt > EncodedProts_MitoCtrl.txt
awk -F '\t' '(NR>1) {print $1}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt > EncodedProts_SecCtrl.txt
cat EncodedProts_Mito3.txt EncodedProts_MitoCtrl.txt > EncodedProts_Mito_Original.txt
cat EncodedProts_Sec3.txt EncodedProts_SecCtrl.txt > EncodedProts_Sec_Original.txt
#now moving these files over to the /home/inf-47-2020/ThesisTrich/DB2_A_ignava/FinalFilt_S3M3/ working directory
mv ../../DB_Construct/FinalFilt_S3M3/EncodedProts_* .
#now we can move on
#don't have to check whether all proteins have OGs - the ones that don't will be filtered out
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_ignava/ directory
wc -l Metamonada_PFamAignava.txt
# 483522 Metamonada_PFamAignava.txt
#for reference vs. the full dataframe
#it makes sense that it's smaller, since not all proteins will cluster
#but it's ok, since for this analysis, we're focusing on proteins that are in orthologous clusters
#first, building the reference files
#working in the /home/inf-47-2020/ThesisTrich/DataFiles/ProtLists/Prot_Species/
awk 'sub(/^>/, "")' ../../EncodedData/Anaeramoeba_lanta_160522_edit.fasta > Anaeramoeba_lanta_160522_edit_prots.txt
#this created a file with the the protein headers (all in new lines)
python ../add_species.py Anaeramoeba_lanta_160522_edit_prots.txt
#and now we can just cat the files together
cat *_species.txt > species_prots_ref_Aignava.txt
#and now the OG reference files
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_ignava/OG_Data/ directory
ln -s /home/inf-47-2020/ThesisTrich/DataFiles/ProtLists/Prot_Species/species_prots_ref_Aignava.txt .
#python ../../Scripts/categorize_prot_species.py species_prots_ref_Aignava.txt Prots_Species_Phyla_Aignava_DB.txt
#forgot to change the script to include A. lanta
python ../../Scripts/categorize_prot_species__v2.py species_prots_ref_Aignava.txt Prots_Species_Phyla_Aignava_DB.txt
python ../../Scripts/og_db_plusSpeciesRep__v2.py OF_OGs_parsed_Alanta.txt Prots_Species_Phyla_Aignava_DB.txt
python ../../Scripts/og_db_plusSpeciesRep__v2.py SP_OGs_parsed_Alanta.txt Prots_Species_Phyla_Aignava_DB.txt
#and now, extracting all OG IDs associated with the protein IDs above, from the new small database
#first, create filtered versions of the Metamonad database
python ../../Scripts/extract_prot_db.py ../Metamonada_PFamAignava.txt EncodedProts_Mito_Original.txt Metamonada_PFamAignava__Mito3Filt.txt
python ../../Scripts/extract_prot_db.py ../Metamonada_PFamAignava.txt EncodedProts_Sec_Original.txt Metamonada_PFamAignava__Sec3Filt.txt
#then extract the unique OG IDs from the appropriate columns
head -1 Metamonada_PFamAignava.txt
# Query   Species_Id      pfamEN_hit      OrthoFinder_OG  SonicParanoid_OG
awk -F '\t' '(NR>1) {print $4}' Metamonada_PFamAignava__Mito3Filt.txt | sort | uniq > Alanta_mito_3_OF_OGs.txt
awk -F '\t' '(NR>1) {print $5}' Metamonada_PFamAignava__Mito3Filt.txt | sort | uniq > Alanta_mito_3_SP_OGs.txt
awk -F '\t' '(NR>1) {print $4}' Metamonada_PFamAignava__Sec3Filt.txt | sort | uniq > Alanta_sec_3_OF_OGs.txt
awk -F '\t' '(NR>1) {print $5}' Metamonada_PFamAignava__Sec3Filt.txt | sort | uniq > Alanta_sec_3_SP_OGs.txt
#Filter these first, to include only OGs present in T. vaginalis
#mitochondria/hydrogenosome
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Alanta_mito_3_OF_OGs.txt -suf filtM3 -i ../OG_Data/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Alanta_mito_3_SP_OGs.txt -suf filtM3 -i ../OG_Data/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
#secretome
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Alanta_sec_3_OF_OGs.txt -suf filtS3 -i ../OG_Data/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type include -cat species -single Trichomonas_vaginalis -query Alanta_sec_3_SP_OGs.txt -suf filtS3 -i ../OG_Data/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
#checking results
wc -l *.txt
#    1665 Alanta_mito_3_OF_OGs.txt
#    1431 Alanta_mito_3_SP_OGs.txt
#    5291 Alanta_sec_3_OF_OGs.txt
#    3878 Alanta_sec_3_SP_OGs.txt
#    4434 EncodedProts_Mito_Original.txt
#   15633 EncodedProts_Sec_Original.txt
#    3547 Metamonada_PFamAignava__Mito3Filt.txt
#   13816 Metamonada_PFamAignava__Sec3Filt.txt
#     807 OF_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtM3.txt
#    2364 OF_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtS3.txt
#     813 SP_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtM3.txt
#    2324 SP_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtS3.txt
#   56003 total
#first, let's change the file names, because these are going to get LONG
#move these to a new subdirectory then copy back with better name
cp OGs_Parsed_FiltVers/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtM3.txt Alanta_mito_OF_OGs_Tv.txt
cp OGs_Parsed_FiltVers/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtS3.txt Alanta_sec_OF_OGs_Tv.txt
cp OGs_Parsed_FiltVers/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtM3.txt Alanta_mito_SP_OGs_Tv.txt
cp OGs_Parsed_FiltVers/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent_incl_filtS3.txt Alanta_sec_SP_OGs_Tv.txt
#so now let's filter out OGs unique to T. vaginalis
#mitochondria/hydrogenosome
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Alanta_mito_OF_OGs_Tv.txt -suf M3nonTvP -i ../OG_Data/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Alanta_mito_SP_OGs_Tv.txt -suf M3nonTvP -i ../OG_Data/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
#secretome
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Alanta_sec_OF_OGs_Tv.txt -suf S3nonTvP -i ../OG_Data/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
python ../../Scripts/filter_OG_speciesDB__v2.py -type threshold -cat species -val 6 -query Alanta_sec_SP_OGs_Tv.txt -suf S3nonTvP -i ../OG_Data/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent.txt
#and now let's rename these
cp OGs_Parsed_FiltVers/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent_M3nonTvP_6.txt Alanta_mito_OF_OGs_Tv_nTvP.txt
cp OGs_Parsed_FiltVers/OF_OGs_parsed_Alanta__OG-Species-PhylaPercent_S3nonTvP_6.txt Alanta_sec_OF_OGs_Tv_nTvP.txt
cp OGs_Parsed_FiltVers/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent_M3nonTvP_6.txt Alanta_mito_SP_OGs_Tv_nTvP.txt
cp OGs_Parsed_FiltVers/SP_OGs_parsed_Alanta__OG-Species-PhylaPercent_S3nonTvP_6.txt Alanta_sec_SP_OGs_Tv_nTvP.txt
#checking in
wc -l Alanta_*.txt
#   1665 Alanta_mito_3_OF_OGs.txt
#   1431 Alanta_mito_3_SP_OGs.txt
#    610 Alanta_mito_OF_OGs_Tv_nTvP.txt
#    807 Alanta_mito_OF_OGs_Tv.txt
#    628 Alanta_mito_SP_OGs_Tv_nTvP.txt
#    813 Alanta_mito_SP_OGs_Tv.txt
#   5291 Alanta_sec_3_OF_OGs.txt
#   3878 Alanta_sec_3_SP_OGs.txt
#   1805 Alanta_sec_OF_OGs_Tv_nTvP.txt
#   2364 Alanta_sec_OF_OGs_Tv.txt
#   1821 Alanta_sec_SP_OGs_Tv_nTvP.txt
#   2324 Alanta_sec_SP_OGs_Tv.txt
#  23437 total
#now create the PFam tables for the OGs
python ../Scripts/og2PFam_pivot__v2.py Metamonada_PFamAignava.txt OrthoFinder_OG
python ../Scripts/og2PFam_pivot__v2.py Metamonada_PFamAignava.txt SonicParanoid_OG
#and not FINALLY, we can move on to Count

```

## Count

### Preparation

Create table with data in format: OG\tQuery\tSpecies_Id (script saved to og_prot_spp_list.py)
 - Use Metamonad database as input

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title:og_prot_spp_list.py
Date: 2022-05-09
Author: Virág Varga

Description:
	This program filters any version of the large Metamonad database that includes
		query IDs, species category designation, and OG ID information from the
		oerthologous clustering program of interest.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Importing modules and assigning command-line arguments.
	2. Importing input data into Pandas dataframe.
	3. Filtering the metamonad database to only include relevant OG ID, Query
		protein ID and species ID desigantions, before writing out results to
		a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but instead based on the input
		OG ID column name.

Usage
	./og_prot_spp_list.py input_db og_col
	OR
	python og_prot_spp_list.py input_db og_col

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import modules and assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line argument; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__filt_scores-startAA-pfam__HEAD200.txt"
og_col = sys.argv[2]
#og_col = "SonicParanoid_OG"
output_db = og_col + '__Prot_Spp.txt'


#Part 2: Import data into Pandas dataframes

#import Metamonad database into a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.


#Part 3: Extract relevant data & write out results

filt_input_df = input_df[[og_col, 'Query', 'Species_Id']].copy()
#copy the relevant columns to a new dataframe

og_df = filt_input_df[filt_input_df[og_col] != "-"].copy()
#remove protein queries without an OG ID from the datafram
og_df.sort_values(by = og_col, inplace=True)
#order the rows by OG ID to make the dataframe look nicer


#write out the resulting dataframe
og_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python og_prot_spp_list.py input_db og_col
#applying it: 
python ../Scripts/og_prot_spp_list.py Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt OrthoFinder_OG
python ../Scripts/og_prot_spp_list.py Metamonada_pred_OG_DB__filt_scores-startAA-pfam.txt SonicParanoid_OG
###
#A. lanta filtration
python ../Scripts/og_prot_spp_list.py Metamonada_PFamAignava.txt OrthoFinder_OG
python ../Scripts/og_prot_spp_list.py Metamonada_PFamAignava.txt SonicParanoid_OG

```

Filter the above database to only include the query OGs (script saved to filter_OG_profile.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: filter_OG_profile.py
Date: 2022-05-09
Author: Virág Varga

Description:
	This program filters a dataframe containing OG and species ID information for
		each query protein ID down, in order to only include proteins belonging to
		OGs from a query OG ID list provided by the user.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Importing modules and assigning command-line arguments.
	2. Importing input data into Pandas dataframe.
	3. Filtering the dataframe to only include the rows associated with protein
		query IDs that are in the list of query OG IDs, before writing out results
		to a tab-separated text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- All input and output file names are user-defined.

Usage
	./filter_OG_profile.py input_db og_ids output_db
	OR
	python filter_OG_profile.py input_db og_ids output_db

	Where input_db must be a file in the format:
		OG_ID\tQuery\tSpecies_Id (as produced by og_prot_spp_list.py)
	Where og_ids must be a query OG ID list in on of the following formats:
		- Singular OG ID provided on the command line
		- Comma-separated list of OG IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of OG IDs in format: ID1\nID2 etc.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import modules and assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python

#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "SonicParanoid_OG__Prot_Spp.txt"

og_ids = sys.argv[2]
#og_ids = "OG_1,OG_10,OG_100"
#og_ids = "MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt"
#import the query list
if os.path.isfile(og_ids):
	#if the input selection of OGs is a file
	with open(og_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
		#eliminate duplicates
		query_list = list(set(query_list))
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = og_ids.split(",")
	#eliminate duplicates
	query_list = list(set(query_list))


output_db = sys.argv[3]
#output_db = "MetamonadCtrl_mito_3_SP_OGProtSpp.txt"


#Part 2: Import data into Pandas dataframes

#import Metamonad database into a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0)
og_col = input_df.columns[0]
#extract the name of the OG ID columns


#Part 3: Extract relevant data & write out results

filt_input_df = input_df[input_df[og_col].isin(query_list)].copy()
#filter the large OG dataframe to only include proteins from the query OGs


#write out the resulting dataframe
filt_input_df.to_csv(output_db, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python filter_OG_profile.py input_db og_ids output_db
#applying it: 
python ../Scripts/filter_OG_profile.py OrthoFinder_OG__Prot_Spp.txt MetamonadCtrl_mito_3_OF_OGs_Tv_nonTvP.txt MetamonadCtrl_mito_3_OF_OGProtSpp.txt
python ../Scripts/filter_OG_profile.py OrthoFinder_OG__Prot_Spp.txt MetamonadCtrl_sec_3_OF_OGs_Tv_nonTvP.txt MetamonadCtrl_sec_3_OF_OGProtSpp.txt
python ../Scripts/filter_OG_profile.py SonicParanoid_OG__Prot_Spp.txt MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt MetamonadCtrl_mito_3_SP_OGProtSpp.txt
python ../Scripts/filter_OG_profile.py SonicParanoid_OG__Prot_Spp.txt MetamonadCtrl_sec_3_SP_OGs_Tv_nonTvP.txt MetamonadCtrl_sec_3_SP_OGProtSpp.txt
###
#A. lanta filtration
python ../../Scripts/filter_OG_profile.py ../OrthoFinder_OG__Prot_Spp.txt Alanta_mito_OF_OGs_Tv_nTvP.txt Alanta_mito_3_OF_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../OrthoFinder_OG__Prot_Spp.txt Alanta_sec_OF_OGs_Tv_nTvP.txt Alanta_sec_3_OF_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../SonicParanoid_OG__Prot_Spp.txt Alanta_mito_SP_OGs_Tv_nTvP.txt Alanta_mito_3_SP_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../SonicParanoid_OG__Prot_Spp.txt Alanta_sec_SP_OGs_Tv_nTvP.txt Alanta_sec_3_SP_OGProtSpp.txt
#going back without excluding the OGs unique to T. vaginalis
python ../../Scripts/filter_OG_profile.py ../OrthoFinder_OG__Prot_Spp.txt Alanta_mito_OF_OGs_Tv.txt Alanta_mito_3_OF_Tv_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../OrthoFinder_OG__Prot_Spp.txt Alanta_sec_OF_OGs_Tv.txt Alanta_sec_3_OF_Tv_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../SonicParanoid_OG__Prot_Spp.txt Alanta_mito_SP_OGs_Tv.txt Alanta_mito_3_SP_Tv_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../SonicParanoid_OG__Prot_Spp.txt Alanta_sec_SP_OGs_Tv.txt Alanta_sec_3_SP_Tv_OGProtSpp.txt
#ignoring all filtrations
python ../../Scripts/filter_OG_profile.py ../OrthoFinder_OG__Prot_Spp.txt Alanta_mito_3_OF_OGs.txt Alanta_mito_3_OF_ALL_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../OrthoFinder_OG__Prot_Spp.txt Alanta_sec_3_OF_OGs.txt Alanta_sec_3_OF_ALL_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../SonicParanoid_OG__Prot_Spp.txt Alanta_mito_3_SP_OGs.txt Alanta_mito_3_SP_ALL_OGProtSpp.txt
python ../../Scripts/filter_OG_profile.py ../SonicParanoid_OG__Prot_Spp.txt Alanta_sec_3_SP_OGs.txt Alanta_sec_3_SP_ALL_OGProtSpp.txt

```

Create table with data in the format: OG\tPFams\t[protein counts per species per OG] (script saved to create_counts_table.py): 
 - The script should be primarily based on the pivotCounts.py script, as well as possibly somewhat on the annotateCounts.py script
 - Reminder: OG to PFam profile databases already exist for each program (produced by the script og2PFam_pivot__v2.py) 


```python 
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: create_counts_table.py
Date: 2022.05.09
Author: Virág Varga

Description:
	This program creates a pivot table of counts of proteins present in an OG per
		per species, as well as the PFam annotations associated with those OGs, on
		the basis of a file in the format [OG_ID]\tQuery\tSpecies_Id (created by either
		the og_prot_spp_list.py or filter_OG_profile.py scripts) and a file in the
		format [OG_ID]\t[PFam_Data] (created by the og2PFam_pivot__v2.py script).

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Importing the data into Pandas dataframes.
	3. Pivot the data table so that eeach row contains one OG ID, with information
		on the number of proteins present in that OG in each species in columns with
		the species IDs as the headers.
	4. Adding the PFam annotation information onto the end of the dataframe.
	5. Writing out the results to a tab-delimited text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a file in the format [OG_ID]\tQuery\tSpecies_Id
		(created by either the og_prot_spp_list.py or filter_OG_profile.py scripts) and a
		file in the format [OG_ID]\t[PFam_Data] (created by the og2PFam_pivot__v2.py script).

Version:
	This program can be considered a Version 2.0 of the pivotCounts.py script, with some
		aspects of the annotateCounts.py scriptincluded. This script integrates integrates
		the functionality of those earlier scripts, and accomadates the greater diversity
		of input data being used.

Usage
	./create_counts_table.py input_db input_pfams output_db
	OR
	python create_counts_table.py input_db input_pfams output_db

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allow execution of code from the command line
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "MetamonadCtrl_mito_3_SP_OGProtSpp.txt"
input_pfams = sys.argv[2]
#input_pfams = "OGs2PFams_SonicParanoid_OG_Counts.txt"
output_db = sys.argv[3]
#output_db = "MetamonadCtrl_mito_3_SP__CountPivot.txt"


#Part 2: Import the data into Pandas dataframes

input_df = pd.read_csv(input_db, sep = '\t', header=0)
#read in the filtered OG_ID\tQuery\tSpecies_Id dataframe
og_col = input_df.columns[0]
#extract the name of the OG ID columns

pfam_df = pd.read_csv(input_pfams, sep = '\t', header=0)
#creating a dataframe for the OG_ID\tPFam


#Part 3: Creating the pivot table of OG, protein & species ID data

#create the pivot table
count_df = pd.pivot_table(data=input_df, index=og_col, values= 'Query', columns='Species_Id',
						  aggfunc='count', fill_value=0)
#the Orthogroup data is used as the index, the data in the Query column is used to fill the table
#and the Species_Id categories are used as headers
#`aggfunc='count'` fills the columns with counts of unique appearances (vs. the actual protein IDs themselves)
#`fill_value=0` fills empty cells with 0 (zero) instead of NaN (don't have to do it manually later)
count_df.reset_index(inplace=True)
#reset the headers to move the OG IDs out of the index


#Part 4: Adding the PFam information into the dataframe

annot_df = count_df.merge(pfam_df, on=og_col, how='left')
#want to do left merge, because the PFam reference dataframe has information on all OGs
#but we only need the information for the filtered OGs in the pivot table
#this adds the annotation column to the rightmost end, which is where it needs to be


#Part 5: Writing out the results

annot_df.to_csv(output_db, sep='\t', index=False)
#the Count program needs the input data table to be in tab-delimited formatting

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python create_counts_table.py input_db input_pfams output_db
#applying it: 
python ../Scripts/create_counts_table.py MetamonadCtrl_mito_3_SP_OGProtSpp.txt OGs2PFams_SonicParanoid_OG_Counts.txt MetamonadCtrl_mito_3_SP__CountPivot.txt
python ../Scripts/create_counts_table.py MetamonadCtrl_sec_3_SP_OGProtSpp.txt OGs2PFams_SonicParanoid_OG_Counts.txt MetamonadCtrl_sec_3_SP__CountPivot.txt
python ../Scripts/create_counts_table.py MetamonadCtrl_mito_3_OF_OGProtSpp.txt OGs2PFams_OrthoFinder_OG_Counts.txt MetamonadCtrl_mito_3_OF__CountPivot.txt
python ../Scripts/create_counts_table.py MetamonadCtrl_sec_3_OF_OGProtSpp.txt OGs2PFams_OrthoFinder_OG_Counts.txt MetamonadCtrl_sec_3_OF__CountPivot.txt
###
#A. lanta filtration
python ../../Scripts/create_counts_table.py Alanta_mito_3_SP_OGProtSpp.txt ../OGs2PFams_SonicParanoid_OG_Counts.txt Alanta_mito_3_SP__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_sec_3_SP_OGProtSpp.txt ../OGs2PFams_SonicParanoid_OG_Counts.txt Alanta_sec_3_SP__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_mito_3_OF_OGProtSpp.txt ../OGs2PFams_OrthoFinder_OG_Counts.txt Alanta_mito_3_OF__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_sec_3_OF_OGProtSpp.txt ../OGs2PFams_OrthoFinder_OG_Counts.txt Alanta_sec_3_OF__CountPivot.txt
#for the sake of time, manually editing the name of Tritrichomonas foetus
#I think the fix is somewhere in the pipeline, but clearly,,, I am not following the entire pipeline :/
#going back without excluding the OGs unique to T. vaginalis
python ../../Scripts/create_counts_table.py Alanta_mito_3_SP_Tv_OGProtSpp.txt ../OGs2PFams_SonicParanoid_OG_Counts.txt Alanta_mito_3_SP_Tv__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_sec_3_SP_Tv_OGProtSpp.txt ../OGs2PFams_SonicParanoid_OG_Counts.txt Alanta_sec_3_SP_Tv__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_mito_3_OF_Tv_OGProtSpp.txt ../OGs2PFams_OrthoFinder_OG_Counts.txt Alanta_mito_3_OF_Tv__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_sec_3_OF_Tv_OGProtSpp.txt ../OGs2PFams_OrthoFinder_OG_Counts.txt Alanta_sec_3_OF_Tv__CountPivot.txt
#ignoring all filtrations
python ../../Scripts/create_counts_table.py Alanta_mito_3_SP_ALL_OGProtSpp.txt ../OGs2PFams_SonicParanoid_OG_Counts.txt Alanta_mito_3_SP_ALL__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_sec_3_SP_ALL_OGProtSpp.txt ../OGs2PFams_SonicParanoid_OG_Counts.txt Alanta_sec_3_SP_ALL__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_mito_3_OF_ALL_OGProtSpp.txt ../OGs2PFams_OrthoFinder_OG_Counts.txt Alanta_mito_3_OF_ALL__CountPivot.txt
python ../../Scripts/create_counts_table.py Alanta_sec_3_OF_ALL_OGProtSpp.txt ../OGs2PFams_OrthoFinder_OG_Counts.txt Alanta_sec_3_OF_ALL__CountPivot.txt

```

The Newick tree that will be used for the Count analysis will be the one produced by OrthoFinder. SonicParanoid does not output a tree, so the SonicParanoid and OrthoFinder OGs cannot use separate trees created by the two programs. 

However, the Newick tree does need to be edited in a few ways: 
 - The species names have to match the ones used in the data table used as input for Count
 - The species names need to be in quotes
 - Fixing the naming of _Tritrichomonas foetus_ (file name incorrectly uses _Trichomonas foetus_)


### _A. lanta_ integration

Creation of the Newick tree from the OrthoFinder version (file name: SpeciesTree_rooted_at_outgroup_22.txt [nodes 9 & 21 as backup]) to the final printout (file name: SpeciesTree_rooted_at_outgroup_22_NameEdit.nwk): 

![Species tree rooted at outgroup 22](Count/SpeciesTree_rooted_at_outgroup_22.png)

```text
(((Anaeramoeba_lanta_160522_edit:0.450921,(BM_newprots_may21.anaeromoeba_edit:0.335199,(SC_newprots_may21.anaeromoeba_edit:0.00659918,BS_newprots_may21.anaeromoeba_edit:9.97265e-06)N12:0.324153)N7:0.0752971)N3:0.085195,(((Tetratrichomonas_gallinarum.5730.aa_edit:0.0016019,Pentatrichomonas_hominis.5728.aa_edit:0.0345883)N13:0.134371,(Trichomonas_vaginalis_GenBank.PRJNA16084_edit:9.97265e-06,Trichomonas_vaginalis_RefSeq.G3_edit:9.97265e-06)N14:0.174424)N8:0.0916459,((Dientamoeba_fragilis.43352.aa_edit:0.159906,(Histomonas_meleagridis.PRJNA594289_edit:0.0108462,Histomonas_meleagridis.135588.aa_edit:0.0901299)N18:0.120525)N15:0.0690226,Trichomonas_foetus.PRJNA345179_edit:0.17672)N9:0.0769582)N4:0.279032)N1:0.0329942,(((EP00708_Paratrimastix_pyriformis_edit:0.302018,EP00771_Trimastix_marina_edit:0.370245)N10:0.0929272,EP00770_Monocercomonoides_exilis_edit:0.432518)N5:0.0989893,(EP00792_Barthelona_sp_PAP020_edit:0.499294,((EP00769_Ergobibamus_cyprinoides_edit:0.490135,Carpediemonas_membranifera.PRJNA719540_edit:0.461779)N16:0.0460798,(EP00764_Aduncisulcus_paluster_edit:0.4689,((EP00766_Chilomastix_caulleryi_edit:0.576054,EP00767_Chilomastix_cuspidata_edit:0.427461)N20:0.143484,(Kipferlia_bialata.PRJDB5223_edit:0.393386,(EP00768_Dysnectes_brevis_edit:0.360694,((Spironucleus_salmonicida.PRJNA60811_edit:0.357159,EP00703_Trepomonas_sp_PC1_edit:0.340399)N24:0.154188,(Giardia_muris.PRJNA524057_edit:0.250193,((GiardiaDB_GintestinalisEP15_edit:0.0161814,(GiardiaDB_GintestinalisADH_edit:0.0045674,(EP00701_Giardia_intestinalis_edit:9.97265e-06,Giardia_intestinalis.PRJNA1439_edit:9.97265e-06)N30:0.00343865)N29:0.016551)N27:0.0203631,(GiardiaDB_GintestinalisBGS_edit:0.00134291,GiardiaDB_GintestinalisBGS_B_edit:0.00664544)N28:0.0390068)N26:0.204119)N25:0.212948)N23:0.13267)N22:0.0832471)N21:0.0437682)N19:0.0502614)N17:0.0394533)N11:0.0430432)N6:0.0327792)N2:0.0329942)N0;
```

```text
((("Anaeramoeba_lanta_160522":0.450921,("BM_newprots_may21.anaeromoeba":0.335199,("SC_newprots_may21.anaeromoeba":0.00659918,"BS_newprots_may21.anaeromoeba":9.97265e-06)N12:0.324153)N7:0.0752971)N3:0.085195,((("Tetratrichomonas_gallinarum.5730.aa":0.0016019,"Pentatrichomonas_hominis.5728.aa":0.0345883)N13:0.134371,("Trichomonas_vaginalis_GenBank.PRJNA16084":9.97265e-06,"Trichomonas_vaginalis_RefSeq.G3":9.97265e-06)N14:0.174424)N8:0.0916459,(("Dientamoeba_fragilis.43352.aa":0.159906,("Histomonas_meleagridis.PRJNA594289":0.0108462,"Histomonas_meleagridis.135588.aa":0.0901299)N18:0.120525)N15:0.0690226,"Tritrichomonas_foetus.PRJNA345179":0.17672)N9:0.0769582)N4:0.279032)N1:0.0329942,((("EP00708_Paratrimastix_pyriformis":0.302018,"EP00771_Trimastix_marina":0.370245)N10:0.0929272,"EP00770_Monocercomonoides_exilis":0.432518)N5:0.0989893,("EP00792_Barthelona_sp_PAP020":0.499294,(("EP00769_Ergobibamus_cyprinoides":0.490135,"Carpediemonas_membranifera.PRJNA719540":0.461779)N16:0.0460798,("EP00764_Aduncisulcus_paluster":0.4689,(("EP00766_Chilomastix_caulleryi":0.576054,"EP00767_Chilomastix_cuspidata":0.427461)N20:0.143484,("Kipferlia_bialata.PRJDB5223":0.393386,("EP00768_Dysnectes_brevis":0.360694,(("Spironucleus_salmonicida.PRJNA60811":0.357159,"EP00703_Trepomonas_sp_PC1":0.340399)N24:0.154188,("Giardia_muris.PRJNA524057":0.250193,(("GiardiaDB_GintestinalisEP15":0.0161814,("GiardiaDB_GintestinalisADH":0.0045674,("EP00701_Giardia_intestinalis":9.97265e-06,"Giardia_intestinalis.PRJNA1439":9.97265e-06)N30:0.00343865)N29:0.016551)N27:0.0203631,("GiardiaDB_GintestinalisBGS":0.00134291,"GiardiaDB_GintestinalisBGS_B":0.00664544)N28:0.0390068)N26:0.204119)N25:0.212948)N23:0.13267)N22:0.0832471)N21:0.0437682)N19:0.0502614)N17:0.0394533)N11:0.0430432)N6:0.0327792)N2:0.0329942)N0;
```

Proceed with the statistics from the Dollo Parsimony: 

### Mitochondria 3 OF

Summary statistics (Dollo Parsimony): 
 - Number of OGs at the Metamonad common ancestor (Node #31): 482
 - Parabasalid/Anaeramoebid common ancestor (Node #11): 
   - Number of OGs: 497
   - Gain: 15
   - Loss: 0
 - Parabasalia (Node #10): 
   - Number of OGs: 641
   - Gain: 191
   - Loss: 47
 - Anaeramoebidae (Node #3): 
   - Number of OGs: 426
   - Gain: 4
   - Loss: 75
 - Preaxostyla (Node #13): 
   - Number of OGs: 415
   - Gain: 2
   - Loss: 92
 - Fornicata (Node #28): 
   - Number of OGs: 510
   - Gain: 23
   - Loss: 9
 - _Trichomonas vaginalis_ (Node #5): 
   - Number of OGs: 799
   - Gain: 189
   - Loss: 70
 - _Giardia_ spp. (Node #22): 
   - Number of OGs: 344
   - Gain: 32
   - Loss: 53

![Dollo Parsimony tree of OG gain/loss (non-Fornicata)](Count/Alanta_ALL/Alanta_mito_3_OF__CountPivot__Dollo_nonFornicates.png)


### Mitochondria 3 SP

Summary statistics (Dollo Parsimony): 
 - Number of OGs at the Metamonad common ancestor (Node #31): 500
 - Parabasalid/Anaeramoebid common ancestor (Node #11): 
   - Number of OGs: 515
   - Gain: 15
   - Loss: 0
 - Parabasalia (Node #10): 
   - Number of OGs: 671
   - Gain: 189
   - Loss: 33
 - Anaeramoebidae (Node #3): 
   - Number of OGs: 450
   - Gain: 3
   - Loss: 68
 - Preaxostyla (Node #13): 
   - Number of OGs: 426
   - Gain: 2
   - Loss: 90
 - Fornicata (Node #28): 
   - Number of OGs: 503
   - Gain: 11
   - Loss: 6
 - _Trichomonas vaginalis_ (Node #5): 
   - Number of OGs: 813
   - Gain: 185
   - Loss: 75
 - _Giardia_ spp. (Node #22): 
   - Number of OGs: 340
   - Gain: 33
   - Loss: 65

![Dollo Parsimony tree of OG gain/loss (non-Fornicata)](Count/Alanta_ALL/Alanta_mito_3_SP__CountPivot__Dollo_nonFornicates.png)


### Secretome 3 OF

Summary statistics (Dollo Parsimony): 
 - Number of OGs at the Metamonad common ancestor (Node #31): 1229
 - Parabasalid/Anaeramoebid common ancestor (Node #11): 
   - Number of OGs: 1286
   - Gain: 57
   - Loss: 0
 - Parabasalia (Node #10): 
   - Number of OGs: 1799
   - Gain: 662
   - Loss: 149
 - Anaeramoebidae (Node #3): 
   - Number of OGs: 1032
   - Gain: 37
   - Loss: 291
 - Preaxostyla (Node #13): 
   - Number of OGs: 1091
   - Gain: 18
   - Loss: 228
 - Fornicata (Node #28): 
   - Number of OGs: 1278
   - Gain: 52
   - Loss: 39
 - _Trichomonas vaginalis_ (Node #5): 
   - Number of OGs: 2363
   - Gain: 558
   - Loss: 189
 - _Giardia_ spp. (Node #22): 
   - Number of OGs: 707
   - Gain: 39
   - Loss: 133

![Dollo Parsimony tree of OG gain/loss (non-Fornicata)](Count/Alanta_ALL/Alanta_sec_3_OF__CountPivot__Dollo_nonFornicates.png)


### Secretome 3 SP

Summary statistics (Dollo Parsimony): 
 - Number of OGs at the Metamonad common ancestor (Node #31): 1304
 - Parabasalid/Anaeramoebid common ancestor (Node #11): 
   - Number of OGs: 1350
   - Gain: 46
   - Loss: 0
 - Parabasalia (Node #10): 
   - Number of OGs: 1806
   - Gain: 563
   - Loss: 107
 - Anaeramoebidae (Node #3): 
   - Number of OGs: 1140
   - Gain: 36
   - Loss: 246
 - Preaxostyla (Node #13): 
   - Number of OGs: 1143
   - Gain: 14
   - Loss: 226
 - Fornicata (Node #28): 
   - Number of OGs: 1330
   - Gain: 21
   - Loss: 40
 - _Trichomonas vaginalis_ (Node #5): 
   - Number of OGs: 2324
   - Gain: 503
   - Loss: 157
 - _Giardia_ spp. (Node #22): 
   - Number of OGs: 777
   - Gain: 35
   - Loss: 134

![Dollo Parsimony tree of OG gain/loss (non-Fornicata)](Count/Alanta_ALL/Alanta_sec_3_SP__CountPivot__Dollo_nonFornicates.png)


### Dollo Parsimony Filtration

Script to filter results of Dollo parsimony analysis, in order to extract OGs present at a given node (script saved to subfilter_Dollo.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: subfilter_Dollo.py
Date: 2022.05.12
Author: Virág Varga

Description:
	This program uses data from the Count program to filter the input file used for
		Count down to the OGs which appear at a given query node.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas import the contents of the Count annotated input data file and
		Count Dollo parsimony data output file.
	3. Filtering out the OGs unique to the query node, before writing out the
		results to a tab-delimited text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of an OG database prepped as input into Count,
		along with an output file from Count containing Dollo Parsimony data.
	- The output file name is only partially user-defined.

Version:
	This script can be considered a Version 2.0 of the subfilterDollo_Parab.py script
		prepared for the preliminary project. It allows for the filtration of the
		Dollo results table produced by Count by any given node.

Usage
	./subfilter_Dollo.py input_db ref_db query_node pre_query_node output_extension
	OR
	python subfilter_Dollo.py input_db ref_db query_node _pre_query_node output_extension

	Where the query_node is the number assigned to the query node in the Dollo parsimony
		analysis; and the pre_query_node is the number assigned to the node one up from
		(prior to) the query node. These are not always numerically linked!

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
#input files
input_db = sys.argv[1]
#input_db = "MetamonadCtrl_sec_3_SP__CountPivot__Dollo.txt"
ref_db = sys.argv[2]
#ref_db = "MetamonadCtrl_sec_3_SP__CountPivot.txt"

#query node
query_node = sys.argv[3]
#query_node = 12 #Parabasalia/Anaeramoebidae split
pre_query_node = sys.argv[4]
#pre_query_node = 13

#output file
output_extension = sys.argv[5]
#output_extension = "ParaAna12"
#determine file name suffix/extension for query
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_db = out_full + "_" + output_extension + ".txt"


#Part 2: Import data into Pandas dataframes

#read in the input dollo parsimony data file, assigning the first row as a header row
input_df = pd.read_csv(input_db, skiprows=0, sep = '\t', header=1)
#the file is a tab-delimited text file, so the separator needs to be specified
#`skiprows=1` allows the first line of the file (which does not contain data or headers) to be skipped
#the header line has to be moved down 1 line as well, as a result
input_df.rename(columns={'# Family':'Orthogroup'}, inplace=True)
#rename first column header for ease of use

ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)
#import the reference dataframe with the first row as a header row


#Part 3: Extracting query OGs & writing out results

#convert the node numbers into strings to use as column header names
query_header = str(query_node)
pre_query_header = str(pre_query_node)

#extract rows from the input database where the OG is present
dollo_df_1 = input_df[input_df[query_header] == 1]
#dollo_df_1 contains all OGs present at the query node
dollo_df_2 = dollo_df_1[dollo_df_1[pre_query_header] == 0]
#by selecting OGs not present at at the pre_query_node,
#we filter down to the OGs unique to the query node and its branches


#extract desired OGs with PFam data from the reference database
filt_OG_list = dollo_df_2['Orthogroup'].tolist()
#convert OG IDs column into a list

ref_og_col = ref_df.columns[0]
#extract the name of the OG column from the reference dataframe
filt_ref_df = ref_df[ref_df[ref_og_col].isin(filt_OG_list)]
#filter the reference database to only those OGs unique to parabasalids


#reformatting for ease of use
col = filt_ref_df['pfamEN_hits_Counts']
#saving the pfamEN_hits_Counts column to a new variable
filt_ref_df.pop('pfamEN_hits_Counts')
#removing the pfamEN_hits_Counts column from the original dataframe
filt_ref_df.insert(filt_ref_df.columns.get_loc(ref_og_col) + 1, col.name, col, allow_duplicates=False)
#putting the pfamEN_hits_Counts column back into the dataframe, but at a different location


#write out results to a tab-separated text file
filt_ref_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#working in the / directory
#model: 
python subfilter_Dollo.py input_db ref_db query_node pre_query_node output_extension
#applying it: 
python subfilter_Dollo.py MetamonadCtrl_sec_3_SP__CountPivot__Dollo.txt MetamonadCtrl_sec_3_SP__CountPivot.txt 12 13 ParaAna12
#extracting PFam list
awk -F '\t' '(NR>1) {print $2}' MetamonadCtrl_sec_3_SP__CountPivot__Dollo_ParaAna12.txt | sort | uniq | grep '[^[:blank:]]' >  MetamonadCtrl_sec_3_SP__Dollo_ParaAna12__PFams.txt
# the `grep` at the end ensures that the final printout doesn't contain blank lines
###
#using score-filtered OG list as the basis for the work below: 
#Parabasalia
#Mito OF
python ../subfilter_Dollo.py Alanta_mito_3_OF_ALL__CountPivot__Dollo.txt Alanta_mito_3_OF_ALL__CountPivot.txt 10 11 Parab

#Mito SP
python ../subfilter_Dollo.py Alanta_mito_3_SP_ALL__CountPivot__Dollo.txt Alanta_mito_3_SP_ALL__CountPivot.txt 10 11 Parab

#Sec OF
python ../subfilter_Dollo.py Alanta_sec_3_OF_ALL__CountPivot__Dollo.txt Alanta_sec_3_OF_ALL__CountPivot.txt 10 11 Parab

#Sec SP
python ../subfilter_Dollo.py Alanta_sec_3_SP_ALL__CountPivot__Dollo.txt Alanta_sec_3_SP_ALL__CountPivot.txt 10 11 Parab

#Para/Ana
#Mito OF
python ../subfilter_Dollo.py Alanta_mito_3_OF_ALL__CountPivot__Dollo.txt Alanta_mito_3_OF_ALL__CountPivot.txt 11 31 ParaAna

#Mito SP
python ../subfilter_Dollo.py Alanta_mito_3_SP_ALL__CountPivot__Dollo.txt Alanta_mito_3_SP_ALL__CountPivot.txt 11 31 ParaAna

#Sec OF
python ../subfilter_Dollo.py Alanta_sec_3_OF_ALL__CountPivot__Dollo.txt Alanta_sec_3_OF_ALL__CountPivot.txt 11 31 ParaAna

#Sec SP
python ../subfilter_Dollo.py Alanta_sec_3_SP_ALL__CountPivot__Dollo.txt Alanta_sec_3_SP_ALL__CountPivot.txt 11 31 ParaAna

#Anaeramoebidae
#Mito OF
python ../subfilter_Dollo.py Alanta_mito_3_OF_ALL__CountPivot__Dollo.txt Alanta_mito_3_OF_ALL__CountPivot.txt 3 11 Anaera

#Mito SP
python ../subfilter_Dollo.py Alanta_mito_3_SP_ALL__CountPivot__Dollo.txt Alanta_mito_3_SP_ALL__CountPivot.txt 3 11 Anaera

#Sec OF
python ../subfilter_Dollo.py Alanta_sec_3_OF_ALL__CountPivot__Dollo.txt Alanta_sec_3_OF_ALL__CountPivot.txt 3 11 Anaera

#Sec SP
python ../subfilter_Dollo.py Alanta_sec_3_SP_ALL__CountPivot__Dollo.txt Alanta_sec_3_SP_ALL__CountPivot.txt 3 11 Anaera



```

An alternative version of the script, to look for OGs that *do* exist at the previous node (script saved to subfilter_Dollo_LCA.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: subfilter_Dollo_LCA.py
Date: 2022.05.31
Author: Virág Varga

Description:
	This program uses data from the Count program to filter the input file used for
		Count down to the OGs which are ancestral to a node realtive to the previous
		node on the evolutionary tree. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas import the contents of the Count annotated input data file and
		Count Dollo parsimony data output file.
	3. Filtering out the OGs unique to the query node, before writing out the
		results to a tab-delimited text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of an OG database prepped as input into Count,
		along with an output file from Count containing Dollo Parsimony data.
	- The output file name is only partially user-defined.

Version:
	This script can be considered a Version 2.0 of the subfilterDollo_Parab.py script
		prepared for the preliminary project. It allows for the filtration of the
		Dollo results table produced by Count by any given node.

Usage
	./subfilter_Dollo_LCA.py input_db ref_db query_node pre_query_node output_extension
	OR
	python subfilter_Dollo_LCA.py input_db ref_db query_node _pre_query_node output_extension

	Where the query_node is the number assigned to the query node in the Dollo parsimony
		analysis; and the pre_query_node is the number assigned to the node one up from
		(prior to) the query node. These are not always numerically linked!

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
#input files
input_db = sys.argv[1]
#input_db = "MetamonadCtrl_sec_3_SP__CountPivot__Dollo.txt"
ref_db = sys.argv[2]
#ref_db = "MetamonadCtrl_sec_3_SP__CountPivot.txt"

#query node
query_node = sys.argv[3]
#query_node = 12 #Parabasalia/Anaeramoebidae split
pre_query_node = sys.argv[4]
#pre_query_node = 13

#output file
output_extension = sys.argv[5]
#output_extension = "ParaAna12"
#determine file name suffix/extension for query
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_db = out_full + "_" + output_extension + ".txt"


#Part 2: Import data into Pandas dataframes

#read in the input dollo parsimony data file, assigning the first row as a header row
input_df = pd.read_csv(input_db, skiprows=0, sep = '\t', header=1)
#the file is a tab-delimited text file, so the separator needs to be specified
#`skiprows=1` allows the first line of the file (which does not contain data or headers) to be skipped
#the header line has to be moved down 1 line as well, as a result
input_df.rename(columns={'# Family':'Orthogroup'}, inplace=True)
#rename first column header for ease of use

ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)
#import the reference dataframe with the first row as a header row


#Part 3: Extracting query OGs & writing out results

#convert the node numbers into strings to use as column header names
query_header = str(query_node)
pre_query_header = str(pre_query_node)

#extract rows from the input database where the OG is present
dollo_df_1 = input_df[input_df[query_header] == 1]
#dollo_df_1 contains all OGs present at the query node
dollo_df_2 = dollo_df_1[dollo_df_1[pre_query_header] == 1]
#by selecting OGs present at at the pre_query_node,
#we filter down to the ancestral OGs still preserved at the query_node


#extract desired OGs with PFam data from the reference database
filt_OG_list = dollo_df_2['Orthogroup'].tolist()
#convert OG IDs column into a list

ref_og_col = ref_df.columns[0]
#extract the name of the OG column from the reference dataframe
filt_ref_df = ref_df[ref_df[ref_og_col].isin(filt_OG_list)]
#filter the reference database to only those OGs unique to parabasalids


#reformatting for ease of use
col = filt_ref_df['pfamEN_hits_Counts']
#saving the pfamEN_hits_Counts column to a new variable
filt_ref_df.pop('pfamEN_hits_Counts')
#removing the pfamEN_hits_Counts column from the original dataframe
filt_ref_df.insert(filt_ref_df.columns.get_loc(ref_og_col) + 1, col.name, col, allow_duplicates=False)
#putting the pfamEN_hits_Counts column back into the dataframe, but at a different location


#write out results to a tab-separated text file
filt_ref_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#Para/Ana AND Metamonada LCA
#Mito OF
python ../subfilter_Dollo_LCA.py Alanta_mito_3_OF_ALL__CountPivot__Dollo.txt Alanta_mito_3_OF_ALL__CountPivot.txt 11 31 MetaParaAna

#Mito SP
python ../subfilter_Dollo_LCA.py Alanta_mito_3_SP_ALL__CountPivot__Dollo.txt Alanta_mito_3_SP_ALL__CountPivot.txt 11 31 MetaParaAna

#Sec OF
python ../subfilter_Dollo_LCA.py Alanta_sec_3_OF_ALL__CountPivot__Dollo.txt Alanta_sec_3_OF_ALL__CountPivot.txt 11 31 MetaParaAna

#Sec SP
python ../subfilter_Dollo_LCA.py Alanta_sec_3_SP_ALL__CountPivot__Dollo.txt Alanta_sec_3_SP_ALL__CountPivot.txt 11 31 MetaParaAna




```

Eventually decided to simplify things, and created a script to allow for testing of presence/absence of OGs at any two given nodes (query node and pre-query node) (script saved to subfilter_Dollo_CHOICE.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: subfilter_Dollo_CHOICE.py
Date: 2022.05.31
Author: Virág Varga

Description:
	This program uses data from the Count program to filter the input file used for
		Count down to the OGs which are ancestral to a node relative to the previous
		node on the evolutionary tree. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Using Pandas import the contents of the Count annotated input data file and
		Count Dollo parsimony data output file.
	3. Filtering out the OGs unique to the query node, before writing out the
		results to a tab-delimited text file.

Known bugs and limitations:
	- There is only limited quality-checking integrated into the code.
	- This program requires the input of an OG database prepped as input into Count,
		along with an output file from Count containing Dollo Parsimony data.
	- The output file name is only partially user-defined.

Version:
	This script can be considered a Version 3.0 of the subfilterDollo_Parab.py script
		prepared for the preliminary project. It allows for the filtration of the
		Dollo results table produced by Count by any given node, and allows for more 
		choices related to the ssearch for ancestral vs. derived OGs on the tree. 

Usage
	./subfilter_Dollo_CHOICE.py input_db ref_db query_node pre_query_node query_presence pre_query_presence 
		output_extension
	OR
	python subfilter_Dollo_CHOICE.py input_db ref_db query_node pre_query_node query_presence pre_query_presence 
		output_extension

	Where the query_node is the number assigned to the query node in the Dollo parsimony
		analysis; and the pre_query_node is the number assigned to the node one up from
		(prior to) the query node. These are not always numerically linked!
	Where the query_presence and pre_query_presence arguments can be either 0 (to represent absence of
		of an OG at the given node)	or 1 (to represent presence of an OG at the given node).

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
#input files
input_db = sys.argv[1]
#input_db = "MetamonadCtrl_sec_3_SP__CountPivot__Dollo.txt"
ref_db = sys.argv[2]
#ref_db = "MetamonadCtrl_sec_3_SP__CountPivot.txt"

#query node
query_node = int(sys.argv[3])
#query_node = 12 #Parabasalia/Anaeramoebidae split
pre_query_node = int(sys.argv[4])
#pre_query_node = 13

#node presence
query_presence = int(sys.argv[5])
pre_query_presence = int(sys.argv[6])
#define the acceptable values for the presence/absence options
presence_absence_profile = [0, 1]

#quick basic QC on input values
if query_presence not in presence_absence_profile: 
	#if the options given are not applicable options (ie. 0 or 1)
	#return this message to the command line 
	print("Note that presence of an OG at a node is represented by 1, while absence represented by 0. Please try again.")
	#and exit the program
	sys.exit(1)
elif pre_query_node > 31 or query_node > 31: 
	#test for node numbers on the tree, to make sure the values are acceptable
	#return this message to the command line 
	print("Please note that this program was designed for the Thesis_Trich project, where the largest node number was 31. \
	   The node number you have selected is too high. Please either correct your command use or modify this program file.")
	#and exit the program
	sys.exit(1)
else: 
	#if the node and presence absence choices are acceptable
	#return the following message to the stdout & continue running the program
	print("Testing for value of "+ str(query_presence) + " at node " + str(query_node) + " and value of " + \
	   str(pre_query_presence) + " at node " + str(pre_query_node) + ".")


#output file
output_extension = sys.argv[7]
#output_extension = "ParaAna12"
#determine file name suffix/extension for query
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_db = out_full + "_" + output_extension + ".txt"


#Part 2: Import data into Pandas dataframes

#read in the input dollo parsimony data file, assigning the first row as a header row
input_df = pd.read_csv(input_db, skiprows=0, sep = '\t', header=1)
#the file is a tab-delimited text file, so the separator needs to be specified
#`skiprows=1` allows the first line of the file (which does not contain data or headers) to be skipped
#the header line has to be moved down 1 line as well, as a result
input_df.rename(columns={'# Family':'Orthogroup'}, inplace=True)
#rename first column header for ease of use

ref_df = pd.read_csv(ref_db, sep = '\t', header = 0)
#import the reference dataframe with the first row as a header row


#Part 3: Extracting query OGs & writing out results

#convert the node numbers into strings to use as column header names
query_header = str(query_node)
pre_query_header = str(pre_query_node)

#extract rows from the input database where the OG is present
dollo_df_1 = input_df[input_df[query_header] == query_presence]
#dollo_df_1 contains all OGs with the given presence/absence profile at the query node
dollo_df_2 = dollo_df_1[dollo_df_1[pre_query_header] == pre_query_presence]
#dollo_df2 filters dollo_df1 to only the OGs 
#that match the given pre_query_node presence/absence profile


#extract desired OGs with PFam data from the reference database
filt_OG_list = dollo_df_2['Orthogroup'].tolist()
#convert OG IDs column into a list

ref_og_col = ref_df.columns[0]
#extract the name of the OG column from the reference dataframe
filt_ref_df = ref_df[ref_df[ref_og_col].isin(filt_OG_list)]
#filter the reference database to only those OGs unique to parabasalids


#reformatting for ease of use
col = filt_ref_df['pfamEN_hits_Counts']
#saving the pfamEN_hits_Counts column to a new variable
filt_ref_df.pop('pfamEN_hits_Counts')
#removing the pfamEN_hits_Counts column from the original dataframe
filt_ref_df.insert(filt_ref_df.columns.get_loc(ref_og_col) + 1, col.name, col, allow_duplicates=False)
#putting the pfamEN_hits_Counts column back into the dataframe, but at a different location


#write out results to a tab-separated text file
filt_ref_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#model: 
python subfilter_Dollo_CHOICE.py input_db ref_db query_node pre_query_node query_presence pre_query_presence output_extension
#using it: 
#present in common ancestor but lost in Fornicata
#Mito OF
python ../subfilter_Dollo_CHOICE.py Alanta_mito_3_OF_ALL__CountPivot__Dollo.txt Alanta_mito_3_OF_ALL__CountPivot.txt 28 31 0 1 MetaLCAnonFornicates
#yup, it works! so I can use it for more deliberate filtrations if I want to

```


### PFam Filtration

Script to filter out OGs without PFam annotations and/or filter PFam information in OGs to only the PFam domain names (excluding number of occurances per OG) (parse_OG_PFams.py)

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_OG_PFams.py
Date: 2022.05.18
Author: Virág Varga

Description:
	This program iterates over a file produced by the create_counts_table.py program
		to create one or multiple of the following:
			- A filtered version of the file without OGs lacking PFam domain hits
			- A indexing database in the format: [OG_ID]\tAssocPFams
			- A text file containing the unique PFam IDs in the input database in
				the format: PFam1\nPFam2\nPFam3 etc.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; assigning command line arguments.
	2. Importing data into Pandas dataframe.
	3.1: Creating database version without OGs lacking PFam annotations.
	3.2: Creating database of OG IDs and associated PFam IDs.
	3.3: Creating list of unique PFam IDs.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file names are based on the input file name.

Usage
	./parse_OG_PFams.py input_db filt_option
	OR
	python parse_OG_PFams.py input_db filt_option

	Where the input_db file contains OG IDs and associated PFams in a column
		named "pfamEN_hits_Counts".
	Where filt_option allows the user to specify the type(s) of filtration to be
		performed:
			- "list": to output a file containing a unique list of PFams in the
				format: PFam1\nPFam2\nPFam3 etc.
			- "index": to output an indexing file in the format:
				[OG_ID]\tAssocPFams
			- "remove": To output a copy of the input database without the OGs
				that lack PFam domain hits.
		Multiple options can be selected by placing them in a comma-separated list
		on the command line (ex.: `python parse_OG_PFams.py input_db list,index`).

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "MetamonadCtrl_sec_3_SP__CountPivot.txt"

filt_option = sys.argv[2]
#filt_option = "test"
#save the contents of the comma-separated string to a list variable
filt_list = filt_option.split(",")


#output file name should be based on input file name
base = os.path.basename(input_db)
#assign the basename of output file
out_base = os.path.splitext(base)[0]


#Part 2: Import data into Pandas dataframe

#read in the Metamonad database to a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0)
#then extract the name of the OG program
og_program = input_df.columns[0]


#Part 3.1: Create database version without OGs lacking PFam annotations

#remove the rows from the dataframe containing OGs without PFam domain hits
filt_df = input_df.dropna().copy()
#`.dropna()` removes rows containing NaNs
#since this only occurs for OGs where there is no PFam entry, this works for our data


if "remove" in filt_list:
	#if the option to output a database without OGs lacking PFam domain hits has been selected
	print("remove option selected")

	#determine output file name
	output_db = out_base + "_allPFam.txt"
	#and write out results to a tab-separated text file
	filt_df.to_csv(output_db, sep = '\t', index=False)


#Part 3.2: Create database of OG IDs and associated PFam IDs

if "index" in filt_list:
	#if the option to output an indexing database in the format: [OG_ID]\tAssocPFams
	#has been selected
	print("index option selected")

	#create a dictionary in the format: pfam_prep_dict[OG_ID] = list_of_PFams
	pfam_prep_dict = filt_df.set_index(og_program).to_dict()['pfamEN_hits_Counts']
	#create an empty dictionary to populate with the actual PFam ID lists
	pfam_dict = {}

	for key in pfam_prep_dict.keys():
		#iterate over the dictionary via its keys (the OG IDs)
		temp_pfam_list = []
		#create an empty list to hold the PFam domain IDs for each OG
		pfam_split_list = pfam_prep_dict[key].split(",")
		#split the PFam domain IDs into a list based on comma placement
		for m in pfam_split_list:
			#iterate over the list of PFam domains with frequency data
			pfam_id = m.split(":")[0]
			#identify the PFam domain ID
			#and add it to the list of PFam IDs
			temp_pfam_list.append(pfam_id)
		#populate the new dictionary with the OG & PFam ID data
		pfam_dict[key] = temp_pfam_list

	#create a new dataframe from the dictionary
	#ref: https://stackoverflow.com/questions/50751184/pandas-dataframe-from-dictionary-of-list-values
	og_pfam_df = pd.DataFrame([(key, var) for (key, L) in pfam_dict.items() for var in L],
							  columns=[og_program, 'AssocPFams'])

	#determine output file name
	output_index = out_base + "_PFamIndex.txt"
	#and write out results to a tab-separated text file
	og_pfam_df.to_csv(output_index, sep = '\t', index=False)


#Part 3.3: Create list of unique PFam IDs

if "list" in filt_list:
	#if the option to output a list of unique PFam IDs has been given
	print("list option selected")

	#send PFams to a list
	full_pfam_list = filt_df.pfamEN_hits_Counts.tolist()

	#create new empty list for PFam domains
	pfam_list = []

	for i in full_pfam_list:
		#iterate over the list of PFam domains associated with number of occurances
		pfam_split_list = i.split(",")
		#split the small list via the comma placement
		for j in pfam_split_list:
			#iterate over the list of PFam domains with frequency data
			pfam_id = j.split(":")[0]
			#identify the PFam domain ID
			#and add it to the list of PFam IDs
			pfam_list.append(pfam_id)

	#remove duplicates from the list
	pfam_setlist = list(set(pfam_list))

	#determine the output file name
	output_pfams = out_base + "_PFamList.txt"
	#and write out results to a file
	with open(output_pfams, "w") as outfile:
		#open the output file for writing
		for k in pfam_setlist:
			#iterate over the list of unique PFam domain IDs
			#and write each PFam domain ID to a new line in the file
			outfile.write(k + "\n")


else:
	#if the user has not provided an appropriate program run option
	#display the following help message
	print("Input arguments are incorrect. Consider the following usage instructions:" + "\n" +
	"python parse_OG_PFams.py input_db filt_option" + "\n" +
	"Where the input_db file contains OG IDs and associated PFams in a column named 'pfamEN_hits_Counts'" + "\n" +
	"Where filt_option allows the user to specify the type(s) of filtration to be performed:" + "\n" +
		"- 'list': to output a file containing a unique list of PFams in the format: PFam1\nPFam2\nPFam3 etc." + "\n" +
		"- 'index': to output an indexing file in the format: [OG_ID]\tAssocPFams" + "\n" +
		"- 'remove': To output a copy of the input database without the OGsthat lack PFam domain hits." + "\n" +
	"Multiple options can be selected by placing them in a comma-separated list on the command line" + "\n" +
	"(ex.: python parse_OG_PFams.py input_db list,index).")

```

Using it: 

```bash
#working in the / directory
#model: 
python parse_OG_PFams.py input_db filt_option
#applying it: 
python parse_OG_PFams.py MetamonadCtrl_sec_3_SP__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python parse_OG_PFams.py MetamonadCtrl_sec_3_OF__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python parse_OG_PFams.py MetamonadCtrl_mito_3_OF__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python parse_OG_PFams.py Alanta_mito_3_SP__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
###
#A. lanta filtration
python parse_OG_PFams.py Alanta_sec_3_SP__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python parse_OG_PFams.py Alanta_sec_3_OF__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python parse_OG_PFams.py Alanta_mito_3_OF__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python parse_OG_PFams.py Alanta_mito_3_SP__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
#including T. vaginalis-unique OGs
python ../parse_OG_PFams.py Alanta_sec_3_SP_Tv__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python ../parse_OG_PFams.py Alanta_sec_3_OF_Tv__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python ../parse_OG_PFams.py Alanta_mito_3_OF_Tv__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python ../parse_OG_PFams.py Alanta_mito_3_SP_Tv__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
#only score filtration
python ../parse_OG_PFams.py Alanta_sec_3_SP_ALL__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python ../parse_OG_PFams.py Alanta_sec_3_OF_ALL__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python ../parse_OG_PFams.py Alanta_mito_3_OF_ALL__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected
python ../parse_OG_PFams.py Alanta_mito_3_SP_ALL__CountPivot.txt remove,index,list
# remove option selected
# index option selected
# list option selected

```


Retrieving functional annotation data from the PFam database is best done via the RESTful API: 
 - https://www.biostars.org/p/5154/ (note that this answer is old, and the links no longer work)
 - https://pfam-docs.readthedocs.io/en/latest/restful-interface.html (RESTful interface - remote querying of data)

```bash
#testing in the /home/inf-47-2020/ThesisTrich/CMD_TEST_DIR/PFam_testing/ directory
#ref: https://pfam-docs.readthedocs.io/en/latest/restful-interface.html
curl -LH 'Expect:' -F output=xml 'https://pfam.xfam.org/family/P
iwi'
# <?xml version="1.0" encoding="UTF-8"?>
# <!-- information on Pfam-A family PF02171 (Piwi), generated: 12:08:55 18-May-2022 -->
# <pfam xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
#       xmlns="https://pfam.xfam.org/"
#       xsi:schemaLocation="https://pfam.xfam.org/
#                           https://pfam.xfam.org/static/documents/schemas/pfam_family.xsd"
#       release="35.0"
#       release_date="2021-11-04">
#   <entry entry_type="Pfam-A" accession="PF02171" id="Piwi">
#     <description>
# <![CDATA[
# Piwi domain
# ]]>
#     </description>
#     <comment>
# <![CDATA[
# This domain is found in the protein Piwi and its relatives.  The function of this domain is the dsRNA guided hydrolysis of ssRNA. Determination of the crystal structure of Argonaute reveals that PIWI is an RNase H domain, and identifies Argonaute as Slicer, the enzyme that cleaves mRNA in the RNAi RISC complex [2].  In addition, Mg+2 dependence and production of 3'-OH and 5' phosphate products are shared characteristics of RNaseH and RISC. The PIWI domain core has a tertiary structure belonging to the RNase H family of enzymes.  RNase H fold proteins all have a five-stranded mixed beta-sheet surrounded by helices. By analogy to RNase H enzymes which cleave single-stranded RNA guided by the DNA strand in an RNA/DNA hybrid, the PIWI domain can be inferred to cleave single-stranded RNA, for example mRNA, guided by double stranded siRNA.
# ]]>
#     </comment>
#     <clan_membership clan_acc="CL0219" clan_id="RNase_H" />
#     <go_terms>
#       <category name="function">
#         <term go_id="GO:0003676">nucleic acid binding</term>
#       </category>
#     </go_terms>
#     <curation_details>
#       <status>CHANGED</status>
#       <seed_source>Bateman A</seed_source>
#       <num_archs>339</num_archs>
#       <num_seqs>
#         <seed>15</seed>
#         <full>14038</full>
#       </num_seqs>
#       <num_species>1613</num_species>
#       <num_structures>169</num_structures>
#       <percentage_identity>37</percentage_identity>
#       <av_length>265.1</av_length>
#       <av_coverage>34.14</av_coverage>
#       <type>Family</type>
#     </curation_details>
#     <hmm_details hmmer_version="3.1b2" model_version="20" model_length="302">
#       <build_commands>hmmbuild  -o /dev/null HMM SEED</build_commands>
#       <search_commands>hmmsearch -Z 61295632 -E 1000 --cpu 4 HMM pfamseq</search_commands>
#       <cutoffs>
#         <gathering>
#           <sequence>28.9</sequence>
#           <domain>28.9</domain>
#         </gathering>
#         <trusted>
#           <sequence>28.9</sequence>
#           <domain>28.9</domain>
#         </trusted>
#         <noise>
#           <sequence>28.8</sequence>
#           <domain>28.8</domain>
#         </noise>
#       </cutoffs>
#     </hmm_details>
#   </entry>
# </pfam>
#results are automatically printed to the stdout, so let's try specifying a destination
curl -LH 'Expect:' -F output=xml 'https://pfam.xfam.org/family/Piwi' > PFam_Piwi.xml
#   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
#                                  Dload  Upload   Total   Spent    Left  Speed
# 100  2874  100  2730  100   144   1946    102  0:00:01  0:00:01 --:--:--  2049
curl -LH 'Expect:' -F output=xml 'https://pfam.xfam.org/family/Piwi' --output PFam_Piwi2.xml
#   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
#                                  Dload  Upload   Total   Spent    Left  Speed
# 100  2874  100  2730  100   144    259     13  0:00:11  0:00:10  0:00:01   641
#so both of these options work, though the `curl` man page suggests the `--output` or `-o` as the inbuilt function

```

Using the above line of unix code as a starting point, the following python script extracts the PFam data in XML format for a given list of PFam IDs (script saved to retrieve_PFam_XML.py)

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: retrieve_PFam_XML.py
Date: 2022.05.18
Author: Virág Varga

Description:
	This program queries the PFam Database's RESTful interface in order to extract
		the XML-formatted data files containing the annotation available for that 
		PFam domain ID. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	subprocess

Procedure:
	1. Loading required modules; assigning command line argument.
	2. Querying the desired data from the PFam RESTful interface.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file names are based on the PFam domain ID names.

Usage
	./retrieve_PFam_XML.py pfam_list
	OR
	python retrieve_PFam_XML.py pfam_list

	Where pfam_list must be a query PFam domain ID list in one of the following formats:
		- Singular PFam domain ID provided on the command line
		- Comma-separated list of PFam domain IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of PFam domain IDs in format: ID1\nID2 etc.

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import subprocess #allows execution of bash commands from Python


#assign command line argument & load input PFam domain list
pfam_list = sys.argv[1]
#pfam_list = "PAS_9"
#pfam_list = "MetamonadCtrl_sec_3_SP__CountPivot_PFamList.txt"
#import the PFam domain query list
if os.path.isfile(pfam_list):
	#if the input selection of PFam domain IDs is a file
	with open(pfam_list, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of PFam domain query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
		#eliminate duplicates
		query_list = list(set(query_list))
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = pfam_list.split(",")
	#eliminate duplicates
	query_list = list(set(query_list))


#Part 2: Query the desired data from the PFam RESTful interface

for i in query_list: 
	#iterate over the list of PFam domain IDs
	outfile = "PFam_" + i + ".xml"
	#determine output file name
	if not os.path.isfile(outfile):
		#ensure that the query PFam domain ID has not already been queried in a previous session
		#if the file does not already exist, continue on to querying
		#ref: https://pfam-docs.readthedocs.io/en/latest/restful-interface.html
		query_address = "https://pfam.xfam.org/family/" + i 
		#note that unlike in the documentation, I removed the single quotes around the web address
		#this is due to the way that `curl` interprets arguments
		#ref: https://stackoverflow.com/questions/70200540/python-subprocess-not-passing-curly-brackets-as-arguments-or-a-problem-with-dou
		#ref: https://docs.python.org/3/library/subprocess.html
		bashCommand = "curl -LH 'Expect:' -F output=xml " + query_address + " --output " + outfile
		bash_list = bashCommand.split()
		subprocess.run(bash_list, capture_output=True)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/PFam_XML_Files/ directory
#model: 
python retrieve_PFam_XML.py pfam_list
#applying it: 
python ../Scripts/retrieve_PFam_XML.py MetamonadCtrl_mito_3_OF__CountPivot_PFamList.txt
python ../Scripts/retrieve_PFam_XML.py MetamonadCtrl_mito_3_SP__CountPivot_PFamList.txt
###
#some interesting notes!
wc -l MetamonadCtrl_mito_3_OF__CountPivot_PFamList.txt
# 844 MetamonadCtrl_mito_3_OF__CountPivot_PFamList.txt
ls PFam_* | wc -l
# 844
wc -l MetamonadCtrl_mito_3_SP__CountPivot_PFamList.txt
# 803 MetamonadCtrl_mito_3_SP__CountPivot_PFamList.txt
ls PFam_* | wc -l
# 969
#this means there's a difference of only 125 PFam domains between the SP & OF predictions! 
#the overlap is SIGNIFICANT, which is a really good sign!!!
###
python ../Scripts/retrieve_PFam_XML.py MetamonadCtrl_sec_3_OF__CountPivot_PFamList.txt
python ../Scripts/retrieve_PFam_XML.py MetamonadCtrl_sec_3_SP__CountPivot_PFamList.txt
###
#same checks as before
wc -l MetamonadCtrl_sec_3_OF__CountPivot_PFamList.txt
# 1552 MetamonadCtrl_sec_3_OF__CountPivot_PFamList.txt
ls PFam_* | wc -l
# 1552
wc -l MetamonadCtrl_sec_3_SP__CountPivot_PFamList.txt
# 1543 MetamonadCtrl_sec_3_SP__CountPivot_PFamList.txt
ls PFam_* | wc -l
# 1718
#so one again, this is good! There's quite a bit of overlap! 
###
ln -s /home/inf-47-2020/ThesisTrich/PFam_XML_Files/Mito_PFams/PFam_* .
ln -s /home/inf-47-2020/ThesisTrich/PFam_XML_Files/Sec_PFams/PFam_* .
#there was overlap - saved the writeout with file names to Overlap_PFams_printout.txt
ls PFam_* | wc -l
# 1825
#969+1718=2687
#2687-1825=862
#which is more than I would like
#but.
#the secretome protein list includes proteins predicted to have been targeted to the cell membrane
#since the mitochondria also has a cell membrane, it's reasonable to assume that some of these have to do with channels & transporters
#let's isolate them
#ref: https://stackoverflow.com/questions/13242469/how-to-use-sed-grep-to-extract-text-between-two-words
grep -o -P '(?<=PFam_).*(?=.xml)' Overlap_PFams_printout.txt > Overlap_PFams.txt
#ok, now I have them if I want to take a closer look at them later
#directory structure for /home/inf-47-2020/ThesisTrich/PFam_XML_Files/
ls
# All_PFam_SymLinks  List_Files  Mito_PFams  Overlap_PFams  Sec_PFams
###
#A. lanta integration
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_mito_3_OF__CountPivot_PFamList.txt
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_mito_3_SP__CountPivot_PFamList.txt
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_sec_3_OF__CountPivot_PFamList.txt
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_sec_3_SP__CountPivot_PFamList.txt
#working in the /home/inf-47-2020/ThesisTrich/PFam_XML_Files/All_PFam_SymLinks/ driectory
ln -s /home/inf-47-2020/ThesisTrich/PFam_XML_Files/Sec_PFams/PFam_* .
#somehow??? removed all files in the Mito_PFams/ directory -_-;;
#so re-running all mito-related PFam lists before symlinking
ln -s /home/inf-47-2020/ThesisTrich/PFam_XML_Files/Mito_PFams/PFam_* .
#then copy out the overlap text into a Overlap_PFams_printout.txt file and deal with it
grep -o -P '(?<=PFam_).*(?=.xml)' Overlap_PFams_printout.txt > Overlap_PFams.txt
###
#did the above on my personal computer because I hit a snag
#home directory
du -hs
# 470G    .
#ThesisTrich/ directory
du -hs ./*
# 8.1G    ./Broccoli_Results
# 351M    ./BUSCO_Results2
# 8.8M    ./CMD_TEST_DIR
# 2.0G    ./DataFiles
# 168M    ./DB2_A_ignava
# 275G    ./DB_Construct
# 450M    ./DeepLoc_Results
# 2.4G    ./EggNOG_dn_Results
# 88G     ./InterProScan_Results
# 4.0K    ./MAFFT_MSA
# 211M    ./MitoFates_Results
# 8.2G    ./OrthoFinder_Results
# 27G     ./PARSED_Results
# 19M     ./PFam_XML_Files
# 376M    ./ProteinOrtho_Results
# 592K    ./Scripts
# 361M    ./SignalP_Results
# 2.4G    ./SonicParanoid_Results
# 148K    ./SortedEncodedData
# 344M    ./TargetP_Results
# 1000M   ./TEST_EggNOG
# 482M    ./UNNECESSARY_FILES
# 2.3G    ./YLoc_Results
#checked in with Mara - she's using more space than I am, and she can't make any new files, either
#conclusion: someone else is to blame lmao
#so we'll come back to this
###
#adding in the additional PFams from the broader datasets
#no longer excluding T. vaginalis unique
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_mito_3_OF_Tv__CountPivot_PFamList.txt &
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_mito_3_SP_Tv__CountPivot_PFamList.txt &
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_sec_3_OF_Tv__CountPivot_PFamList.txt &
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_sec_3_SP_Tv__CountPivot_PFamList.txt &
#all PFams based on score filtration
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_mito_3_OF_ALL__CountPivot_PFamList.txt &
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_mito_3_SP_ALL__CountPivot_PFamList.txt &
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_sec_3_OF_ALL__CountPivot_PFamList.txt &
python ../../Scripts/retrieve_PFam_XML.py ../List_Files/Alanta_sec_3_SP_ALL__CountPivot_PFamList.txt &
#now conglomerating again... (hopefully for the final time...)
ln -s /home/inf-47-2020/ThesisTrich/PFam_XML_Files/Sec_PFams/PFam_* .
ln -s /home/inf-47-2020/ThesisTrich/PFam_XML_Files/Mito_PFams/PFam_* . 
grep -o -P '(?<=PFam_).*(?=.xml)' Overlap_PFams_printout.txt > Overlap_PFams.txt
###
#sec notes
ls PFam_* |wc -l
# 1731
ls PFam_* | wc -l
# 1734
ls PFam_* | wc -l
# 1835
ls PFam_* | wc -l
# 2016
ls PFam_* | wc -l
# 2094
ls PFam_* | wc -l
# 2108
#mito notes
ls PFam_* | wc -l
# 985
ls PFam_* | wc -l
# 987
ls PFam_* | wc -l
# 1071
ls PFam_* | wc -l
# 1085
ls PFam_* | wc -l
# 1100
###

```

```bash
#some preliminary exploration of the xml files 
#working in the /home/inf-47-2020/ThesisTrich/PFam_XML_Files/All_PFam_SymLinks/ directory
grep -A 5 "<go_terms>" PFam_*
#checking out the way the GO term data looks
#example of one file with multiple GO hits
# PFam_zf-DNA_Pol.xml:    <go_terms>
# PFam_zf-DNA_Pol.xml-      <category name="function">
# PFam_zf-DNA_Pol.xml-        <term go_id="GO:0003887">DNA-directed DNA polymerase activity</term>
# PFam_zf-DNA_Pol.xml-      </category>
# PFam_zf-DNA_Pol.xml-      <category name="process">
# PFam_zf-DNA_Pol.xml-        <term go_id="GO:0006260">DNA replication</term>
#ok, well that definitely gice me a pettern to go off of
# let's see what all of the categories are...
#ref: https://stackoverflow.com/questions/8488301/bash-find-file-with-max-lines-count
find . -type f -name "*.xml" -exec grep -H -c '[^[:space:]]' {} \; | sort -nr -t":" -k2 | awk -F: '{print $1; exit;}'
#actually, need to modify this to look for symlinks with `-type l`
find . -type l -name "*.xml" -exec grep -H -c '[^[:space:]]' {} \; | sort -nr -t":" -k2 | awk -F: '{print $1; exit;}'
# ./PFam_Thiol-ester_cl.xml
cat PFam_Thiol-ester_cl.xml
#LMFAOOO
#the output was SO WEIRD, that I had to look up the PFam domain on the web to see if maybe something had gone wrong????
#but it turns out,,, this is a "dead" PFam domain! wonderful. 
#it's been merged into TED_complement cluster
find . -type l -name "*.xml" -exec grep -H -c '[^[:space:]]' {} \; | sort -nr -t":" -k2 | awk -F: '{print $1}'
#used the above modification to list them by decreasing size, but #2 was also a dead family
#so sorting this way, instead, though it's maybe less precise
wc -l *.xml | sort
#ok, yeah
#so some of these are in the 321-334 range, but most are 50-70
#we can go ahead and assume that the larger range is the dead families then, lmao
#going with one of the 70-line files
cat PFam_zf-ANAPC11.xml
#even this doesn't have IPRScan annotation, so I'll need to find some other reference file for that
###
#trying to deal with the dead clan problem
wc -l *.xml | sort
#going to leave only the problematic ones in the results
    # 321 PFam_A2M_comp.xml
    # 321 PFam_Abi.xml
    # 321 PFam_Ald_Xan_dh_C2.xml
    # 321 PFam_Amidinotransf.xml
    # 321 PFam_Bac_surface_Ag.xml
    # 321 PFam_BCIP.xml
    # 321 PFam_CTD_bind.xml
    # 321 PFam_DHR-2.xml
    # 321 PFam_DUF106.xml
    # 321 PFam_DUF1126.xml
    # 321 PFam_DUF1394.xml
    # 321 PFam_DUF1619.xml
    # 321 PFam_DUF1682.xml
    # 321 PFam_DUF1966.xml
    # 321 PFam_DUF2012.xml
    # 321 PFam_DUF2373.xml
    # 321 PFam_DUF2870.xml
    # 321 PFam_DUF296.xml
    # 321 PFam_DUF3361.xml
    # 321 PFam_DUF3398.xml
    # 321 PFam_DUF4205.xml
    # 321 PFam_DUF4414.xml
    # 321 PFam_DUF4483.xml
    # 321 PFam_DUF4968.xml
    # 321 PFam_DUF4981.xml
    # 321 PFam_DUF563.xml
    # 321 PFam_DUF667.xml
    # 321 PFam_DUF766.xml
    # 321 PFam_DUF812.xml
    # 321 PFam_DUF814.xml
    # 321 PFam_DUF866.xml
    # 321 PFam_EFG_II.xml
    # 321 PFam_GPI-anchored.xml
    # 321 PFam_Grp1_Fun34_YaaH.xml
    # 321 PFam_KOG2701.xml
    # 321 PFam_LAP1C.xml
    # 321 PFam_NARP1.xml
    # 321 PFam_PaaSYMP.xml
    # 321 PFam_PEPCK_C.xml
    # 321 PFam_Pkinase_Tyr.xml
    # 321 PFam_Poxvirus.xml
    # 321 PFam_RHD3.xml
    # 321 PFam_SMK-1.xml
    # 321 PFam_SNF2_N.xml
    # 321 PFam_Spc97_Spc98.xml
    # 321 PFam_SUA5.xml
    # 321 PFam_Tetraspannin.xml
    # 321 PFam_UBA_e1_thiolCys.xml
    # 321 PFam_UPF0051.xml
    # 321 PFam_VCBS.xml
    # 328 PFam_DLD.xml
    # 328 PFam_DUF1712.xml
    # 334 PFam_Autophagy_C.xml
    # 334 PFam_Autophagy_N.xml
    # 334 PFam_Thiol-ester_cl.xml
grep "Dead family" *.xml
# PFam_Autophagy_C.xml:          <title>Pfam: Dead family</title>
# PFam_Autophagy_N.xml:          <title>Pfam: Dead family</title>
# PFam_DLD.xml:          <title>Pfam: Dead family</title>
# PFam_DUF1712.xml:          <title>Pfam: Dead family</title>
# PFam_Thiol-ester_cl.xml:          <title>Pfam: Dead family</title>
#so clearly this doesn't catch them all
#ref: https://stackoverflow.com/questions/33370890/bash-find-a-list-of-files-with-more-than-3-lines
#find symlinks with line counts over 300
find . -type l -exec bash -c '[[ $(wc -l < "$1") -gt 300 ]] && echo "$1"' _ '{}' \;
find . -type l | xargs wc -l | awk '$1 > 300' | wc -l
# 56
#but it adds one line where the total line count is reported, so the true answer is 55
grep "Renamed family" *.xml | wc -l
# 50
grep "Dead family" *.xml | wc -l
# 5
#ok so those are the patterns I need to search for



```

Parsing the PFam XML data files in conjunction with an indexing file to obtain a summary data table (script saved to parse_OG_PFam_XML.py): 
 - Save PFam domain ID & accession number; GO categories, IDs & descriptions; "description" header category contents (need to strip some extraneous boundary characters)
 - Want to find a way to cross-reference with interproscan
 - Index data with OG IDs


```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_OG_PFam_XML.py
Date: 2022.05.19
Author: Virág Varga

Description:
	This program iterates over an indexing file created by parse_OG_PFams.py (or similar 
		file in in the same format: [OG_PROGRAM]_OG\tAssocPFams) with the aid of PFam 
		domain XML-formatted informational files created with retrieve_PFam_XML.py, 
		in order to create a large database in the format: 
			PFam_IDs\tSonicParanoid_OG\tPFamAccesion\tAlt_Name\tPFam_Description\tMolFunc_GO_ID\t
				MolFunc_GO_Desc\tBiolProc_GO_ID\tBiolProc_GO_Desc\tCellComp_GO_ID\tCellComp_GO_Desc

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas
	re

Procedure:
	1. Loading required modules; assigning command line arguments.
	2. Importing data into Pandas dataframe and restructuring the data.
	3. Extracting relevant PFam information from the PFam XML files into the
		PFam grouped dataframe.
	4. Writing out results in tab-delimited text file. If any dead or renamed
		PFam domains were encountered, these are written out to a file in the 
		format PFam1\nPFam2\nPFam3 etc.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is only partially user-defined. 

Usage
	./parse_OG_PFam_XML.py input_db data_path output_base
	OR
	python parse_OG_PFam_XML.py input_db data_path output_base

	Where the input_db is file in the format:
		[OG_PROGRAM]_OG\tAssocPFams
	Where data_path is a string input in quotation marks on the command line of the
		absolute path to the directory where the data PFam XML files are located.
		This should include a trailing backslash.
	Where output_base is the basename to be used for the output annotation 
		database. 

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python
import re #enables regex pattern matching


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "MetamonadCtrl_sec_3_SP__CountPivot_PFamIndex.txt"
data_path = sys.argv[2]
#data_path = "C:/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/Count/Test_Files/"

output_base = sys.argv[3]
#output_base = "MetamonadCtrl_sec_3_SP"
output_db = output_base + "__AnnotTable.txt"


#Part 2: Import data into Pandas dataframe & restructure

#read in the Metamonad database to a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0)
#then extract the name of the OG program
og_program = input_df.columns[0]

#group the dataframe according to PFam IDs
grouped_pfam_df = input_df.groupby('AssocPFams')[og_program].apply(list).reset_index(name=og_program)
#the OG IDs in the og_program column are grouped into lists according to which PFam IDs have been matched to them
#the new column of lists is named after the original OG program OG ID column name
grouped_pfam_df.rename(columns={'AssocPFams': 'PFam_IDs'}, inplace=True)
#for clarity, rename the PFam ID column
#then join all lists into comma- and space-separated strings
grouped_pfam_df[og_program] = grouped_pfam_df[og_program].apply(lambda x: ', '.join(map(str, x)))
#and add columns for the new data to be imported from the XML files
#ref: https://stackoverflow.com/questions/30926670/add-multiple-empty-columns-to-pandas-dataframe
pfam_df = grouped_pfam_df.reindex(columns=[*grouped_pfam_df.columns.tolist(), 
										   'PFamAccesion', 'Alt_Name', 'PFam_Description', 'MolFunc_GO_ID', 'MolFunc_GO_Desc', 
										   'BiolProc_GO_ID', 'BiolProc_GO_Desc', 'CellComp_GO_ID', 'CellComp_GO_Desc'], 
								  fill_value="-")


#Part 3: Extract relevant PFam information from the PFam XML files

#create empty list of problematic PFam domains
bad_pfams_list = []

for index, row in pfam_df.iterrows(): 
	#iterate over the pfam dataframe row by row (ie. by pfam IDs)
	key = row['PFam_IDs']
	#identify the PFam ID in the row and save it as variable key
	pfam_file = data_path + "PFam_" + key + ".xml"
	#define the file name and path
	if os.path.isfile(pfam_file):
		#ensure that the query PFam domain ID has an XML file associated with it
		with open(pfam_file, "r") as infile: 
			#open the PFam data xml file for reading
			if 'Dead family' in infile.read(): 
				#check if the PFam domain is considered dead
				#and if it is, let the user know
				print("Warning! The provided PFam ID, " + key + ", belongs to a dead family!")
				#and write the problematic PFam domain to the bad domain list
				bad_pfams_list.append(key)
			elif 'Renamed family' in infile.read(): 
				#check if the PFam domain has been renamed
				#and if so, let the user know
				print("Warning! The provided PFam ID, " + key + ", belongs to a renamed family!")
				#and write the problematic PFam domain to the bad domain list
				bad_pfams_list.append(key)
			else: 
				#if the PFam domain still exists, move forward with parsing the xml file
				#for some reason that I cannot figure out, the program enters the else statement
				#but does not proceed into the following for statement
				#so I am reopening the file here
				with open(pfam_file, "r") as infile:
					for line in infile:
						#iterate over the file line by line
						if line.startswith("  <entry"):
							#identify the line that contains basic PFam information
							#ref: https://stackoverflow.com/questions/3368969/find-string-between-two-substrings
							pfam_accession_start = re.search('accession="(.*)" id', line)
							pfam_accession = pfam_accession_start.group(1)
							#extract the PFam accession number
							#and add it into the pfam_df
							#ref: https://stackoverflow.com/questions/25478528/updating-value-in-iterrow-for-pandas
							pfam_df.loc[index, 'PFamAccesion'] = pfam_accession
							next(infile)
							next(infile)
							#skip two lines to reach the line in the file with the alternative name
							alt_name = next(infile).strip()
							#extract the line contents without the endline character
							#and add the alternative name to the dataframe
							pfam_df.loc[index, 'Alt_Name'] = alt_name
						if line.startswith("    <comment>"):
							#identify the portion of the xml file right before the longform description
							next(infile)
							#skip one line
							#then extract teh description from the next line, minus the endline character
							pfam_desc = next(infile).strip()
							#add the description of the PFam domain to the dataframe
							pfam_df.loc[index, 'PFam_Description'] = pfam_desc
						if line.startswith('      <category name="process"'):
							#identify the part of the XML file where GO information is stored
							#specifically for the Biological Process GOs
							newline = next(infile)
							while newline.startswith("        <term go_id"):
								#iterate over the lines that have GO information stored in them
								go_id_start = re.search('go_id="(.*)">', newline)
								go_id = go_id_start.group(1)
								#extract the GO ID & add it to the PFam dataframe
								pfam_df.loc[index, 'BiolProc_GO_ID'] = go_id
								go_desc_start = re.search('">(.*)</term>', newline)
								go_desc = go_desc_start.group(1)
								#extract the GO description and add it to the PFam dataframe
								pfam_df.loc[index, 'BiolProc_GO_Desc'] = go_desc
								#redefine the next line as newline, to continue the while loop
								newline = next(infile)
						if line.startswith('      <category name="component"'):
							#identify the part of the XML file where GO information is stored
							#specifically for the Cellular Component GOs
							newline = next(infile)
							while newline.startswith("        <term go_id"):
								#iterate over the lines that have GO information stored in them
								go_id_start = re.search('go_id="(.*)">', newline)
								go_id = go_id_start.group(1)
								#extract the GO ID & add it to the PFam dataframe
								pfam_df.loc[index, 'CellComp_GO_ID'] = go_id
								go_desc_start = re.search('">(.*)</term>', newline)
								go_desc = go_desc_start.group(1)
								#extract the GO description and add it to the PFam dataframe
								pfam_df.loc[index, 'CellComp_GO_Desc'] = go_desc
								#redefine the next line as newline, to continue the while loop
								newline = next(infile)
						if line.startswith('      <category name="function"'):
							#identify the part of the XML file where GO information is stored
							#specifically for the Molecular Function GOs
							newline = next(infile)
							while newline.startswith("        <term go_id"):
								#iterate over the lines that have GO information stored in them
								go_id_start = re.search('go_id="(.*)">', newline)
								go_id = go_id_start.group(1)
								#extract the GO ID & add it to the PFam dataframe
								pfam_df.loc[index, 'MolFunc_GO_ID'] = go_id
								go_desc_start = re.search('">(.*)</term>', newline)
								go_desc = go_desc_start.group(1)
								#extract the GO description and add it to the PFam dataframe
								pfam_df.loc[index, 'MolFunc_GO_Desc'] = go_desc
								#redefine the next line as newline, to continue the while loop
								newline = next(infile)
	else: 
		#if the reference XML file for the PFam domain doesn't exist
		#write the following message to the stdout
		print("Warning! XML reference file for PFam domain " + key + " does not exist!")


#Part 4: Write out results

if len(bad_pfams_list) > 0:
	#if bad PFam domains were encountered during the parsing process
	#create an output file for these PFam domains
	output_bad = output_base + "__BadPFams.txt"
	with open(output_bad, "w") as outfile: 
		#open the output file for writing
		for i in bad_pfams_list: 
			#iterate over the elements in the bad PFam domains list
			#and write them to the file, 1 PFam domain per line
			outfile.write(i + "\n")


#write out main pfam dataframe to a tab-separated text file
pfam_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#testing phase
#working in the /home/inf-47-2020/ThesisTrich/PFam_XML_Files/ directory
#model: 
python parse_OG_PFam_XML.py input_db data_path output_base
#applying it: 
python ../Scripts/parse_OG_PFam_XML.py MetamonadCtrl_sec_3_SP__CountPivot_PFamIndex__HEAD200.txt /home/inf-47-2020/ThesisTrich/PFam_XML_Files/All_PFam_SymLinks/ MetamonadCtrl_sec_3_SP__TEST
#the correct file was made
python ../Scripts/parse_OG_PFam_XML.py MetamonadCtrl_sec_3_SP__CountPivot_PFamIndex.txt /home/inf-47-2020/ThesisTrich/PFam_XML_Files/All_PFam_SymLinks/ MetamonadCtrl_sec_3_SP__TEST2
# Warning! The provided PFam ID, Autophagy_C, belongs to a dead family!
# Warning! The provided PFam ID, Autophagy_N, belongs to a dead family!
# Warning! The provided PFam ID, DLD, belongs to a dead family!
# Warning! The provided PFam ID, DUF1712, belongs to a dead family!
#the correct files were made

```


## ALE

### Creating OG MSAs

ALE requires Multiple Sequence Alignments (MSAs) of the input Orthologous Groups' protein sequences. Below is the methodology used for obtaining these MSAs. 

Script to extract OG data from Metamonad database (script saved to og_prot_list.py): 
 - Use the Metamonad database (or some modified version of it) as input
 - Use list of OGs as input/query
 - Include dictionary in script of Species IDs to FASTA file names
 - Extract: Encoded protein IDs of proteins associated with each OG, "official" Species ID, FASTA file name
 - Output file name should be something related to the OG name

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: og_prot_list.py
Date: 2022.05.14
Author: Virág Varga

Description:
	This program filters (a version of) the Metamonad database in order to extract
		the encoded protein query IDs and species IDs associated with a provided list
		of query OG IDs. For each input query OG ID, the program will output a
		results file in the format: Query\tSpecies_Id\t[OG_Program_OG_ID]

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; assigning command line arguments.
	2. Importing data into Pandas dataframe and creating OG ID to list of query protein
		IDs dictionary.
	3. Extracting information on proteins in query OGs, and writing out results to
		a tab-separated text file per OG.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file names are not user-defined.

Usage
	./og_prot_list.py input_db og_program og_ids [search_identifier]
	OR
	python og_prot_list.py input_db og_program og_ids [search_identifier]

	Where the input_db is a (possibly filtered) version of the Metamonad database
		including Query IDs, Species IDs, and OG IDs of orthologous clustering
		program whose OGs are being queried.
	Where og_program is the column name of the orthologous clustering program whose
		OG IDs are being queried.
	Where og_ids must be a query OG ID list in on of the following formats:
		- Singular OG ID provided on the command line
		- Comma-separated list of OG IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of OG IDs in format: ID1\nID2 etc.
	Where search_identifier should be a string describing the particular type of 
		data being filtered for (ex.: 'mito3'), that will be included in the output file
		name. 

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__filt_scores-startAA-pfam__HEAD200.txt"

og_program = sys.argv[2]
#og_program = "SonicParanoid_OG"
if og_program == "SonicParanoid_OG": 
	#if the program whose OGs are being queried is SonicParanoid
	#save a shortened version of the name to a variable
	#for use in the output file name
	og_program_short = "SP"
elif og_program == "OrthoFinder_OG": 
	#if the program whose OGs are being queried is OrthoFinder
	#save a shortened version of the name to a variable
	#for use in the output file name
	og_program_short = "OF"
else: 
	#if none of SP or OF are selected
	print("Incorrect orthologous clustering program selected. Please try again!")

og_ids = sys.argv[3]
#og_ids = "OG_25,OG_39"
#og_ids = "MetamonadCtrl_mito_3_SP_OGs_Tv_nonTvP.txt"
#import the query list
if os.path.isfile(og_ids):
	#if the input selection of OGs is a file
	with open(og_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
		#eliminate duplicates
		query_list = list(set(query_list))
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = og_ids.split(",")
	#eliminate duplicates
	query_list = list(set(query_list))


#Part 2: Import data into Pandas dataframe & create OG dictionary

#read in the Metamonad database to a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0)
#extract the relevant portions of the dataframe to a new dataframe
og_df = input_df[['Query', 'Species_Id', og_program]].copy()
#then remove those rows that are not associated with any OG
og_df = og_df.loc[og_df[og_program] != '-']


#create a dictionary in the format: og_dict[og_id] = list_of_proteins
#this will allow easier analysis
grouped_og_df = og_df.groupby(og_program)['Query'].apply(list).reset_index(name="OG_members")
#the proteins in the 'Query' column are grouped into lists according to the OG they belong to
#the new column of lists is named 'OG_members'
og_dict = grouped_og_df.set_index(og_program).to_dict()['OG_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 3: Extracting query OGs & writing out results

#create empty dictionary for query OG information
filt_og_dict = {}

for og_key in og_dict.keys():
	#iterate over the keys of the OG dictionary
	if og_key in query_list:
		#identify OG IDs that are in the list of queries
		#and populate those entries over into the new filtered dictionary
		filt_og_dict[og_key] = og_dict[og_key]


for filt_key in filt_og_dict.keys():
	#iterate over the query OG IDs in the filtered dictionary
	if len(sys.argv) == 5: 
		#if the user provides an identifier for the file
		search_identifier = sys.argv[4]
		#add the search_identifier to the output file name
		output_db = og_program_short + "_" + search_identifier + "__" + filt_key + ".txt"
	else: 
		#otherwise simply use the OG program and the OG ID in the output file name
		output_db = og_program_short + "__" + filt_key + ".txt"
	#create the output file for that OG
	prot_list = filt_og_dict[filt_key]
	#extract the query proteins associated with the OG into a list
	temp_df = og_df[og_df['Query'].isin(prot_list)].copy()
	#use `.isin()` to iterate over entire list of query protein IDs
	#use `.copy()` to ensure the dataframe is separate from the og_df
	#and now write out results to a tab-separated text file
	temp_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA_OG_DBs/ directory
#setting things up: 
ln -s /home/inf-47-2020/ThesisTrich/DB2_A_ignava/Metamonada_PFamAignava.txt .
ln -s /home/inf-47-2020/ThesisTrich/DB2_A_ignava/FinalFilt_S3M3/Alanta_mito_3_OF_OGs.txt .
ln -s /home/inf-47-2020/ThesisTrich/DB2_A_ignava/FinalFilt_S3M3/Alanta_mito_3_SP_OGs.txt .
ln -s /home/inf-47-2020/ThesisTrich/DB2_A_ignava/FinalFilt_S3M3/Alanta_sec_3_SP_OGs.txt .
ln -s /home/inf-47-2020/ThesisTrich/DB2_A_ignava/FinalFilt_S3M3/Alanta_sec_3_OF_OGs.txt .
#not like it matters much at this point, but symlinks do at least somewhat save on file storage space vs. copies of files
#model: 
python og_prot_list.py input_db og_program og_ids [search_identifier]
#applying it: 
#in Mito_OF/
python ../../../Scripts/og_prot_list.py ../Metamonada_PFamAignava.txt OrthoFinder_OG ../Alanta_mito_3_OF_OGs.txt Mito3
ls OF_Mito3__OG00* | wc -l
# 1664
#in Sec_OF/
python ../../../Scripts/og_prot_list.py ../Metamonada_PFamAignava.txt OrthoFinder_OG ../Alanta_sec_3_OF_OGs.txt Sec3
ls OF_Sec3__OG00* | wc -l
# 5290
#in Mito_SP/
python ../../../Scripts/og_prot_list.py ../Metamonada_PFamAignava.txt SonicParanoid_OG ../Alanta_mito_3_SP_OGs.txt Mito3
ls SP_Mito3__OG_* | wc -l
# 1430
#in Sec_SP/
python ../../../Scripts/og_prot_list.py ../Metamonada_PFamAignava.txt SonicParanoid_OG ../Alanta_sec_3_SP_OGs.txt Sec3
ls SP_Sec3__OG_* | wc -l
# 3877
#a quick comparison...
wc -l Alanta_*
#   1665 Alanta_mito_3_OF_OGs.txt
#   1431 Alanta_mito_3_SP_OGs.txt
#   5291 Alanta_sec_3_OF_OGs.txt
#   3878 Alanta_sec_3_SP_OGs.txt
#  12265 total
#accounting for the header line, that means this worked correctly! 
#so let's move on
###
#some simple statistics on these files: 
#in Mito_OF/
#average number of lines in file: 
#ref: https://askubuntu.com/questions/1258726/how-to-find-the-average-number-of-lines-in-a-directory
#takes into account the header line (subtracts it)
awk 'END{FNUM=ARGC-1; print (NR-FNUM)/FNUM}' OF_Mito3__OG00*.txt
# 40.5276
#minimum and maximum line counts (ie. protein numbers)
#ref: https://unix.stackexchange.com/questions/611910/get-minimum-and-maximum-line-count-from-files-within-a-directory
LC_ALL=C find -type f -exec wc -l {} + |
    awk '
        $2 != "total" {
            if (max=="" || $1>max) {max=$1; mxf=$2};
            if (min=="" || $1<min) {min=$1; mnf=$2};
        }
        END { printf "Min %d for %s, max %d for %s\n", min, mnf, max, mxf }
    '
# Min 3 for ./OF_Mito3__OG0059656.txt, max 2843 for ./OF_Mito3__OG0000000.txt
#keep in mind that this does not account for header lines, so:
#min of 2 proteins; max of 2842 proteins
#mode (most common file size)
#sed remove whitespace at start ref: https://stackoverflow.com/questions/34322188/removing-all-spaces-from-the-beginning-of-lines
#replace space with tab ref: https://stackoverflow.com/questions/1424126/replace-whitespaces-with-tabs-in-linux
wc -l OF_Mito3__OG00* | sed 's/^[[:space:]]*//g' | tr ' ' \\t > LineCountIndex_Mito_OF.txt
#ref: https://www.unix.com/shell-programming-and-scripting/265317-printing-most-frequent-string-column.html
awk '{if(count[$1]++ >= max) max = count[$1]} END {for ( i in count ) if(max == count[i]) print i, count[i] }' LineCountIndex_Mito_OF.txt
# 3 533
#where again, need to -1 because line count is going to include the header line
#so the most common length is 2 proteins/lines, with 533 occurrances
#in Mito_SP/
#average number of lines in file: 
#ref: https://askubuntu.com/questions/1258726/how-to-find-the-average-number-of-lines-in-a-directory
#takes into account the header line (subtracts it)
awk 'END{FNUM=ARGC-1; print (NR-FNUM)/FNUM}' SP_Mito3__OG_*.txt
# 34.8601
#minimum and maximum line counts (ie. protein numbers)
#ref: https://unix.stackexchange.com/questions/611910/get-minimum-and-maximum-line-count-from-files-within-a-directory
LC_ALL=C find -type f -exec wc -l {} + |
    awk '
        $2 != "total" {
            if (max=="" || $1>max) {max=$1; mxf=$2};
            if (min=="" || $1<min) {min=$1; mnf=$2};
        }
        END { printf "Min %d for %s, max %d for %s\n", min, mnf, max, mxf }
    '
# Min 3 for ./SP_Mito3__OG_37098.txt, max 820 for ./SP_Mito3__OG_2.txt
#keep in mind that this does not account for header lines, so:
#min of 2 proteins; max of 819 proteins
#mode (most common file size)
#sed remove whitespace at start ref: https://stackoverflow.com/questions/34322188/removing-all-spaces-from-the-beginning-of-lines
#replace space with tab ref: https://stackoverflow.com/questions/1424126/replace-whitespaces-with-tabs-in-linux
wc -l SP_Mito3__OG_* | sed 's/^[[:space:]]*//g' | tr ' ' \\t > LineCountIndex_Mito_SP.txt
#ref: https://www.unix.com/shell-programming-and-scripting/265317-printing-most-frequent-string-column.html
awk '{if(count[$1]++ >= max) max = count[$1]} END {for ( i in count ) if(max == count[i]) print i, count[i] }' LineCountIndex_Mito_SP.txt
# 3 535
#where again, need to -1 because line count is going to include the header line
#so the most common length is 2 proteins/lines, with 535 occurrances
#in Sec_OF/
#average number of lines in file: 
#ref: https://askubuntu.com/questions/1258726/how-to-find-the-average-number-of-lines-in-a-directory
#takes into account the header line (subtracts it)
awk 'END{FNUM=ARGC-1; print (NR-FNUM)/FNUM}' OF_Sec3__OG00*.txt
# 25.4493
#minimum and maximum line counts (ie. protein numbers)
#ref: https://unix.stackexchange.com/questions/611910/get-minimum-and-maximum-line-count-from-files-within-a-directory
LC_ALL=C find -type f -exec wc -l {} + |
    awk '
        $2 != "total" {
            if (max=="" || $1>max) {max=$1; mxf=$2};
            if (min=="" || $1<min) {min=$1; mnf=$2};
        }
        END { printf "Min %d for %s, max %d for %s\n", min, mnf, max, mxf }
    '
# Min 3 for ./OF_Sec3__OG0053183.txt, max 2843 for ./OF_Sec3__OG0000000.txt
#keep in mind that this does not account for header lines, so:
#min of 2 proteins; max of 2842 proteins
#mode (most common file size)
#sed remove whitespace at start ref: https://stackoverflow.com/questions/34322188/removing-all-spaces-from-the-beginning-of-lines
#replace space with tab ref: https://stackoverflow.com/questions/1424126/replace-whitespaces-with-tabs-in-linux
wc -l OF_Sec3__OG00* | sed 's/^[[:space:]]*//g' | tr ' ' \\t > LineCountIndex_Sec_OF.txt
#ref: https://www.unix.com/shell-programming-and-scripting/265317-printing-most-frequent-string-column.html
awk '{if(count[$1]++ >= max) max = count[$1]} END {for ( i in count ) if(max == count[i]) print i, count[i] }' LineCountIndex_Sec_OF.txt
# 3 1579
#where again, need to -1 because line count is going to include the header line
#so the most common length is 2 proteins/lines, with 1579 occurrances
#in Sec_SP/
#average number of lines in file: 
#ref: https://askubuntu.com/questions/1258726/how-to-find-the-average-number-of-lines-in-a-directory
#takes into account the header line (subtracts it)
awk 'END{FNUM=ARGC-1; print (NR-FNUM)/FNUM}' SP_Sec3__OG_*.txt
# 28.3193
#minimum and maximum line counts (ie. protein numbers)
#ref: https://unix.stackexchange.com/questions/611910/get-minimum-and-maximum-line-count-from-files-within-a-directory
LC_ALL=C find -type f -exec wc -l {} + |
    awk '
        $2 != "total" {
            if (max=="" || $1>max) {max=$1; mxf=$2};
            if (min=="" || $1<min) {min=$1; mnf=$2};
        }
        END { printf "Min %d for %s, max %d for %s\n", min, mnf, max, mxf }
    '
# Min 3 for ./SP_Sec3__OG_59949.txt, max 846 for ./SP_Sec3__OG_1.txt
#keep in mind that this does not account for header lines, so:
#min of 2 proteins; max of 845 proteins
#mode (most common file size)
#sed remove whitespace at start ref: https://stackoverflow.com/questions/34322188/removing-all-spaces-from-the-beginning-of-lines
#replace space with tab ref: https://stackoverflow.com/questions/1424126/replace-whitespaces-with-tabs-in-linux
wc -l SP_Sec3__OG_* | sed 's/^[[:space:]]*//g' | tr ' ' \\t > LineCountIndex_Sec_SP.txt
#ref: https://www.unix.com/shell-programming-and-scripting/265317-printing-most-frequent-string-column.html
awk '{if(count[$1]++ >= max) max = count[$1]} END {for ( i in count ) if(max == count[i]) print i, count[i] }' LineCountIndex_Sec_SP.txt
# 3 1275
#where again, need to -1 because line count is going to include the header line
#so the most common length is 2 proteins/lines, with 1275 occurrances
###

```

Script to create FASTA files for MSAs, with the encoded FASTA protein headers and sequence lines of all proteins in a given OG (script saved to extract_OG_MSA.py): 
 - Use the output OG data file of the previous program as input
 - (Can run this by looping over a directory containing the OG data files)
 - Use the file names column: open files and extract the protein headers and associated sequences, save them to a dictionary & then print out to a new FASTA with the OG ID in the file name

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: extract_OG_MSA.py
Date: 2022.05.16
Author: Virág Varga

Description:
	This program iterates over a file produced by the og_prot_list.py program to create
		a FASTA file containing the amino acid sequences of all proteins in a given
		orthologous group. This is intended for use as an input for a Multiple Sequence
		Alignment (MSA) program.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; assigning command line arguments.
	2. Importing data into Pandas dataframe and creating dictionary of FASTA files
		to protein sequences contained within them.
	3. Creating reference dictionary for file names and species IDs.
	4. Extracting sequence data from files to create MSA prep FASTA file.
	5. Writing out results in single-line FASTA formatting.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is based on the input file name.

Usage
	./extract_OG_MSA.py input_db data_path
	OR
	python extract_OG_MSA.py input_db data_path

	Where the input_db is file in the format:
		Query\tSpecies_ID\t[OG_PROGRAM]_OG
	Where data_path is a string input in quotation marks on the command line of the
		absolute path to the directory where the data FASTA files are located.
		This should include a trailing backslash.

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "TEST_SonicParanoid_OG__OG_25.txt"

data_path = sys.argv[2]
#data_path = "C:/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/ALE/"

#output file name should be based on input file name
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_fasta = out_full + "_MSAprep.fasta"


#Part 2: Import data into Pandas dataframe & create species dictionary

#read in the Metamonad database to a Pandas dataframe
input_df = pd.read_csv(input_db, sep = '\t', header=0)
#then extract the name of the OG
query_og_id = input_df.iloc[:, 2].to_list()[0]
#transform the data in the 'Species_Id' column into the actual file names
input_df['Species_Id'] = input_df['Species_Id'] + '_edit.fasta'


#create a dictionary in the format: species_dict[species_file_name] = list_of_proteins
#this will allow easier analysis
grouped_species_df = input_df.groupby('Species_Id')['Query'].apply(list).reset_index(name="Species_members")
#the proteins in the 'Query' column are grouped into lists according to the species FASTA file they are from
#the new column of lists is named 'Species_members'
species_dict = grouped_species_df.set_index('Species_Id').to_dict()['Species_members']
#the new dataframe is converted into a dictionary,
#where the OG IDs are the keys, and the lists of protein members of the OGs are the values


#Part 3: Create reference dictionary for file names and species IDs

species_fasta_ref_dict = {"Anaeramoeba_lanta_160522_edit.fasta": "A_lanta__", 
						  "BM_newprots_may21.anaeromoeba_edit.fasta": "A_ignava_BM__", 
						  "BS_newprots_may21.anaeromoeba_edit.fasta": "A_flamelloides_BS__", 
						  "Carpediemonas_membranifera.PRJNA719540_edit.fasta": "C_membranifera__", 
						  "Dientamoeba_fragilis.43352.aa_edit.fasta": "D_fragilis__", 
						  "EP00701_Giardia_intestinalis_edit.fasta": "G_intestinalis_A_EukProt__", 
						  "EP00703_Trepomonas_sp_PC1_edit.fasta": "Trepomonas_PC1__", 
						  "EP00708_Paratrimastix_pyriformis_edit.fasta": "P_pyriformis__", 
						  "EP00764_Aduncisulcus_paluster_edit.fasta": "A_paluster__", 
						  "EP00766_Chilomastix_caulleryi_edit.fasta": "C_caulleryi__", 
						  "EP00767_Chilomastix_cuspidata_edit.fasta": "C_cuspidata__", 
						  "EP00768_Dysnectes_brevis_edit.fasta": "D_brevis__", 
						  "EP00769_Ergobibamus_cyprinoides_edit.fasta": "E_cyprinoides__", 
						  "EP00770_Monocercomonoides_exilis_edit.fasta": "M_exilis__", 
						  "EP00771_Trimastix_marina_edit.fasta": "T_marina__", 
						  "EP00792_Barthelona_sp_PAP020_edit.fasta": "Barthelona_PAP020__", 
						  "GiardiaDB_GintestinalisADH_edit.fasta": "G_intestinalis_ADH__", 
						  "GiardiaDB_GintestinalisBGS_edit.fasta": "G_intestinalis_BGS__", 
						  "GiardiaDB_GintestinalisBGS_B_edit.fasta": "G_intestinalis_BGS_B__", 
						  "GiardiaDB_GintestinalisEP15_edit.fasta": "G_intestinalis_EP15__", 
						  "Giardia_intestinalis.PRJNA1439_edit.fasta": "G_intestinalis_A_NCBI__", 
						  "Giardia_muris.PRJNA524057_edit.fasta": "G_muris__", 
						  "Histomonas_meleagridis.135588.aa_edit.fasta": "H_meleagridis_OLD__", 
						  "Histomonas_meleagridis.PRJNA594289_edit.fasta": "H_meleagridis_NEW__", 
						  "Kipferlia_bialata.PRJDB5223_edit.fasta": "K_bialata__", 
						  "Pentatrichomonas_hominis.5728.aa_edit.fasta": "P_hominis__", 
						  "SC_newprots_may21.anaeromoeba_edit.fasta": "A_flamelloides_SC__", 
						  "Spironucleus_salmonicida.PRJNA60811_edit.fasta": "S_salmonicida__", 
						  "Tetratrichomonas_gallinarum.5730.aa_edit.fasta": "T_gallinarum__", 
						  "Trichomonas_foetus.PRJNA345179_edit.fasta": "T_foetus__", 
						  "Trichomonas_vaginalis_GenBank.PRJNA16084_edit.fasta": "T_vaginalis_GenBank__", 
						  "Trichomonas_vaginalis_RefSeq.G3_edit.fasta": "T_vaginalis_RefSeq__"}


#Part 4: Extract sequence data from files to create MSA prep FASTA file

#create new empty dictionary for protein IDs and protein sequences
seq_dict = {}

for key in species_dict.keys():
	#iterate over the dictionary via its keys (ie. file names)
	file_path = data_path + key
	#define the full file name including the path
	spp_prot_list = species_dict[key]
	#pull out the query protein IDs into a list
	with open(file_path, 'r') as infile:
		#open the species FASTA proteome file for reading
		for line in infile:
			#iterate over the file line by line
			for prot in spp_prot_list:
				#iterate over the list of query prot IDs
				if prot in line:
					#if the query protein is found in the line
					header_start = line[1:].strip()
					#remove the ">" header line indicator and strip the endline character
					for ref_species_key in species_fasta_ref_dict.keys():
						#iterate over the FASTA reference dictionary
						if key == ref_species_key: 
							#find the location of the species FASTA name reference
							#and copy the header version of the species ID to a variable
							species_header_name = species_fasta_ref_dict[ref_species_key]
					header = species_header_name + header_start
					#create the new FASTA header as a combination of the original encoded protein ID
					#and the shortened species ID name designed for the header
					seq_line = next(infile).strip()
					#identify the sequence line associated with the header
					#and remove the endline character
					#finally, add the header and protein sequence to the dictionary
					seq_dict[header] = seq_line


#Part 5: Write out results to new FASTA file

with open(output_fasta, "w") as outfile:
	#open the output file for writing
	for prot_key in seq_dict.keys():
		#iterate over the encoded protein headers
		#and write the header and sequence lines out in FASTA format
		outfile.write(">" + prot_key + "\n" + seq_dict[prot_key] + "\n")

```

Using it: 

```bash
#working in the / directory
#model: 
python extract_OG_MSA.py input_db data_path
#applying it:
#first, testing...
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/ directory
python ../../Scripts/extract_OG_MSA.py ../Input_FASTA_OG_DBs/Sec_SP/SP_Sec3__OG_9981.txt "/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/"
#looks like it works! 
#so now time to loop it
#in Mito_OF/
ls ../../Input_FASTA_OG_DBs/Mito_OF/OF_Mito3__OG00*.txt | while read file; do
	python ../../../Scripts/extract_OG_MSA.py $file "/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/"; 
done
ls OF_Mito3__OG00* | wc -l
# 1664
#in Mito_SP/
ls ../../Input_FASTA_OG_DBs/Mito_SP/SP_Mito3__OG_*.txt | while read file; do
	python ../../../Scripts/extract_OG_MSA.py $file "/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/"; 
done
ls SP_Mito3__OG_* | wc -l
# 1430
#in Sec_OF/
ls ../../Input_FASTA_OG_DBs/Sec_OF/OF_Sec3__OG00*.txt | while read file; do
	python ../../../Scripts/extract_OG_MSA.py $file "/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/"; 
done
ls OF_Sec3__OG00* | wc -l
# 5290
#in Sec_SP/
ls ../../Input_FASTA_OG_DBs/Sec_SP/SP_Sec3__OG_*.txt | while read file; do
	python ../../../Scripts/extract_OG_MSA.py $file "/home/inf-47-2020/ThesisTrich/DataFiles/EncodedData/"; 
done
ls SP_Sec3__OG_* | wc -l
# 3877
#a quick comparison...
wc -l Alanta_*
#   1665 Alanta_mito_3_OF_OGs.txt
#   1431 Alanta_mito_3_SP_OGs.txt
#   5291 Alanta_sec_3_OF_OGs.txt
#   3878 Alanta_sec_3_SP_OGs.txt
#  12265 total
#alright, looks good to go! 

```

## MAFFT

The multiple sequence alignments will be created using the MAFFT software. 

```bash
conda activate env-MAFFT
mafft -v
# /home/inf-47-2020/miniconda3/envs/env-MAFFT/bin/mafft: Cannot open -v.
# ------------------------------------------------------------------------------
#   MAFFT v7.505 (2022/Apr/10)
#   https://mafft.cbrc.jp/alignment/software/
#   MBE 30:772-780 (2013), NAR 30:3059-3066 (2002)
# ------------------------------------------------------------------------------
# High speed:
#   % mafft in > out
#   % mafft --retree 1 in > out (fast)
# High accuracy (for <~200 sequences x <~2,000 aa/nt):
#   % mafft --maxiterate 1000 --localpair  in > out (% linsi in > out is also ok)
#   % mafft --maxiterate 1000 --genafpair  in > out (% einsi in > out)
#   % mafft --maxiterate 1000 --globalpair in > out (% ginsi in > out)
# If unsure which option to use:
#   % mafft --auto in > out

# --op # :         Gap opening penalty, default: 1.53
# --ep # :         Offset (works like gap extension penalty), default: 0.0
# --maxiterate # : Maximum number of iterative refinement, default: 0
# --clustalout :   Output: clustal format, default: fasta
# --reorder :      Outorder: aligned, default: input order
# --quiet :        Do not report progress
# --thread # :     Number of threads (if unsure, --thread -1)
# --dash :         Add structural information (Rozewicki et al, submitted)
#ok so `-v` doesn't seem to exist which is kind of funny
#but even funnier that it grabbed the error page which DOES contain that info, lol

```

Usage: 
 - From the manual page, it seems like the best option for us comes from the "Accuracy-oriented methods" section:
   - "L-INS-i (probably most accurate; recommended for <200 sequences; iterative refinement method incorporating local pairwise alignment information)"
   - `mafft --localpair --maxiterate 1000 input [> output]`
   - `linsi input [> output]`
   - Unclear if the above are *alternatives* to one another, or need to be run in order, one after the other
   - So I think I need to use the first line as per: https://mafft.cbrc.jp/alignment/software/linuxportable.html

```bash
#running MAFFT, within the env-MAFFT conda environment
#linsi method: probably most accurate; recommended for <200 sequences; iterative refinement method incorporating local pairwise alignment information
#All pairwise alignments are computed with the Smith-Waterman algorithm. More accurate but slower than --6merpair
#in Mito_SP/
ls /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/Mito_SP/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	file_msa_base="${file_base%_MSAprep}"; #this removes the "_MSAprep" substring
	mafft --localpair --maxiterate 1000 --amino --quiet $file > ${file_msa_base}_MSA.fasta;
done &
# [1] 2123806
#this is the shortest one of the 4 (fewest OGs)
#but 5 hours in it's at 174/1430
#meaning a rough estimate says 41 hrs (a little under 2 full days) to completion
#so I need a different solution for the others
ls SP_Mito3__OG_* | wc -l
# 1435
#huh.
#a quick `ls` tells me there's multiple SP_Mito3__OG_1404_MSA.fasta.* files
#maybe this run didn't go quite right? we can try it again
#wait, nevermind, I figured it out
ls SP_Mito3__OG_*.fasta | wc -l
# 1430
#IQ-TREE puts results files in the directory of the source file, not the directory you run the command in -_-
#so we're fine, and this is good to go
#in Mito_OF/
###
#saving the following to mito_OF3_Round0.sh
ls /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/Mito_OF/OF_Mito3__OG000*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	file_msa_base="${file_base%_MSAprep}"; #this removes the "_MSAprep" substring
	mafft --localpair --maxiterate 1000 --amino --quiet $file > ${file_msa_base}_MSA.fasta;
done &
#saving similar scripts with different "Round#" to signify different portions of the OG list 
#and then running those scripts as below, with nohup
###
#ref: https://unix.stackexchange.com/questions/347541/syntax-error-while-trying-to-set-nohup-shell-loop
chmod +x mito_OF3_Round*
nohup ./mito_OF3_Round0.sh
# nohup: ignoring input and appending output to 'nohup.out'
#multiple nohups ref: https://stackoverflow.com/questions/4549489/can-i-change-the-name-of-nohup-out
nohup ./mito_OF3_Round2.sh &> nohup2.out &
# [1] 2195454
# [1]+  Done                    nohup ./mito_OF3_Round2.sh &> nohup2.out
#so as abserved below, I get the done message, but the process is actually still ongoing
#guess it's just a weirdness of combining nohup and background processes
nohup ./mito_OF3_Round4.sh &> nohup4.out &
# [1] 2230045
nohup ./mito_OF3_Round6.sh &> nohup6.out &
# [1] 2241523
nohup ./mito_OF3_Round1.sh &> nohup1.out &
# [1] 2250497
nohup ./mito_OF3_Round3.sh &> nohup3.out &
# [1] 2331408
nohup ./mito_OF3_Round5.sh &> nohup5.out &
# [1] 2342650
nohup ./mito_OF3_Round7.sh &> nohup7.out &
# [1] 2354032
ls OF_Mito3__OG00* | wc -l
# 1664
find . -type f -empty
# ./nohup.out
#good! movign on!!!
#in Sec_OF/
ls /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/Sec_OF/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	file_msa_base="${file_base%_MSAprep}"; #this removes the "_MSAprep" substring
	mafft --localpair --maxiterate 1000 --amino --quiet $file > ${file_msa_base}_MSA.fasta;
done &
#variation of script above in smaller run scripts
chmod +x sec_OF3_Round*
nohup ./sec_OF3_Round0.sh
# nohup: ignoring input and appending output to 'nohup.out'
nohup ./sec_OF3_Round2.sh &> nohup2.out &
# [1] 2425403
nohup ./sec_OF3_Round4.sh &> nohup4.out &
# [1] 2885391
nohup ./sec_OF3_Round6.sh &> nohup6.out &
# [1] 2895457
nohup ./sec_OF3_Round1.sh &> nohup1.out &
# [1] 2962175
nohup ./sec_OF3_Round3.sh &> nohup3.out &
# [1] 2965871
nohup ./sec_OF3_Round5.sh &> nohup5.out &
# [1] 2977946
nohup ./sec_OF3_Round7.sh &> nohup7.out &
# [1] 3000707
#the following run raised an error: 
#the debugging process is shown below
mafft --localpair --maxiterate 1000 --amino --quiet /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/V2_OF_Sec3__OG0003668_MSAprep.fasta > V2_OF_Sec3__OG0003668_MSA.fasta
#this file really does not want to make an alignment. Fascinating. 
#tried it on the MAFFT web service and got a clue
grep -v ">" *.fasta | grep "U"
# OF_Sec3__OG0003668_MSAprep.fasta:MEIKVVMCQSUGSMPRFKGLQAFLKGKLGVEAAFEYGALGDFEVFANGTLVFSKAKEGAYPSPPAVLAAIEALGA
# OF_Sec3__OG0003668_MSAprep.fasta:MEIKVVMCQSUGSMPRFKGLQAFLKGKLGVEAAFEYGALGDFEVFANGTLVFSKAKEGAYPSPPAVLAAIEALGA
#Wild! Let's try taking out the U
#removed Us with nano
mafft --localpair --maxiterate 1000 --amino --quiet /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/OF_Sec3__OG0003668_noU_MSAprep.fasta > OF_Sec3__OG0003668_noU_MSA.fasta
#that worked!!! brilliant
#in Sec_SP/
ls /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/Sec_SP/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	file_msa_base="${file_base%_MSAprep}"; #this removes the "_MSAprep" substring
	mafft --localpair --maxiterate 1000 --amino --quiet $file > ${file_msa_base}_MSA.fasta;
done &
#variation of script above in numbered mini-scripts
chmod +x sec_SP3_Round*
nohup ./sec_SP3_Round1.sh
# nohup: ignoring input and appending output to 'nohup.out'
nohup ./sec_SP3_Round3.sh &> nohup3.out &
# [1] 2179560
nohup ./sec_SP3_Round5.sh &> nohup5.out &
# [1] 2189665
#I can already see that some of these are ending up empty??? -_-;;
find . -type f -empty
# ./SP_Sec3__OG_504_MSA.fasta
# ./SP_Sec3__OG_109_MSA.fasta
# ./SP_Sec3__OG_308_MSA.fasta
# ./nohup.out
#let's try rerunning one of these manually
#mafft --localpair --maxiterate 1000 --amino --quiet /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/Sec_SP/SP_Sec3__OG_504_MSAprep.fasta > SP_Sec3__OG_504_MSA.fasta
#oh wait, nevermind!!!
find . -type f -empty
# ./SP_Sec3__OG_30_MSA.fasta
# ./SP_Sec3__OG_109_MSA.fasta
# ./SP_Sec3__OG_509_MSA.fasta
# ./nohup.out
#running the command again just a little later gets different results! 
#so even though I already got the done messages, it's still running
#weird, but... whatever.
nohup ./sec_SP3_Round7.sh &> nohup7.out &
# [1] 2423400
nohup ./sec_SP3_Round9.sh &> nohup9.out &
# [1] 2507868
nohup ./sec_SP3_Round2.sh &> nohup2.out &
# [1] 2508123
nohup ./sec_SP3_Round4.sh &> nohup4.out &
# [1] 2567508
nohup ./sec_SP3_Round6.sh &> nohup6.out &
# [1] 2568420
nohup ./sec_SP3_Round8.sh &> nohup8.out &
# [1] 2585451
ls SP_Sec3__OG_* | wc -l
# 3877
#which means all files exist!
#but...
find . -type f -empty
# ./SP_Sec3__OG_3_MSA.fasta
# ./SP_Sec3__OG_1_MSA.fasta
# ./nohup.out
wc -l SP_Sec3__OG_1.txt
# 846 SP_Sec3__OG_1.txt
wc -l SP_Sec3__OG_3.txt
# 699 SP_Sec3__OG_3.txt
#so it's fine, both of these are massive files, and they'll take a while to align
#just gotta wait it out
find . -type f -empty

```


## Alignment trimming & filtration

### Sequence number filtration

Alignements of clusters with less than 4 sequences were removed at this stage. 

```bash
#Mito_OF/
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Mito_OF/ directory
#ref: https://stackoverflow.com/questions/20360151/using-if-within-a-while-loop-in-bash
#ref: https://stackoverflow.com/questions/54108174/how-to-find-files-containing-a-string-n-times-or-more-often-using-egrep
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' OF_Mito3__OG00* > 4seq_Mito_OF_Index.txt
#double check that worked...
while read line; do
	grep -c ">" $line; 
done < 4seq_Mito_OF_Index.txt
#yup all values are >=4
wc -l 4seq_Mito_OF_Index.txt
# 999 4seq_Mito_OF_Index.txt
#moving over to work in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_OF/Input_Files/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Mito_OF/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Mito_OF/4seq_Mito_OF_Index.txt
#quick check
ls OF_Mito3__OG00* | wc -l
# 999
#Mito_SP/
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Mito_SP/ directory
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' SP_Mito3__OG_* > 4seq_Mito_SP_Index.txt
#double check that worked...
while read line; do
	grep -c ">" $line; 
done < 4seq_Mito_SP_Index.txt
#yup all values are >=4
wc -l 4seq_Mito_SP_Index.txt
# 830 4seq_Mito_SP_Index.txt
#for comparison...
ls SP_Mito3__OG_* | wc -l
# 1430
#moving over to work in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_SP/Input_Files/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Mito_SP/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Mito_SP/4seq_Mito_SP_Index.txt
#just to check...
ls SP_Mito3__OG_* | wc -l
# 830
#we're good to go
#Sec_OF/
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Sec_OF/ directory
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' OF_Sec3__OG00* > 4seq_Sec_OF_Index.txt
#double check that worked...
while read line; do
	grep -c ">" $line; 
done < 4seq_Sec_OF_Index.txt
#yup all values are >=4
wc -l 4seq_Sec_OF_Index.txt
# 3223 4seq_Sec_OF_Index.txt
#for comparison: 
ls OF_Sec3__OG00* | wc -l
# 5291
#moving over to work in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_OF/Input_Files/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Sec_OF/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Sec_OF/4seq_Sec_OF_Index.txt
ls OF_Sec3__OG00* | wc -l
# 3223
#Sec_SP/
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Sec_SP/ directory
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' SP_Sec3__OG_* > 4seq_Sec_SP_Index.txt
#double check that worked...
while read line; do
	grep -c ">" $line; 
done < 4seq_Sec_SP_Index.txt
#yup all values are >=4
wc -l 4seq_Sec_SP_Index.txt
# 2386 4seq_Sec_SP_Index.txt
#for comparison: 
ls SP_Sec3__OG_* | wc -l
# 3877
#moving over to work in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/Input_Files/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Sec_SP/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/MAFFT_MSA/MAFFT_Outputs/Sec_SP/4seq_Sec_SP_Index.txt
#and checking that it worked
ls SP_Sec3__OG_* | wc -l
# 2386
#good, we're good to go

```

### trimAl

The TrimAl program is used to trim the alignments. 

```bash
#program usage
trimal --version
# trimAl v1.4.rev15 build[2013-12-17]
#building the model:
trimal -in <inputfile> -out <outputfile> -(other options) 
trimal -in <inputfile> -out <outputfile> -gappyout -sgt -sct > ${file}.log
#`-fasta`: output file should be in FASTA format
#unnecessary because default is to keep input file format in output
#`-gappyout`: Use automatic selection on "gappyout" mode. This method only uses information based on gaps' distribution.
#`-sgt`: Print accumulated gap percentage count.
#`-sct`: Print accumulated conservation values count.
#testing it: 
trimal -in Input_Files/SP_Mito3__OG_1441_MSA.fasta -out SP_Mito3__OG_1441_MSA_trim.fasta -gappyout -sgt -sct > SP_Mito3__OG_1441_MSA.fasta.log
# ERROR: Parameter "-sct" not valid.
#played around with the above, but that command just doesn't appear to be working. oh well. 
trimal -in Input_Files/SP_Mito3__OG_1441_MSA.fasta -out SP_Mito3__OG_1441_MSA_trim.fasta -gappyout -sgt > SP_Mito3__OG_1441_MSA.fasta.log
#this worked! try something else real quick...
trimal -in Input_Files/SP_Mito3__OG_1441_MSA.fasta -out SP_Mito3__OG_1441_MSA_trim.fasta -gappyout -sgt > Gap_Files/SP_Mito3__OG_1441_MSA_gapLog.txt
#great! that worked as I'd hoped for
#Mito_OF/
ls Input_Files/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	trimal -in $file -out Trimmed_Files/${file_base}_trim.fasta -gappyout -sgt > Gap_Files/${file_base}_gapLog.txt; 
done
#sequences composed entirely of gaps are removed entirely, and a warning message about these is printed to the stdout
#copied the text and saved it in the file: Removed_seq_log_trimAl_MitoOF.txt
ls Trimmed_Files/OF_Mito3__OG00* | wc -l
# 999
#Mito_SP/
ls Input_Files/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	trimal -in $file -out Trimmed_Files/${file_base}_trim.fasta -gappyout -sgt > Gap_Files/${file_base}_gapLog.txt; 
done
#sequences composed entirely of gaps are removed entirely, and a warning message about these is printed to the stdout
#copied the text and saved it in the file: Removed_seq_log_trimAl_MitoSP.txt
ls Trimmed_Files/*.fasta | wc -l
# 830
#Sec_OF/
ls Input_Files/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	trimal -in $file -out Trimmed_Files/${file_base}_trim.fasta -gappyout -sgt > Gap_Files/${file_base}_gapLog.txt; 
done
#sequences composed entirely of gaps are removed entirely, and a warning message about these is printed to the stdout
#copied the text and saved it in the file: Removed_seq_log_trimAl_SecOF.txt
ls Trimmed_Files/OF_Sec3__OG00* | wc -l
# 3223
#Sec_SP/
ls Input_Files/*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	trimal -in $file -out Trimmed_Files/${file_base}_trim.fasta -gappyout -sgt > Gap_Files/${file_base}_gapLog.txt; 
done
#sequences composed entirely of gaps are removed entirely, and a warning message about these is printed to the stdout
#copied the text and saved it in the file: Removed_seq_log_trimAl_SecSP.txt
ls Trimmed_Files/SP_Sec3__OG_* | wc -l
# 2386

```

#### Linking results FASTA files

The trimmed alignment files need to be linked into the relevant IQ-tree directories. Another round of file filtration is done here, since sequences were removed from alignment files, and so it is possible that OGs which previously had >=4 sequences in them, now have less. 

```bash
#Mito_OF/
#working in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_OF/ directory
#ref: https://stackoverflow.com/questions/20360151/using-if-within-a-while-loop-in-bash
#ref: https://stackoverflow.com/questions/54108174/how-to-find-files-containing-a-string-n-times-or-more-often-using-egrep
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' OF_Mito3__OG00* > 4seqTrim_Mito_OF_Index.txt
wc -l 4seqTrim_Mito_OF_Index.txt
# 998
#moving over to work in the /home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Mito_OF/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_OF/Trimmed_Files/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_OF/4seqTrim_Mito_OF_Index.txt
#double checking that it worked...
ls OF_Mito3__OG00* | wc -l
# 998
#good to go
#Mito_SP/
#working in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_SP/ directory
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' SP_Mito3__OG_* > 4seqTrim_Mito_SP_Index.txt
wc -l 4seqTrim_Mito_SP_Index.txt
# 830 4seqTrim_Mito_SP_Index.txt
#vs
# 830 4seq_Mito_SP_Index.txt
#for comparison...
ls SP_Mito3__OG_* | wc -l
# 1430
#moving over to work in the /home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Mito_SP/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_SP/Trimmed_Files/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_SP/4seqTrim_Mito_SP_Index.txt
#just to check...
ls SP_Mito3__OG_* | wc -l
# 830
#we're good to go
#Sec_OF/
#working in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_OF/ directory
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' OF_Sec3__OG00* > 4seqTrim_Sec_OF_Index.txt
wc -l 4seqTrim_Sec_OF_Index.txt
# 3219 4seqTrim_Sec_OF_Index.txt
#moving over to work in the /home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Sec_OF/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_OF/Trimmed_Files/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_OF/4seqTrim_Sec_OF_Index.txt
ls OF_Sec3__OG00* | wc -l
# 3219
#Sec_SP/
#working in the /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/ directory
awk 'FNR==1{n=0} />/{++n} n>3{print FILENAME; nextfile}' SP_Sec3__OG_* > 4seqTrim_Sec_SP_Index.txt
wc -l 4seqTrim_Sec_SP_Index.txt
# 2386 4seqTrim_Sec_SP_Index.txt
#vs
# 2386 4seq_Sec_SP_Index.txt
#for comparison: 
ls SP_Sec3__OG_* | wc -l
# 3877
#moving over to work in the /home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Sec_SP/ directory
while read line; do
	ln -s /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/Trimmed_Files/${line} . ; 
done < /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/4seqTrim_Sec_SP_Index.txt
#and checking that it worked
ls SP_Sec3__OG_* | wc -l
# 2386
#good, we're good to go

```


## Sample of gene trees with IQ-TREE

The IQ-TREE program will be used to create the sample of gene trees used as input for ALE.

```bash
conda activate env-IQ-TREE
#the model command is based on the methods in Martijn et al. 2020
nohup iqtree -s SP_Sec3__OG_990_MSA_trim.fasta \
	--prefix SP_Sec3__OG_990_MSA_IQ \
	-m MFP \
	-B 1000 -bnni \
	-mset LG \
	-madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 \
	-seed 12345 \
	-wbtl \
	-T AUTO -ntmax 15 &
#-s specify the name of the alignment file (always required by IQ-TREE to work)
#--prefix Output file basename
#-m specify the model name to use during the analysis. 
#The special MFP key word stands for ModelFinder Plus, which tells IQ-TREE to perform ModelFinder and the remaining analysis using the selected model.
#-B specifies the number of bootstrap replicates where 1000 is the minimum number recommended
#-bnni reduce the risk of overestimating branch supports with UFBoot due to severe model violations
#-wbtl Like -wbt but bootstrap trees written with branch lengths. 
#-mset Specify the name of a program (raxml, phyml or mrbayes) to restrict to only those models supported by the specified program. Alternatively, one can specify a comma-separated list of base models. For example, -mset WAG,LG,JTT will restrict model selection to WAG, LG, and JTT instead of all 18 AA models to save computations
#-seed Specify a random number seed to reproduce a previous run. This is normally used for debugging purposes. DEFAULT: based on current machine clock
#-keep-ident Keep identical sequences in the alignment. Bu default: IQ-TREE will remove them during the analysis and add them in the end.
#-madd Specify a comma-separated list of mixture models to additionally consider for model selection. For example, -madd LG4M,LG4X to additionally include these two protein mixture models.
#according to Courtney, I should use the Martijn et al. 2020 paper's model, but without the -keep-ident since I'm going to have a lot of indentical sequences
#-T specify the number of threads/cores to use (parallel processing)
#the AUTO option allows the program to determine teh ideal number of cores
#-ntmax set the maximum number of cores to be used
###
#Please note that below is a simplified workflow
#in the interest of time, the largest OGs were analyzed in individual runs, rather than in loops
#for ex: 
nohup iqtree -s OF_Mito3__OG0000000_MSA_trim.fasta --prefix OF_Mito3__OG0000000_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Mito3__OG0000000.out &
#other OGs were run in loops, like so: 
nohup ls OF_Mito3__OG000002*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	mkdir $dir_base; #create a directory based on the file name
	iqtree -s $file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $file ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}; #moving the results files into the OG directory
done &> nohup_20.out &
# [9] 999403
# nohup: ignoring input and redirecting stderr to stdout
#this was repeated in loops containing varying numbers of OGs, for all 4 main filtrations: 
#Mito_OF, Mito_SP, Sec_OF, & Sec_SP
#a few remnant OGs were also run as shell scripts, after a server failure interrupted a number of loops: 
#Mito_SP/
chmod +x *.sh
nohup ./remnant_IQ-TREE_3.sh &> nohup_remnant_IQ-TREE_3.out &
# [1] 45468
nohup ./remnant_IQ-TREE_4.sh &> nohup_remnant_IQ-TREE_4.out &
# [2] 45729
nohup ./remnant_IQ-TREE_5.sh &> nohup_remnant_IQ-TREE_5.out &
# [3] 45991
#and remnants that I *still* missed: 
nohup iqtree -s SP_Mito3__OG_5773_MSA_trim.fasta --prefix SP_Mito3__OG_5773_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Mito3__OG_5773.out &
# [1] 861085
#final checks...
ls *_trim.fasta | wc -l
# 830
tree | tail
# │   ├── SP_Mito3__OG_998_MSA_IQ.log
# │   ├── SP_Mito3__OG_998_MSA_IQ.mldist
# │   ├── SP_Mito3__OG_998_MSA_IQ.model.gz
# │   ├── SP_Mito3__OG_998_MSA_IQ.splits.nex
# │   ├── SP_Mito3__OG_998_MSA_IQ.treefile
# │   ├── SP_Mito3__OG_998_MSA_IQ.ufboot
# │   └── SP_Mito3__OG_998_MSA_trim.fasta
# └── SP_Mito3__OG_998_MSA_trim.fasta -> /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_SP/Trimmed_Files/SP_Mito3__OG_998_MSA_trim.fasta
# 829 directories, 11034 files
#oh no... that means there's one missing!!!
ls SP_Mito3__OG_*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	if [ ! -d "${dir_base}" ] ; then
		echo ${full_file} ; 
	fi ; 
done
# SP_Mito3__OG_54_MSA_trim.fasta
nohup iqtree -s SP_Mito3__OG_54_MSA_trim.fasta --prefix SP_Mito3__OG_54_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Mito3__OG_54.out &
# [1] 866009
#check one more thing...
find . –type d -empty
#I don't this is working as intended???
#but I did happen on an ENTIRELY DIFFERENT ERROR 
#I swear I'm gonna lose my fucking mind
grep -l "ERROR: Cannot write to file" */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/'
# SP_Mito3__OG_305_MSA_trim.fasta
#ok well thankfully this seems only to have been a problem once, then
#so let's rerun this file
nohup iqtree -s SP_Mito3__OG_305_MSA_trim.fasta --prefix SP_Mito3__OG_305_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Mito3__OG_305.out &
# [2] 942649
#AND SOMETHING ELSE OH MY FUCKING GODS
#the SP_Mito3__OG_2/ has a BUNCH of files that don't belong there????
#primarily *_trim.fasta files, but a number of IQ-TREE results files, too...
#ref: https://superuser.com/questions/617050/find-directories-containing-a-certain-number-of-files
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF>=13'
# . 1753
# ./SP_Mito3__OG_2 838
# ./SP_Mito3__OG_503 13
# ./SP_Mito3__OG_21 13
# ./SP_Mito3__OG_5 13
# ./SP_Mito3__OG_401 13
#checked and at least one of the 13's have 1 of my zip folders, so that's fine
#so at least whatever this is, it only happened once
mv SP_Mito3__OG_2 ERROR_SP_Mito3__OG_2
mkdir SP_Mito3__OG_2
cp ERROR_SP_Mito3__OG_2/SP_Mito3__OG_2_* SP_Mito3__OG_2/
mv ERROR_SP_Mito3__OG_2/ ../Error_Files/
#based on the above, new way to find empty directories, since the first way didn't work: 
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF<=1'
# ./SP_Mito3__OG_5889 1
# ./SP_Mito3__OG_305 1
#already working on 305 (seen above), but need to deal with 5889 then
#except nope, it's fine! this contains a Pre-Keep-Ident zip file
#which actually is a good catch, not a great idea to zip yet
#unzipped files back into the directory
tail */*.log | grep "ERROR: "
#revealed only errors related to the <4 sequences issue
#also double-checked the TrimAl results
wc -l 4seqTrim_Mito_SP_Index.txt
# 830 4seqTrim_Mito_SP_Index.txt
#so it matches
#real quick, let's double back for final checks
#Mito_OF/
ls *_trim.fasta | wc -l
# 998
tree | tail
# │   ├── OF_Mito3__OG0025016_MSA_IQ.mldist
# │   ├── OF_Mito3__OG0025016_MSA_IQ.model.gz
# │   ├── OF_Mito3__OG0025016_MSA_IQ.splits.nex
# │   ├── OF_Mito3__OG0025016_MSA_IQ.treefile
# │   ├── OF_Mito3__OG0025016_MSA_IQ.ufboot
# │   └── OF_Mito3__OG0025016_MSA_trim.fasta
# ├── OF_Mito3__OG0025016_MSA_trim.fasta -> /home/inf-47-2020/ThesisTrich/TrimAl_Results/Mito_OF/Trimmed_Files/OF_Mito3__OG0025016_MSA_trim.fasta
# └── --prefix.log
# 998 directories, 12406 files
#good!!! all FASTA files have associated directories
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF>=13'
# . 2062
#good!
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF<=1'
#no results - good! 
grep -l "ERROR: Cannot write to file" */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/'
# OF_Mito3__OG0000000_MSA_trim.fasta
# OF_Mito3__OG0000001_MSA_trim.fasta
# OF_Mito3__OG0000002_MSA_trim.fasta
# OF_Mito3__OG0000008_MSA_trim.fasta
# OF_Mito3__OG0000014_MSA_trim.fasta
# OF_Mito3__OG0000018_MSA_trim.fasta
# OF_Mito3__OG0023715_MSA_trim.fasta
#oh good fucking Gods...
#but, hmmm... the log file in the first one looks fine, actually???
#oh wait, ok, I think I see what's happened: 
#the log files record everything that happened during the process 
#and since IQ-TREE uses checkpoints, trees that failed out but then restart and run will still have error messages in the log file
#OF_Mito3__OG0023715 did actually fail out, however
nohup iqtree -s OF_Mito3__OG0023715_MSA_trim.fasta --prefix OF_Mito3__OG0023715_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Mito3__OG0023715.out &
# [2] 1021325
grep -l "ERROR: " */*.log
# OF_Mito3__OG0000000/OF_Mito3__OG0000000_MSA_IQ.log
# OF_Mito3__OG0000001/OF_Mito3__OG0000001_MSA_IQ.log
# OF_Mito3__OG0000002/OF_Mito3__OG0000002_MSA_IQ.log
# OF_Mito3__OG0000008/OF_Mito3__OG0000008_MSA_IQ.log
# OF_Mito3__OG0000012/OF_Mito3__OG0000012_MSA_IQ.log
# OF_Mito3__OG0000014/OF_Mito3__OG0000014_MSA_IQ.log
# OF_Mito3__OG0000018/OF_Mito3__OG0000018_MSA_IQ.log
#only OF_Mito3__OG0000012 was new here, but a check of the log file reveals no issues
#and double-checked the TrimAl results
wc -l 4seqTrim_Mito_OF_Index.txt
# 998 4seqTrim_Mito_OF_Index.txt
#looks good
#Sec_OF/
chmod +x *.sh
nohup ./remnant_IQ-TREE_1.sh &> nohup_remnant_IQ-TREE_1.out &
# [4] 46360
nohup ./remnant_IQ-TREE_2.sh &> nohup_remnant_IQ-TREE_2.out &
# [5] 46619
nohup ./remnant_IQ-TREE_11.sh &> nohup_remnant_IQ-TREE_11.out &
# [1] 1023898
#final checks... 
ls *_trim.fasta | wc -l
# 3219
tree | tail 
# │   ├── OF_Sec3__OG0025017_MSA_IQ.splits.nex
# │   ├── OF_Sec3__OG0025017_MSA_IQ.treefile
# │   ├── OF_Sec3__OG0025017_MSA_IQ.ufboot
# │   └── OF_Sec3__OG0025017_MSA_trim.fasta
# ├── OF_Sec3__OG0025017_MSA_trim.fasta -> /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_OF/Trimmed_Files/OF_Sec3__OG0025017_MSA_trim.fasta
# ├── remnant_IQ-TREE_11.sh
# ├── remnant_IQ-TREE_1.sh
# └── remnant_IQ-TREE_2.sh
# 3219 directories, 39386 files
#the number of directories is correct! 
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF>=13'
# . 6510
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF<=1'
#no result, which is what we want
grep -l "ERROR: Cannot write to file" */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/'
# OF_Sec3__OG0000000_MSA_trim.fasta
# OF_Sec3__OG0000004_MSA_trim.fasta
# OF_Sec3__OG0000008_MSA_trim.fasta
# OF_Sec3__OG0022086_MSA_trim.fasta
#of these, there is only an actual issue with OF_Sec3__OG0022086, based on the log files
#so let's rerun that one
nohup iqtree -s OF_Sec3__OG0022086_MSA_trim.fasta --prefix OF_Sec3__OG0022086_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Sec3__OG0022086.out &
# [1] 2239222
#and double-checked the TrimAl results
wc -l 4seqTrim_Sec_OF_Index.txt
# 3219 4seqTrim_Sec_OF_Index.txt
#looks good
#Sec_SP/
chmod +x *.sh
nohup ./remnant_IQ-TREE_6.sh &> nohup_remnant_IQ-TREE_6.out &
# [6] 46932
nohup ./remnant_IQ-TREE_7.sh &> nohup_remnant_IQ-TREE_7.out &
# [7] 47193
nohup ./remnant_IQ-TREE_8.sh &> nohup_remnant_IQ-TREE_8.out &
# [8] 47451
nohup ./remnant_IQ-TREE_9.sh &> nohup_remnant_IQ-TREE_9.out &
# [9] 47715
nohup ./remnant_IQ-TREE_10.sh &> nohup_remnant_IQ-TREE_10.out &
# [10] 47977
nohup ./remnant_IQ-TREE_12.sh &> nohup_remnant_IQ-TREE_12.out &
# [2] 1024806
nohup ./remnant_IQ-TREE_13.sh &> nohup_remnant_IQ-TREE_13.out &
# [1] 1051828
#final checks...
ls *_trim.fasta | wc -l
# 2386
tree | tail
# │   ├── SP_Sec3__OG_9981_MSA_IQ.model.gz
# │   ├── SP_Sec3__OG_9981_MSA_IQ.splits.nex
# │   ├── SP_Sec3__OG_9981_MSA_IQ.treefile
# │   ├── SP_Sec3__OG_9981_MSA_IQ.ufboot
# │   └── SP_Sec3__OG_9981_MSA_trim.fasta
# ├── SP_Sec3__OG_9981_MSA_trim.fasta -> /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/Trimmed_Files/SP_Sec3__OG_9981_MSA_trim.fasta
# ├── SP_Sec3__OG_998_MSA_trim.fasta -> /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/Trimmed_Files/SP_Sec3__OG_998_MSA_trim.fasta
# └── SP_Sec3__OG_99_MSA_trim.fasta -> /home/inf-47-2020/ThesisTrich/TrimAl_Results/Sec_SP/Trimmed_Files/SP_Sec3__OG_99_MSA_trim.fasta
# 2383 directories, 29298 files
#marvelous -_-;; so now let's figure out which 3 files got lost
ls SP_Sec3__OG_*.fasta | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	if [ ! -d "${dir_base}" ] ; then
		echo ${full_file} ; 
	fi ; 
done
# SP_Sec3__OG_14_MSA_trim.fasta
# SP_Sec3__OG_2_MSA_trim.fasta
# SP_Sec3__OG_97_MSA_trim.fasta
#brilliant, especially given that 2 is gonna be long, and 14 probably won't be short, either
grep -c ">" SP_Sec3__OG_2_MSA_trim.fasta
# 815
grep -c ">" SP_Sec3__OG_14_MSA_trim.fasta
# 472
grep -c ">" SP_Sec3__OG_97_MSA_trim.fasta
# 205
#ah boy yeah the 2 is gonna be here until tomorrow
nohup iqtree -s SP_Sec3__OG_2_MSA_trim.fasta --prefix SP_Sec3__OG_2_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Sec3__OG_2.out &
# [1] 1420995
nohup iqtree -s SP_Sec3__OG_14_MSA_trim.fasta --prefix SP_Sec3__OG_14_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Sec3__OG_14.out &
# [2] 1424718
nohup iqtree -s SP_Sec3__OG_97_MSA_trim.fasta --prefix SP_Sec3__OG_97_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Sec3__OG_97.out &
# [3] 1427879
#ok!!!
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF>=13'
# . 4830
# ./SP_Sec3__OG_102 13
find . -maxdepth 1 -type d -exec bash -c "echo -ne '{} '; ls '{}' | wc -l" \; | awk '$NF<=1'
#no results, which is what we want
grep -l "ERROR: Cannot write to file" */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/'
# SP_Sec3__OG_2089_MSA_trim.fasta
#checked it and yes, there is an issue with this file, so let's re-run it
nohup iqtree -s SP_Sec3__OG_2089_MSA_trim.fasta --prefix SP_Sec3__OG_2089_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Sec3__OG_2089.out &
# [1] 1571634
#and double-checked the TrimAl results
wc -l 4seqTrim_Sec_SP_Index.txt
# 2386 4seqTrim_Sec_SP_Index.txt
#looks good
###
#cleaning up "files" made during the server failure
find . -type f  ! -name "*.*"  -delete
#the above removes all files without extensions
#this is necessary because instead of creating directories, 
#empty files without extensions were made
###
#finding trees that ran into the less than 4 sequences problem 
#and re-running with `-keep-ident`
grep -l "ERROR: It makes no sense to perform bootstrap with less than 4 sequences." */*.log
#this grabs the file names including the paths into the OG directories for the .log files
grep -l "ERROR: It makes no sense to perform bootstrap with less than 4 sequences." */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)'
#the above addition graps the basename of the file up to the "_MSA" string
grep -l "ERROR: It makes no sense to perform bootstrap with less than 4 sequences." */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/' > 4seq_reruns_MitoOF.txt
#the addition of the sed adds the "_trim.fasta" string to the end of the line
ls 4seq_reruns_MitoOF.txt | while read line; do
	full_file="${line##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	zip -m ${file_base}_Pre-Keep-Ident ${dir_base}/*; #zip files in directory, removing as you go
	mv ${file_base}_Pre-Keep-Ident.zip ${dir_base}/ ;
	iqtree -s $file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $line ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}/ ; #moving the results files into the OG directory
done
#went to Courtney with the above, and she recommended this, instead: 
for full_file in `cat 4seq_reruns_MitoOF.txt` ; do
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	zip -m ${file_base}_Pre-Keep-Ident ${dir_base}/*; #zip files in directory, removing as you go
	mv ${file_base}_Pre-Keep-Ident.zip ${dir_base}/ ;
	iqtree -s $full_file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $line ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}/ ; #moving the results files into the OG directory
done
#saved the above to 4seq_reruns_MitoOF.sh
nohup ./4seq_reruns_MitoOF.sh &> nohup_4seq_MitoOF.out &
# [1] 10809
#had an issue with the variable names (forgot to update 1) so...
for full_file in `cat 4seq_reruns_MitoOF.txt` ; do
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	#zip -m ${file_base}_Pre-Keep-Ident ${dir_base}/*; #zip files in directory, removing as you go
	#mv ${file_base}_Pre-Keep-Ident.zip ${dir_base}/ ;
	#don't need the zip commands since these are already done
	iqtree -s $full_file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $full_file ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}/ ; #moving the results files into the OG directory
done
#saved the above to 4seq_reruns_MitoOF_v2.sh
chmod +x 4seq_reruns_MitoOF_v2.sh
nohup ./4seq_reruns_MitoOF_v2.sh &> nohup_4seq_MitoOF_v2.out &
# [1] 35666
wc -l 4seq_MitoOF.txt
# 22
#there are 22 files in the list that need to be rerun with `-keep-ident`
#can track progress with: 
grep -c "ALISIM" nohup_4seq_MitoOF_v2.out
#had to interrupt the loop to do a "turn it off and on again" with the server
#there are 4 left to run: 
nohup iqtree -s OF_Mito3__OG0023767_MSA_trim.fasta --prefix OF_Mito3__OG0023767_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Mito3__OG0023767.out &
# [1] 7294
nohup iqtree -s OF_Mito3__OG0023888_MSA_trim.fasta --prefix OF_Mito3__OG0023888_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Mito3__OG0023888.out &
# [2] 7580
nohup iqtree -s OF_Mito3__OG0024703_MSA_trim.fasta --prefix OF_Mito3__OG0024703_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Mito3__OG0024703.out &
# [3] 7840
nohup iqtree -s OF_Mito3__OG0024754_MSA_trim.fasta --prefix OF_Mito3__OG0024754_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_OF_Mito3__OG0024754.out &
# [4] 8096
#and done! 
#Mito_SP/
grep -l "ERROR: It makes no sense to perform bootstrap with less than 4 sequences." */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/' > 4seq_reruns_MitoSP.txt
wc -l 4seq_reruns_MitoSP.txt
# 18 4seq_reruns_MitoSP.txt
for full_file in `cat 4seq_reruns_MitoSP.txt` ; do
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	zip -m ${file_base}_Pre-Keep-Ident ${dir_base}/*; #zip files in directory, removing as you go
	mv ${file_base}_Pre-Keep-Ident.zip ${dir_base}/ ;
	iqtree -s $full_file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $full_file ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}/ ; #moving the results files into the OG directory
done
#saved the above to 4seq_reruns_MitoSP.sh
chmod +x 4seq_reruns_MitoSP.sh
nohup ./4seq_reruns_MitoSP.sh &> nohup_4seq_MitoSP.out &
# [1] 1022069
#track progress with: 
grep -c "ALISIM" nohup_4seq_MitoSP.out
#Sec_OF/
grep -l "ERROR: It makes no sense to perform bootstrap with less than 4 sequences." */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/' > 4seq_reruns_SecOF.txt
wc -l 4seq_reruns_SecOF.txt
# 17 4seq_reruns_SecOF.txt
for full_file in `cat 4seq_reruns_SecOF.txt` ; do
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	zip -m ${file_base}_Pre-Keep-Ident ${dir_base}/*; #zip files in directory, removing as you go
	mv ${file_base}_Pre-Keep-Ident.zip ${dir_base}/ ;
	iqtree -s $full_file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $full_file ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}/ ; #moving the results files into the OG directory
done
#saved the above to 4seq_reruns_SecOF.sh
chmod +x 4seq_reruns_SecOF.sh
nohup ./4seq_reruns_SecOF.sh &> nohup_4seq_SecOF.out &
# [1] 2239737
#track progress with: 
grep -c "ALISIM" nohup_4seq_SecOF.out
#Sec_SP/
grep -l "ERROR: It makes no sense to perform bootstrap with less than 4 sequences." */*.log | grep -o -P '(?<=\/).*(?=_IQ.log)' | sed -e 's/$/_trim.fasta/' > 4seq_reruns_SecSP.txt
wc -l 4seq_reruns_SecSP.txt
# 21 4seq_reruns_SecSP.txt
for full_file in `cat 4seq_reruns_SecSP.txt` ; do
	file_base="${full_file%.*}"; #this line removes the file extension
	file_tree_base="${file_base%_trim}"; #this removes the "_trim" substring
	dir_base="${file_base%_MSA_trim}"; #this removes the "_MSA_trim" substring
	zip -m ${file_base}_Pre-Keep-Ident ${dir_base}/*; #zip files in directory, removing as you go
	mv ${file_base}_Pre-Keep-Ident.zip ${dir_base}/ ;
	iqtree -s $full_file --prefix ${file_tree_base}_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -keep-ident -seed 12345 -wbtl -T AUTO -ntmax 15;
	cp $full_file ${dir_base}/ ; #copy the FASTA file in the OG IQ-TREE directory
	mv ${file_tree_base}_IQ.* ${dir_base}/ ; #moving the results files into the OG directory
done
#saved the above to 4seq_reruns_SecSP.sh
chmod +x 4seq_reruns_SecSP.sh
nohup ./4seq_reruns_SecSP.sh &> nohup_4seq_SecSP.out &
# [1] 1622065
#track progress with: 
grep -c "ALISIM" nohup_4seq_SecSP.out
###
#final checks to make sure I caught everything
grep -lv "Date and Time" */*.log
#lmao well that didn't work
#ref: https://stackoverflow.com/questions/7205004/how-to-find-text-files-not-containing-text-on-linux
grep -iL "Date and Time" */*.log
#but that one did! 
#confirmed bad files from Mito_SP/
#found no files in Mito_OF/ or Sec_SP/  or Sec_OF/
###
#copying files over to Uppmax
#working in the /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/ directory
#before we start, a quick check: 
uquota
# Your project     Your File Area           Unit      Usage  Quota Limit  Over Quota
# ---------------  -----------------------  --------  -----  -----------  ----------
# home             /home/vivarga            GBytes      3.5         32.0
# home             /home/vivarga            Files     70927       300000
# snic2022-22-256  /proj/rhodoquinone_2022  GBytes      2.3          128
#Mito_OF/
scp inf-47-2020@bioinf-serv2.cob.lu.se:/home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Mito_OF/*/*.ufboot .
#lmao this takes,,, longer than I was expecting, whoops
#probably a combination of file size and the */*.ufboot forcing a search through sub-directories
#though it gets faster as we rise through the numbers (smaller trees!)
#a `uquota` check shows the same GB usage, so that's a relief!!!
ls *.ufboot | wc -l
# 998
#that's correct!
#Mito_SP/
scp inf-47-2020@bioinf-serv2.cob.lu.se:/home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Mito_SP/*/*.ufboot .
ls *.ufboot | wc -l
# 827
#that means there's three missing!!! it should be 830
#working back on the server
#ref: https://askubuntu.com/questions/370024/find-directories-that-dont-contain-a-file-but-yes-another-one
find . -type d '!' -exec sh -c 'ls -1 "{}"|egrep -iq "^*\.(ufboot)$" ' ';' -print
# .
# ./SP_Mito3__OG_583
# ./SP_Mito3__OG_548
# ./SP_Mito3__OG_5491
#all 3 of these have incomplete log files
#so let's rerun them
nohup iqtree -s SP_Mito3__OG_583_MSA_trim.fasta --prefix SP_Mito3__OG_583_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Mito3__OG_583.out &
# [1] 2031445
#72 sequences
nohup iqtree -s SP_Mito3__OG_548_MSA_trim.fasta --prefix SP_Mito3__OG_548_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Mito3__OG_548.out &
# [2] 2031706
#75 sequences
nohup iqtree -s SP_Mito3__OG_5491_MSA_trim.fasta --prefix SP_Mito3__OG_5491_MSA_IQ -m MFP -B 1000 -bnni -mset LG -madd LG+C10,LG+C20,LG+C30,LG+C40,LG+C50,LG+C60 -seed 12345 -wbtl -T AUTO -ntmax 15 &> nohup_SP_Mito3__OG_5491.out &
# [3] 2031971
#this one only has 10 sequnces, so might need to `-keep-ident` it
#but no, it worked! 
scp inf-47-2020@bioinf-serv2.cob.lu.se:/home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Mito_SP/*.ufboot .
ls *.ufboot | wc -l
# 830
#great, done.
#Sec_OF/
scp inf-47-2020@bioinf-serv2.cob.lu.se:/home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Sec_OF/*/*.ufboot .
ls *.ufboot | wc -l
# 3219
#Sec_SP/
scp inf-47-2020@bioinf-serv2.cob.lu.se:/home/inf-47-2020/ThesisTrich/IQ-TREE_Results/Sec_SP/*/*.ufboot .
ls *.ufboot | wc -l
# 2386
#that's correct

```


## ALE 

ALE is the gene tree species tree aware ancestral state reconstruction program. 

```bash
#on my local computer: 
singularity pull ale.sif docker://boussau/alesuite:latest
#well, it made a file, at least! now we have to see whether it works on Rackham, I guess
#transferred over with FileZilla
#using the Singularity image created locally & transferred to the HPC
#need to make it executable
chmod +x ale.sif
#docker usage model: 
docker run -v $PWD:$PWD  -w $PWD boussau/alesuite ALEobserve $PWD/geneFamily.treelist
docker run -v $PWD:$PWD  -w $PWD boussau/alesuite ALEml_undated $PWD/species_tree.newick $PWD/geneFamily.treelist.ale
docker run -v $PWD:$PWD  -w $PWD boussau/alesuite ALEmcmc_undated $PWD/species_tree.newick $PWD/geneFamily.treelist.ale
#adapted to Singularity (model version): 
singularity exec ale.sif ALEobserve geneFamily.treelist
singularity exec ale.sif ALEml_undated species_tree.newick geneFamily.treelist.ale
singularity exec ale.sif ALEmcmc_undated species_tree.newick geneFamily.treelist.ale

```

Creating the species tree:

The species tree used for Count is the basis, with the following modifications made: 
 - Quotation marks removed from names
 - Removal of "N" character from "N[number]:" strings
 - Replacing "_" characters in species names with "-" characters
 - File saved to SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk

```text
(((Anaeramoeba-lanta-160522:0.450921,(BM-newprots-may21.anaeromoeba:0.335199,(SC-newprots-may21.anaeromoeba:0.00659918,BS-newprots-may21.anaeromoeba:9.97265e-06)12:0.324153)7:0.0752971)3:0.085195,(((Tetratrichomonas-gallinarum.5730.aa:0.0016019,Pentatrichomonas-hominis.5728.aa:0.0345883)13:0.134371,(Trichomonas-vaginalis-GenBank.PRJNA16084:9.97265e-06,Trichomonas-vaginalis-RefSeq.G3:9.97265e-06)14:0.174424)8:0.0916459,((Dientamoeba-fragilis.43352.aa:0.159906,(Histomonas-meleagridis.PRJNA594289:0.0108462,Histomonas-meleagridis.135588.aa:0.0901299)18:0.120525)15:0.0690226,Tritrichomonas-foetus.PRJNA345179:0.17672)9:0.0769582)4:0.279032)1:0.0329942,(((EP00708-Paratrimastix-pyriformis:0.302018,EP00771-Trimastix-marina:0.370245)10:0.0929272,EP00770-Monocercomonoides-exilis:0.432518)5:0.0989893,(EP00792-Barthelona-sp-PAP020:0.499294,((EP00769-Ergobibamus-cyprinoides:0.490135,Carpediemonas-membranifera.PRJNA719540:0.461779)16:0.0460798,(EP00764-Aduncisulcus-paluster:0.4689,((EP00766-Chilomastix-caulleryi:0.576054,EP00767-Chilomastix-cuspidata:0.427461)20:0.143484,(Kipferlia-bialata.PRJDB5223:0.393386,(EP00768-Dysnectes-brevis:0.360694,((Spironucleus-salmonicida.PRJNA60811:0.357159,EP00703-Trepomonas-sp-PC1:0.340399)24:0.154188,(Giardia-muris.PRJNA524057:0.250193,((GiardiaDB-GintestinalisEP15:0.0161814,(GiardiaDB-GintestinalisADH:0.0045674,(EP00701-Giardia-intestinalis:9.97265e-06,Giardia-intestinalis.PRJNA1439:9.97265e-06)30:0.00343865)29:0.016551)27:0.0203631,(GiardiaDB-GintestinalisBGS:0.00134291,GiardiaDB-GintestinalisBGS-B:0.00664544)28:0.0390068)26:0.204119)25:0.212948)23:0.13267)22:0.0832471)21:0.0437682)19:0.0502614)17:0.0394533)11:0.0430432)6:0.0327792)2:0.0329942)0;
```

Python script to edit the species name versions used in the FASTA headers & gene trees to match the versions of the names that appear in the species tree (script saved to ale_tree_spp_names.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: ale_tree_spp_names.py
Date: 2022.06.14
Author: Virág Varga

Description:
	This program iterates over a file containing gene trees in order to replace the 
		shortened versions of the species names with the full versions found in the 
		species tree. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys

Procedure:
	1. Loading required modules; assigning command-line arguments.
	2. Creating reference dictionary for shortened and official species names.
	3. Iterating over the tree file and changing the species designations, writing 
		out the new tree versions to the output file as each one is created. 

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but is instead based on the input
		file name.  

Usage
	./ale_tree_spp_names.py input_tree
	OR
	python ale_tree_spp_names.py input_tree

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments


#assign command line arguments; load input and output files
input_tree = sys.argv[1]
#input_tree = "SP_Mito3__OG_2636_MSA_IQ.ufboot"

#determine output file name based on input file name
input_name_list = input_tree.split(".")
#save contents of the name to a list
input_name_list.insert(-1, 'names')
#add the string "name" in the second to last position in the list
#and join the list elements back together to form the output file name
output_tree = '.'.join(input_name_list)


#Part 2: Create reference dictionary for shortened and official species names

species_dict = {"A_lanta": "Anaeramoeba_lanta_160522", 
						  "A_ignava_BM": "BM_newprots_may21.anaeromoeba", 
						  "A_flamelloides_BS": "BS_newprots_may21.anaeromoeba", 
						  "C_membranifera": "Carpediemonas_membranifera.PRJNA719540", 
						  "D_fragilis": "Dientamoeba_fragilis.43352.aa", 
						  "G_intestinalis_A_EukProt": "EP00701_Giardia_intestinalis", 
						  "Trepomonas_PC1": "EP00703_Trepomonas_sp_PC1", 
						  "P_pyriformis": "EP00708_Paratrimastix_pyriformis", 
						  "A_paluster": "EP00764_Aduncisulcus_paluster", 
						  "C_caulleryi": "EP00766_Chilomastix_caulleryi", 
						  "C_cuspidata": "EP00767_Chilomastix_cuspidata", 
						  "D_brevis": "EP00768_Dysnectes_brevis", 
						  "E_cyprinoides": "EP00769_Ergobibamus_cyprinoides", 
						  "M_exilis": "EP00770_Monocercomonoides_exilis", 
						  "T_marina": "EP00771_Trimastix_marina", 
						  "Barthelona_PAP020": "EP00792_Barthelona_sp_PAP020", 
						  "G_intestinalis_ADH": "GiardiaDB_GintestinalisADH", 
						  "G_intestinalis_BGS": "GiardiaDB_GintestinalisBGS", 
						  #"G_intestinalis_BGS_B": "GiardiaDB_GintestinalisBGS_B", 
						  "G_intestinalis_EP15": "GiardiaDB_GintestinalisEP15", 
						  "G_intestinalis_A_NCBI": "Giardia_intestinalis.PRJNA1439", 
						  "G_muris": "Giardia_muris.PRJNA524057", 
						  "H_meleagridis_OLD": "Histomonas_meleagridis.135588.aa", 
						  "H_meleagridis_NEW": "Histomonas_meleagridis.PRJNA594289", 
						  "K_bialata": "Kipferlia_bialata.PRJDB5223", 
						  "P_hominis": "Pentatrichomonas_hominis.5728.aa", 
						  "A_flamelloides_SC": "SC_newprots_may21.anaeromoeba", 
						  "S_salmonicida": "Spironucleus_salmonicida.PRJNA60811", 
						  "T_gallinarum": "Tetratrichomonas_gallinarum.5730.aa", 
						  "T_foetus": "Tritrichomonas_foetus.PRJNA345179", 
						  "T_vaginalis_GenBank": "Trichomonas_vaginalis_GenBank.PRJNA16084", 
						  "T_vaginalis_RefSeq": "Trichomonas_vaginalis_RefSeq.G3"}


'''
#adjusting for the BGS_B gene issue
bgs_b_genes_list = ["BLORVD0pGH4kejnx", "BRjoXadcs79DtjjF", "BB2HAPVsl9lf351v", "BUsgl1nKIPQG1ucj", 
					"BYl0e5oRqMzYC6wa", "BTzVlEzomPpywGb0", "Bzo2CNkJawQ6Pyhl", "BKzslU785QvQF0ad", 
					"B5j2b5nm4M2m3mic", "BFvSp0l4Ln6mjVUC", "BiadCsqVhO1MBks1", "BKgLWt9bWfuI3DEn", 
					"B0gbE44LXH9djDHT", "BYLnN871mzBbkFuJ", "BMeGKFdtNYL2g2fH", "BBTPCcDLReHslTDI", 
					"BHXel28xfXPp1Ra0", "BFszMXUvv0SOcdZa", "BkwJo5zXEnJhv3Sz", "BsWpnXvrlFQyeDAP", 
					"BAos68CMKm9k9QCr", "BeoRNavusDsOiRb3", "BkikAI4aSPZ7Y97Q", "BC5cv8sa7Gzvvt1X", 
					"ByVlU35cYj6OffrB", "BUUCjwM9PYfOMI5P", "Bq41wMZ8M7rzZro6", "BOl7XHzhCBb31XW2", 
					"BQfm5C47MGMr2abH", "BMH8YDcbCngex7fR", "BegUCpQNbgDUowkA", "BSQHQW34S9dRyGY8", 
					"BtPA7Ui5t9RJU71x", "BtqJd8jjmM4rVu6n", "Bx1bafhnUtOdNv1N", "BJQBqTGluqwTr34I", 
					"BmKtOHnymUBa33eg", "BjnfjcDnMs1zt8Xl", "BAgN9eLniiX2Jxmc", "BamHO4WZojR51OAE", 
					"B6FFVp87EoD4IblZ", "B8bWSHSt0bL0MMOT", "BGEdRqx4eipnt7Z1", "B7TU32EyBKRsELp8", 
					"BRizxJ6HWlHCrNdR", "BkOwqdFhPOKmXJNS", "B47OEOOOjw9mK73B", "BBgfNKZ8zUO0PUQT", 
					"BmSD9EcptubXHDyf", "B9bI4bcLWKzQVgwX", "BwmjEBJI4FirMzIi", "Bs3oX68lFioeG1ef", 
					"BvCeIzeBgjYwFhP6", "BqYrA1RYW0PhADyJ", "BFREhbPv4r5Sdiqi", "BYgeh8Uo77O2YZ4g", 
					"BTcP6q1Aw83YibpG", "Bsrb7Dk8mM3ET74A", "Bk0YeYCPJ1jthKBA", "BD4JED05Dx6kgAvy", 
					"BfRijwHuvXNd7JFn", "B9V9S8JXsleiVQHh", "Bqm5IkqF70P2R0dX", "BjsgPIqrNvGbJRMs", 
					"Bg62EMQIS0wzBGjG", "B29MSdxhRqxsfNOp", "BgaCciT5RJx7A5RE", "B5HaCfRVw97nDkci", 
					"BeGinTggURwLPQ1f", "BwefHOmQ5brWEMuz", "BLCszYHp48mn4LeS", "B8i362TviaJkAVCf", 
					"B0tEv21BwSfDTggC", "BqpARYLmFSdVxc70", "BBmD330dsGexAzQp", "BspGZdfe1TtP5FGV", 
					"BMvRrb5Sq2v9IzfU", "BnZF7IfjyTYvzZv1", "BkwaIUaJX26RcX03", "BRsTJAbzZBQc8ttq", 
					"BWMgvapEBfsLyobd", "BMG5n4ochzot6Uew", "BxuAIF5oe5Em8TG9", "BZg6wivF4OYW2ADu", 
					"BU0cs1deeBWOaSvA", "BmbAoXjE2ZEGm9Ln", "Bv1uSsX3aakePX4G"]
'''


#Part 3: Iterate over the tree file and change the species designations

with open(input_tree, "r") as infile, open(output_tree, "w") as outfile: 
	#open the input tree for reading, and the output tree file for writing
	for line in infile: 
		#iterate over the tree file line by line
		tree_line = line.strip()
		#save the old gene tree to a variable, stripping the end-line character
		if "G_intestinalis_BGS_B__" in tree_line: 
			#check to see if thee BGS_B Giardia genome occurs in the tree
			#if it occurs, overwrite the genome prior to match searching
			tree_line = tree_line.replace("G_intestinalis_BGS_B__", "GiardiaDB-GintestinalisBGS-B__")
		for spp_key in species_dict.keys(): 
			#iterate over the keys of the dictionary, 
			#to identify which of the species occur in the tree file
			if spp_key in tree_line: 
				#check to see if the shorter species designation occurs in the gene tree
				#if it is, save the full species designation to a variable
				#and replace the "_" characters with "-" characters in the species names
				value_edit = species_dict[spp_key].replace("_", "-")
				#and overwrite the tree with the full species designation
				tree_line = tree_line.replace(spp_key, value_edit)
				tree_line = tree_line.replace("__", "_")
				#the modifications to the naming scheme with regards to the underscores is necessary
				#because ALE (or at least the container version of it)
				#doesn't accept the `separators` argument shown on their GitHub
				#that would allow the user to specify the separator 
				#between a species name and gene ID
		#once all species have been checked, write out the new tree to the output file
		#write each tree to a new line
		outfile.write(tree_line + "\n")

```

Actually running the ALE process: 

```bash
#working with files in the /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/ directory
cp /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/TESTData/SpeciesTree_rooted_at_outgroup_22_ALE.* .
#running ALE from /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/bin/ directory
#Mito_OF/
ls *.ufboot | while read file; do
	python ../../Scripts/ale_tree_spp_names.py $file;
done
#for some reason, Uppmax doesn't like the multi-line version, so did the above in one line
#now onto the ALE
nohup ls ../ALE_Files/Mito_OF/OF_Mito3__OG000*.names.ufboot | while read file; do
	singularity exec ale.sif ALEobserve $file >> ../ALE_Files/Mito_OF/ALEobserve_MitoOF_0.log;
done &
# [1] 25849
# nohup: ignoring input and redirecting stderr to stdout
# terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<std::overflow_error> >'
#   what():  Error in function boost::math::tgamma<e>(e): Result of tgamma is too large to represent.
# /bin/bash: line 36: 26021 Aborted                 singularity exec ale.sif ALEobserve $file >> ../ALE_Files/Mito_OF/ALEobserve_MitoOF_0.log
#GREAT!!!! LOVE THAT!!!
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0000000_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# terminate called after throwing an instance of 'boost::exception_detail::clone_impl<boost::exception_detail::error_info_injector<std::overflow_error> >'
#   what():  Error in function boost::math::tgamma<e>(e): Result of tgamma is too large to represent.
# Aborted
#this is an error that is due to file size 
#the OrthoFinder OG named OG0000000 is too large (has too many members) for ALE to analyze
nohup ls ../ALE_Files/Mito_OF/OF_Mito3__OG001*.names.ufboot | while read file; do
	singularity exec ale.sif ALEobserve $file >> ../ALE_Files/Mito_OF/ALEobserve_MitoOF_1.log;
done &
# [1] 31158
# ignoring input and redirecting stderr to stdout
nohup ls ../ALE_Files/Mito_OF/OF_Mito3__OG002*.names.ufboot | while read file; do
	singularity exec ale.sif ALEobserve $file >> ../ALE_Files/Mito_OF/ALEobserve_MitoOF_2.log;
done &
# [1] 40334
# nohup: ignoring input and redirecting stderr to stdout
#before I run the next step of this... 
grep -l "G_intestinalis_BGS__B" OF_Mito3__OG000*.ufboot
# OF_Mito3__OG0000188_MSA_IQ.ufboot
# OF_Mito3__OG0000201_MSA_IQ.ufboot
# OF_Mito3__OG0000210_MSA_IQ.ufboot
# OF_Mito3__OG0000211_MSA_IQ.ufboot
# OF_Mito3__OG0004827_MSA_IQ.ufboot
#ah, fuck, ok, going to need to redo these
#but I won't interrupt the loop, there's not enough of them to be worth that effort
python ../../Scripts/ale_tree_spp_names.py OF_Mito3__OG0000188_MSA_IQ.ufboot
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0000188_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# # observe 1000 tree(s) from: ../ALE_Files/Mito_OF/OF_Mito3__OG0000188_MSA_IQ.names.ufboot
# 0 burn in per file discarded.
# # saved in ../ALE_Files/Mito_OF/OF_Mito3__OG0000188_MSA_IQ.names.ufboot.ale
python ../../Scripts/ale_tree_spp_names.py OF_Mito3__OG0000201_MSA_IQ.ufboot
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0000201_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# # observe 1000 tree(s) from: ../ALE_Files/Mito_OF/OF_Mito3__OG0000201_MSA_IQ.names.ufboot
# 0 burn in per file discarded.
# # saved in ../ALE_Files/Mito_OF/OF_Mito3__OG0000201_MSA_IQ.names.ufboot.ale
python ../../Scripts/ale_tree_spp_names.py OF_Mito3__OG0000210_MSA_IQ.ufboot
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0000210_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# # observe 1000 tree(s) from: ../ALE_Files/Mito_OF/OF_Mito3__OG0000210_MSA_IQ.names.ufboot
# 0 burn in per file discarded.
# # saved in ../ALE_Files/Mito_OF/OF_Mito3__OG0000210_MSA_IQ.names.ufboot.ale
python ../../Scripts/ale_tree_spp_names.py OF_Mito3__OG0000211_MSA_IQ.ufboot
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0000211_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# # observe 1000 tree(s) from: ../ALE_Files/Mito_OF/OF_Mito3__OG0000211_MSA_IQ.names.ufboot
# 0 burn in per file discarded.
# # saved in ../ALE_Files/Mito_OF/OF_Mito3__OG0000211_MSA_IQ.names.ufboot.ale
python ../../Scripts/ale_tree_spp_names.py OF_Mito3__OG0004827_MSA_IQ.ufboot
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0004827_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# # observe 1000 tree(s) from: ../ALE_Files/Mito_OF/OF_Mito3__OG0004827_MSA_IQ.names.ufboot
# 0 burn in per file discarded.
# # saved in ../ALE_Files/Mito_OF/OF_Mito3__OG0004827_MSA_IQ.names.ufboot.ale
nohup ls ../ALE_Files/Mito_OF/OF_Mito3__OG000*.names.ufboot.ale | while read file; do
	singularity exec ale.sif ALEml_undated ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk $file >> ../ALE_Files/Mito_OF/ALEml_undated_MitoOF_0.log;
	rm core.* ; #failed runs create a binary core.* file that isn't needed for anything
	mv *.names.ufboot.ale.uml_rec ../ALE_Files/Mito_OF/ ; #move the file over to the results directory
done &> nohup_Expected_bpp_error_MitoOF_0.log &
# [1] 36063
# nohup: ignoring input and redirecting stderr to stdout
kill 36063
#got an unhappy email about CPU usage, so I terminated the process 
#I'll figure out SLURM before continuing
projinfo
# (Counting the number of core hours used since 2022-05-29/00:00:00 until now.)
# Project             Used[h]   Current allocation [h/month]
#    User
# -----------------------------------------------------
# snic2022-22-256        0.00                2000
sbatch aleML_loops_Mito_OF_1.sh
# Submitted batch job 27964731
#this one timed out, so I'm writing a new script to make up for it
sbatch aleML_loops_Mito_OF_2.sh
# Submitted batch job 27964732
sbatch aleML_loops_Mito_OF_3.sh
# Submitted batch job 28013829
nohup ls ../ALE_Files/Mito_OF/OF_Mito3__OG001*.names.ufboot.ale | while read file; do
	singularity exec ale.sif ALEml_undated ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk $file >> ../ALE_Files/Mito_OF/ALEml_undated_MitoOF_1.log;
	rm core.* ; #failed runs create a binary core.* file that isn't needed for anything
	mv *.names.ufboot.ale.uml_rec ../ALE_Files/Mito_OF/ ; #move the file over to the results directory
done &> nohup_Expected_bpp_error_MitoOF_1.log &
# [1] 4118
# nohup: ignoring input and redirecting stderr to stdout
#strange, it skipped one, so let's try that manually...
singularity exec ale.sif ALEml_undated ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale
# ALEml_undated using ALE v0.4
# Read species tree from: ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk..
# Read summary of tree sample for 1000 trees from: ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale..
# Reconciliation model initialised, starting DTL rate optimisation..
# #optimizing delta rate
# #optimizing tau rate
# #optimizing lambda rate
# Error: gene name GiardiaDB-GintestinalisBGS-BLcnVWiAlADMzmw9 is associated to species name GiardiaDB-GintestinalisBGS-BLcnVWiAlADMzmw9 that cannot be found in the species tree.
#huh. 
grep "GiardiaDB-GintestinalisBGS-BLcnVWiAlADMzmw9" OF_Mito3__OG0011053_MSA_IQ.names.ufboot
#ok so this is a G_intestinalis_BGS gene (not G_intestinalis_BGS_B) based on: 
grep "BLcnVWiAlADMzmw9" OF_Mito3__OG0011053_MSA_IQ.ufboot
#weird. let's try re-running the naming of it again
python ../../Scripts/ale_tree_spp_names.py OF_Mito3__OG0011053_MSA_IQ.ufboot
grep "GiardiaDB-GintestinalisBGS-BLcnVWiAlADMzmw9" OF_Mito3__OG0011053_MSA_IQ.names.ufboot
#shows the same as before. 
#ok, so this is happening because the naming script recognizes the G_intestinalis_BGS__BLcnVWiAlADMzmw9 as GiardiaDB-GintestinalisBGS-BLcnVWiAlADMzmw9
#ie. as a BGS-B gene instead of as a simple BGS gene. Funky!
#grabbed the list of genes this effects with : 
grep ">B" GiardiaDB_GintestinalisBGS_B_edit.fasta
#and modified the naming script
grep ">B" GiardiaDB_GintestinalisBGS_B_edit.fasta | wc -l
# 87
#ok!!! fixed the naming issue, I think! 
singularity exec ale.sif ALEobserve ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot
# ALEobserve using ALE v0.4
# 1000
# # observe 1000 tree(s) from: ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot
# 0 burn in per file discarded.
# # saved in ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale
singularity exec ale.sif ALEml_undated ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale
# ALEml_undated using ALE v0.4
# Read species tree from: ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk..
# Read summary of tree sample for 1000 trees from: ../ALE_Files/Mito_OF/OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale..
# Reconciliation model initialised, starting DTL rate optimisation..
# #optimizing delta rate
# #optimizing tau rate
# #optimizing lambda rate
# #optimizing rates
# Optimizing... \ 31
# ML rates:  delta=1e-06; tau=1e-06; lambda=1e-06.
# LL=-4.25176
# Sampling reconciled gene trees..
# 0%   10   20   30   40   50   60   70   80   90   100%
# |----|----|----|----|----|----|----|----|----|----|
# ***************************************************
# Results in: OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale.uml_rec
# Calculating consensus tree.
# [======================================] 100%
# Consensus tree in OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale.ucons_tree
# Transfers in: OF_Mito3__OG0011053_MSA_IQ.names.ufboot.ale.uTs
#lmao when did that get fixed????
#it's making the consensus tree after all, now????
#I mean, I guess that's a good, thing, lololololol
#though it only appears to be working for some of them, lmao
nohup ls ../ALE_Files/Mito_OF/OF_Mito3__OG002*.names.ufboot.ale | while read file; do
	singularity exec ale.sif ALEml_undated ../ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk $file >> ../ALE_Files/Mito_OF/ALEml_undated_MitoOF_2.log;
	rm core.* ; #failed runs create a binary core.* file that isn't needed for anything
	mv *.names.ufboot.ale.uml_rec ../ALE_Files/Mito_OF/ ; #move the file over to the results directory
done &> nohup_Expected_bpp_error_MitoOF_2.log &
# [1] 33773
# nohup: ignoring input and redirecting stderr to stdout
###
#these are now theoretically done
#so let's perform some analysis
ls *.names.ufboot | wc -l
# 998
ls *.ale | wc -l
# 997
ls *.ale.uml_rec | wc -l
# 630
#lmao that is,,, quite a few missing, then :/
#ref: https://unix.stackexchange.com/questions/628848/find-filenames-of-one-extension-that-dont-have-a-matching-filename-of-a-differe
find . -name "*.names.ufboot.ale" | perl -nle 's/\.names.ufboot.ale$/.names.ufboot.ale.uml_rec/; print unless -e' | cat -v | wc -l
# 367
#ref: https://stackoverflow.com/questions/14840953/how-to-remove-a-character-at-the-end-of-each-line-in-unix
#ref: https://www.unix.com/shell-programming-and-scripting/87242-sed-remove-1st-two-characters-every-line-text-file.html
find . -name "*.names.ufboot.ale" | perl -nle 's/\.names.ufboot.ale$/.names.ufboot.ale.uml_rec/; print unless -e' | cat -v | awk '{gsub(/.uml_rec$/,""); print}' | sed 's/^..//' > ALEml_missed_MitoOF.txt
#now run these through a new batch run
for file in `/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/ALEml_missed_MitoOF.txt` ; do
	singularity exec /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/bin/ale.sif ALEml_undated /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/${file} >> /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/ALEml_undated_MitoOF_missed.log;
done
#the above is saved to aleML_loops_Mito_OF_4.sh
sbatch aleML_loops_Mito_OF_4.sh
# Submitted batch job 28014471
ls *.ale.uml_rec | wc -l
# 630
#lmao what
#ok, well, let's take a look at the log file, then
cat slurm-28014471.out
# /var/spool/slurmd/job28014471/slurm_script: line 15: /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/ALEml_missed_MitoOF.txt: Permission denied
# mv: cannot stat ‘/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/Scripts/OF_Mito3__OG00*’: No such file or directory
#oh my fucking Gods I'm so stupid
for file in `cat /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/ALEml_missed_MitoOF.txt` ; do
	singularity exec /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/bin/ale.sif ALEml_undated /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/${file} >> /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/ALEml_undated_MitoOF_missed.log;
done
#I left out the `cat` JFC
sbatch aleML_loops_Mito_OF_4.sh
# Submitted batch job 28015464
#ok, now let's check again, and?
ls *.ale.uml_rec | wc -l
# 997
#great!!!
#Mito_SP/
ls *.ufboot | while read file; do
	python ../../Scripts/ale_tree_spp_names.py $file;
done
#correcting for the issue with the BGS files
grep -l "G_intestinalis_BGS__B" SP_Mito3__OG_*.ufboot
# SP_Mito3__OG_10583_MSA_IQ.ufboot
# SP_Mito3__OG_118_MSA_IQ.ufboot
# SP_Mito3__OG_181_MSA_IQ.ufboot
# SP_Mito3__OG_84_MSA_IQ.ufboot
# SP_Mito3__OG_8551_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Mito3__OG_10583_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Mito3__OG_118_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Mito3__OG_181_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Mito3__OG_84_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Mito3__OG_8551_MSA_IQ.ufboot
#now onto the ALE
jobinfo -u vivarga
# CLUSTER: rackham
# Running jobs:
#    JOBID PARTITION                      NAME     USER        ACCOUNT ST          START_TIME  TIME_LEFT  NODES CPUS NODELIST(REASON)
# Nodes in use:                            604
# Nodes in devel, free to use:               2
# Nodes in other partitions, free to use:    0
# Nodes available, in total:               606
# Nodes in test and repair:                  8
# Nodes, otherwise out of service:          16
# Nodes, all in total:                     630
# Waiting jobs:
#    JOBID    POS PARTITION                      NAME     USER        ACCOUNT ST          START_TIME   TIME_LEFT PRIORITY CPUS NODELIST(REASON)     FEATURES DEPENDENCY
# Waiting bonus jobs:
#working in the /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/Scripts/ directory
sbatch aleObserve_loops_Mito_SP.sh 
# Submitted batch job 27963358
scancel 27963358
#cancelled so I could email notifications to the batch script
sbatch aleObserve_loops_Mito_SP.sh
# Submitted batch job 27963410
sbatch aleML_loops_Mito_SP_1.sh
# Submitted batch job 27964733
sbatch aleML_loops_Mito_SP_2.sh
# Submitted batch job 27964734
sbatch aleML_loops_Mito_SP_3.sh
# Submitted batch job 27964735
###
#these are now theoretically done
#so let's perform some analysis
ls *.names.ufboot | wc -l
# 830
ls *.ale | wc -l
# 830
ls *.ale.uml_rec | wc -l
# 830
#brilliant! so these are actually done! (unlike the Mito_OF lol)
#Sec_OF/
nohup ls *.ufboot | while read file; do
	python ../../Scripts/ale_tree_spp_names.py $file;
done &
# [1] 901
# nohup: ignoring input and redirecting stderr to stdout
#now onto the ALE
sbatch aleObserve_loops_Sec_OF.sh
# Submitted batch job 27964730
sbatch aleML_loops_Sec_OF_1.sh
# Submitted batch job 27996024
#this job timed out, so created new scripts to run the remaining OGs
sbatch aleML_loops_Sec_OF_2.sh
# Submitted batch job 27996026
sbatch aleML_loops_Sec_OF_3.sh
# Submitted batch job 27996028
#bad exist status for these last 2
#a lot of it is bipartitions exception, but some of it is this: 
# terminate called after throwing an instance of 'bpp::NodePException'
#   what():  NodeException: Node::getName: no name associated to this node.(id:1)
# /var/spool/slurmd/job27996028/slurm_script: line 15: 12376 Aborted                 singularity exec /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/bin/ale.sif ALEml_undated /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk $file >> /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/ALEml_undated_SecOF_0.log
#I checked the files in the _3.sh file, and the necessary results files *.uml_rec are there and not empty
#so it's just screaming at me because the program is working as intended - incorrectly
sbatch aleML_loops_Sec_OF_4.sh
# Submitted batch job 28030872
sbatch aleML_loops_Sec_OF_5.sh
# Submitted batch job 28030873
#final checks...
ls *.names.ufboot | wc -l
# 3219
ls *.ale | wc -l
# 3218
ls *.ale.uml_rec | wc -l
# 2439
#ok,,, so it happened again, let's fix it
find . -name "*.names.ufboot.ale" | perl -nle 's/\.names.ufboot.ale$/.names.ufboot.ale.uml_rec/; print unless -e' | cat -v | awk '{gsub(/.uml_rec$/,""); print}' | sed 's/^..//' | wc -l
# 779
find . -name "*.names.ufboot.ale" | perl -nle 's/\.names.ufboot.ale$/.names.ufboot.ale.uml_rec/; print unless -e' | cat -v | awk '{gsub(/.uml_rec$/,""); print}' | sed 's/^..//' > ALEml_missed_SecOF.txt
wc -l ALEml_missed_SecOF.txt
# 779 ALEml_missed_SecOF.txt
#now run these through a new batch run
for file in `cat /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_OF/ALEml_missed_SecOF.txt` ; do
	singularity exec /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/bin/ale.sif ALEml_undated /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/SpeciesTree_rooted_at_outgroup_22_ALE.names.nwk /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_OF/${file} >> /proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_OF/ALEml_undated_SecOF_missed.log;
done
#the above is saved to aleML_loops_Sec_OF_6.sh
sbatch aleML_loops_Sec_OF_6.sh
# Submitted batch job 28039326
ls *.ale.uml_rec | wc -l
# 3218
#alright, we're finally good to go!
#Sec_SP/
ls *.ufboot | while read file; do
	python ../../Scripts/ale_tree_spp_names.py $file;
done
grep -l "G_intestinalis_BGS__B" SP_Sec3__OG_*.ufboot
# SP_Sec3__OG_118_MSA_IQ.ufboot
# SP_Sec3__OG_1220_MSA_IQ.ufboot
# SP_Sec3__OG_1577_MSA_IQ.ufboot
# SP_Sec3__OG_163_MSA_IQ.ufboot
# SP_Sec3__OG_181_MSA_IQ.ufboot
# SP_Sec3__OG_207_MSA_IQ.ufboot
# SP_Sec3__OG_356_MSA_IQ.ufboot
# SP_Sec3__OG_3_MSA_IQ.ufboot
# SP_Sec3__OG_405_MSA_IQ.ufboot
# SP_Sec3__OG_539_MSA_IQ.ufboot
# SP_Sec3__OG_69_MSA_IQ.ufboot
# SP_Sec3__OG_78_MSA_IQ.ufboot
# SP_Sec3__OG_8188_MSA_IQ.ufboot
# SP_Sec3__OG_84_MSA_IQ.ufboot
# SP_Sec3__OG_969_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_118_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_1220_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_1577_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_163_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_181_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_207_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_356_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_3_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_405_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_539_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_69_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_78_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_8188_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_84_MSA_IQ.ufboot
python ../../Scripts/ale_tree_spp_names.py SP_Sec3__OG_969_MSA_IQ.ufboot
#now onto the ALE
sbatch aleObserve_loops_Sec_SP.sh
# Submitted batch job 27963660
sbatch aleML_loops_Sec_SP_1.sh
# Submitted batch job 27964740
#this job timed out, so creating a new script for the remaining file
sbatch aleML_loops_Sec_SP_2.sh
# Submitted batch job 27996032
sbatch aleML_loops_Sec_SP_3.sh
# Submitted batch job 27996038
sbatch aleML_loops_Sec_SP_4.sh
# Submitted batch job 28013871
ls *.names.ufboot | wc -l
# 2386
ls *.ale | wc -l
# 2386
ls *.ale.uml_rec | wc -l
# 2386
#all files have been made! 

```

The results of the `ALEml_undated` program can be parsed to yield further data (script saved to: parse_ALE_results.py): 
 - Note that the Singularity build I managed to use does *NOT* create a consensus tree
 - The .uml_rec file that the program creates, is, however, complete, and so can be parsed to yield further information (ref: https://github.com/ssolo/ALE/issues/33)

Ok so the thing I was originally trying (editing Max's scripts) is proving entirely too unweildy. So I figured it was time to go back to the drawing board. Fundementally, I have a tree from ALE. What I need to do, is edit the node labels on that tree. 
 - https://www.youtube.com/watch?v=wBdz3vFQ4Ks
 - https://biopython.org/wiki/Phylo

So! Here's what we're gonna do: 
 1. Script to pull out the data table from the .uml_rec files (parse_ALE_Events.py) ~~as well as summary data (into different results file)~~ [summary data line doesn't specify nodes; better to just parse the larger table]
 2. Use bash to consolidate pre-OG tables into 1 large table per filtration & program type (4 total)
 3. Use Pandas to parse the large data table to get a summary of transfers, losses, etc. per *node*
 4. Use `Bio.Phylo` and `matplotlib` (and others?) to visualize the species tree, annotated with the data from the per-node dataframe
 5. Create parsing script for large annotation files, to be able to extract data on per-node basis, as well as per-OG bases 

Script to pull out the data table from the .uml_rec files (script saved to parse_ALE_Events.py)

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_ALE_Events.py
Date: 2022.06.24
Author: Virág Varga
Based on the work of: Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil)

Description:
	This program parses a *.ale.uml_rec file produced by the ALEml_undated program (part 
		of the ALE suite, which can be found here: https://github.com/ssolo/ALE), in
		order to output a text file with only the DTL events summarized in a tab-separated
		format, which can be used for further data parsing. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys

Procedure:
	1. Loading required module; assigning command-line argument.
	2. Parsing DTL events data from the ALE *.ale.uml_rec file into a tab-separated 
		output text file.  

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but is instead based on the input
		file name.  

Citation: 
	This program is a based off of the ALE parsing programs used in the ALE-pipeline 
		program written by Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil), 
		which can be found here: https://github.com/maxemil/ALE-pipeline
		I have taken inspiration from their methods of parsing the ALE data, as well as 
		adapted portions of their code, particularly the extractDTLevents.py script, 
		which can be found here: 
				https://github.com/maxemil/ALE-pipeline/blob/master/templates/extractDTLevents.py

Usage
	./parse_ALE_Events.py input_tree
	OR
	python parse_ALE_Events.py input_tree
	
	Where the input_tree file should be a *.ale.uml_rec file produced be the ALEml_undated 
		program, which is part of the ALE suite. 

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary module; assign command-line argument

#import necessary modules
import sys #allows assignment of command line arguments


#assign command line arguments; load input and output files
input_tree = sys.argv[1]
#input_tree = "SP_Mito3__OG_2636_MSA_IQ.names.ufboot.ale.uml_rec"

#determine output file name based on input file name
input_name_list = input_tree.split(".")
#save contents of the name to a list
gene_family_basename = input_name_list[0]
#save the file basename
#and the gene family ID
gene_family_ID = gene_family_basename.replace("_MSA_IQ", "")
#and finally, designate the output file name
output_events = gene_family_basename + "_ALE.events.txt"


#Part 2: Parsing DTL events into a data file
#This portion of code is adapted from the extractDTLevents.py script of the ALE-pipeline
#Original script: https://github.com/maxemil/ALE-pipeline/blob/master/templates/extractDTLevents.py

with open(input_tree, "r") as f, open(output_events, 'w') as outhandle:
	#open the input ALE results file fro reading; and the events parsing output file for writing
	outhandle.write("Gene_Family" + "\t" + "Node" + "\t" + "Duplications" + "\t" + "Transfers"+ "\t" + 
				 "Losses" + "\t" + "Originations" + "\t" + "Copies" + "\n")
	#write out column headers for the data that will go into the outfile
	for line in f:
		#iterate over the input file line by line
		if any([line.startswith(x) for x in ['S_terminal_branch', 'S_internal_branch']]):
			#find the lines of the file that have the DTL data table
			line = line.split()
			#split the line into a list, based on the tab placement
			#and print those portions, along with the gene family ID, to a tab-separated text file
			#print("\t".join([gene_family_ID] + line[1:]), file=outhandle)
			#the above raised a syntax error when I tried to run it on Uppmax
			out_string = "\t".join([gene_family_ID] + line[1:])
			outhandle.write(out_string + "\n")

```

Using it: 

```bash
#initial testing in the / directory
#model: 
python parse_ALE_Events.py input_tree
#applying it: 
ls *.ale.uml_rec | while read file; do python parse_ALE_Events.py $file; done
#great! working as intended
###
#actually using it: 
#model: 
python parse_ALE_Events.py input_tree
#applying it: 
ls *.ale.uml_rec | while read file; do 
	python parse_ALE_Events.py $file; 
done
#1-line version for Uppmax: 
ls *.ale.uml_rec | while read file; do python ../../Scripts/parse_ALE_Events.py $file; done
# SyntaxError: invalid syntax
#   File "../../Scripts/parse_ALE_Events.py", line 92
#     print("\t".join([gene_family_ID] + line[1:]), file=outhandle)
#lol whoops, let's figure out what that's about...
#ok, I seem to have figured it out
#completed for: 
#Mito_SP/
ls *.events.txt | wc -l
# 830
#Sec_SP/
ls *.events.txt | wc -l
# 2386
#Mito_OF/
ls *.events.txt | wc -l
# 997
#Sec_OF/
ls *.events.txt | wc -l
# 3218

```

Next, use bash to extract a species tree from one of the files (they're all the same tree ←←← **DOUBLE-CHECK THIS**) and consolidate the per-OG dataframes into one large dataframe (per-program & -filtration type). 

```bash
#species tree extraction from 1 file: 
#ref: https://github.com/maxemil/ALE-pipeline/blob/master/main.nf
#model: 
grep '^S:' $gene | cut -f2 > ${species_tree}.tree
#adapting it: 
grep '^S:' SP_Mito3__OG_2636_MSA_IQ.names.ufboot.ale.uml_rec | cut -f2 > SP_Mito3_Species_Tree.nwk
#here's hoping all of the species trees will look the same!!! we'll have to see...
###
#and the dataframe consolidation
ls *.events.txt | while read file; do
	full_file="${file##*/}"; #this line removes the path before the file name
	file_base="${full_file%.*}"; #this line removes the file extension
	tail -n +2 $file > ${file_base}_temp.txt;
done
head -1 SP_Mito3__OG_2636_MSA_IQ_ALE.events.txt > Events_columns.txt
#the above has to go after the loop because of the naming convention
cat Events_columns.txt *_temp.txt > SP_Mito3__Events_Final.txt
#ok, that's good to go! 
###
#now, using it on the actual data
#Mito_OF/
grep '^S:' OF_Mito3__OG0025016_MSA_IQ.names.ufboot.ale.uml_rec | cut -f2 > OF_Mito3_Species_Tree.nwk
#this seems the same
#1-line version of the loop: 
ls *.events.txt | while read file; do full_file="${file##*/}"; file_base="${full_file%.*}"; tail -n +2 $file > ${file_base}_temp.txt; done
head -1 OF_Mito3__OG0025016_MSA_IQ_ALE.events.txt > Events_columns.txt
#the above has to go after the loop because of the naming convention
cat Events_columns.txt *_temp.txt > OF_Mito3__Events_Final.txt
#finally, copy the file to the local ALE directory: Documents/LundUni/Trich_Parab/Thesis_Work/ALE/
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/OF_Mito3__Events_Final.txt .
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_OF/OF_Mito3_Species_Tree.nwk .
#files successfully transferred
#Mito_SP/
grep '^S:' SP_Mito3__OG_998_MSA_IQ.names.ufboot.ale.uml_rec | cut -f2 > SP_Mito3_Species_Tree.nwk
#this seems the same
#1-line version of the loop: 
ls *.events.txt | while read file; do full_file="${file##*/}"; file_base="${full_file%.*}"; tail -n +2 $file > ${file_base}_temp.txt; done
head -1 SP_Mito3__OG_998_MSA_IQ_ALE.events.txt > Events_columns.txt
#the above has to go after the loop because of the naming convention
cat Events_columns.txt *_temp.txt > SP_Mito3__Events_Final.txt
#finally, copy the file to the local ALE directory: Documents/LundUni/Trich_Parab/Thesis_Work/ALE/
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_SP/SP_Mito3__Events_Final.txt .
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Mito_SP/SP_Mito3_Species_Tree.nwk .
#files successfully transferred
#Sec_OF/ 
grep '^S:' OF_Sec3__OG0025016_MSA_IQ.names.ufboot.ale.uml_rec | cut -f2 > OF_Sec3_Species_Tree.nwk
#this seems the same
#1-line version of the loop: 
ls *.events.txt | while read file; do full_file="${file##*/}"; file_base="${full_file%.*}"; tail -n +2 $file > ${file_base}_temp.txt; done
head -1 OF_Sec3__OG0025016_MSA_IQ_ALE.events.txt > Events_columns.txt
#the above has to go after the loop because of the naming convention
cat Events_columns.txt *_temp.txt > OF_Sec3__Events_Final.txt
#finally, copy the file to the local ALE directory: Documents/LundUni/Trich_Parab/Thesis_Work/ALE/
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_OF/OF_Sec3__Events_Final.txt .
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_OF/OF_Sec3_Species_Tree.nwk .
#files successfully transferred
#Sec_SP/
grep '^S:' SP_Sec3__OG_99_MSA_IQ.names.ufboot.ale.uml_rec | cut -f2 > SP_Sec3_Species_Tree.nwk
#this seems the same
#1-line version of the loop: 
ls *.events.txt | while read file; do full_file="${file##*/}"; file_base="${full_file%.*}"; tail -n +2 $file > ${file_base}_temp.txt; done
head -1 SP_Sec3__OG_99_MSA_IQ_ALE.events.txt > Events_columns.txt
#the above has to go after the loop because of the naming convention
cat Events_columns.txt *_temp.txt > SP_Sec3__Events_Final.txt
#finally, copy the file to the local ALE directory: Documents/LundUni/Trich_Parab/Thesis_Work/ALE/
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_SP/SP_Sec3__Events_Final.txt .
scp vivarga@rackham.uppmax.uu.se:/proj/rhodoquinone_2022/nobackup/Vi_TrichoCompare/ALE_Files/Sec_SP/SP_Sec3_Species_Tree.nwk .
#files successfully transferred

```

Script to parse the large per-filtration & -program dataframe into a table that summarizes events per node (script saved to parse_ALE_Nodes__v2.py): 
 - **NOTE**: By using the `df.round()` functionality, my standards of proof for any given node are higher than those used in the Hikarchaea paper! They rounded up from 0.3, but I use standard rounding: >0.5 rounds 1, while =<0.5 rounds to 0.

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_ALE_Nodes__v2.py
Date: 2022.06.25
Author: Virág Varga
With gratitude to: Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil)

Description:
	This program iterates over a file containing data on the types of events taking place
		at each node per OG, and consolidates it into a summary data table showing the 
		total number of each type of event occurring at each node. 
	The data to be used as input should be dreived from the results of the ALE program, 
		after the initial parsing of the data with the parse_ALE_Events.py script and subsequent
		consolidation, or a filtered version of the same (via the parse_ALE_Annotations.py
		program in this workflow). 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; assigning command-line arguments.
	2. Importing the contents of the DTL events data table into a Pandas dataframe.
		Optionally, create a indexing dictionary to enable conversion of nodes in 
			the dataframe to those used in R. 
	3. Consolidating the DTL events dataframe so that each row summarizes the number 
		of each event type occurring per node. 
		Optionally, use an input indexing file to convert the node numbers to those
			used in R. 
	4. Writing out the results to a tab-separated text file. 

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but is instead based on the input
		file name.  

Citation: 
	This program is a based off of the ALE parsing programs used in the ALE-pipeline 
		program written by Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil), 
		which can be found here: https://github.com/maxemil/ALE-pipeline

Version: 
	This is Version 2.0 of this program. It can optionally output a version of the 
		summary annotation table where the node numbers have been adjusted to match 
		those generated internally when the species tree is read into R, if an 
		indexing file is provided in the format:  R_Nodes\tOriginal_Nodes

Usage
	./parse_ALE_Nodes__v2.py input_events [R_indexing_file]
	OR
	python parse_ALE_Nodes__v2.py input_events [R_indexing_file]

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_events = sys.argv[1]
#input_events = "SP_Mito3__Events_final.txt"

#determine output file name based on input file name
base = os.path.basename(input_events)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_db = out_full + "_Nodes.txt"

if len(sys.argv) == 3: 
	#if the optional indexig file for node number conversion
	#to match the numbering system internal to R
	#is provided, then identify the indexng file
	R_indexing_file = sys.argv[2]
	#R_indexing_file = "ALE_2R_Indexing.txt"
	#and define a new output file
	output_db_R = out_full + "_Nodes_R.txt"


#Part 2: Import the contents of the DTL events data table into a Pandas dataframe
events_df = pd.read_csv(input_events, sep = '\t', header=0)
#the file is a tab-delimited text file, so the separator needs to be specified


if 'output_db_R' in vars(): 
	#check to see whether the option R-formatted node file has been requested
	#if so, create a an empty dictionary to store the node indexing information
	#ref: https://stackoverflow.com/questions/53961659/python-how-to-convert-txt-file-two-columns-to-dictionary-elements
	indexing_dict = {}
	with open(R_indexing_file, "r") as infile: 
		#open the indexing file for reading
		next(infile)
		#and skip the first line of the file  since it's a header line
		for line in infile: 
			#read through the indexing file line by line
			value, key = line.strip().split() 
			#split each line into two elements based on the tab placement
			#and remove the newline character at the end of the line
			#then save the node values to the indexing dictionary
			indexing_dict[key] = (value) 


#Part 3: Consolidate table so that each row summarizes the number of each event type per node

#drop the first column, containing the gene family IDs
events_df = events_df.iloc[: , 1:]

#group the rows based on the node IDs in the Node column
node_summary_df = events_df.groupby(['Node']).sum()
#and round the values to whole numbers for easier visualization
node_summary_df = node_summary_df.round(decimals=0)

#pull the nodes column out of the index
node_summary_df.reset_index(inplace=True)


if 'output_db_R' in vars(): 
	#check to see whether the option R-formatted node file has been requested
	#then use the dictionary to replace the ALE-derived node IDs 
	#with the R-generated node IDs
	#ref: https://sparkbyexamples.com/pandas/pandas-remap-values-in-column-with-a-dictionary-dict/#:~:text=Using%20Pandas%20DataFrame.-,replace(),regular%20expressions%20for%20regex%20substitutions.
	node_summary_df_R = node_summary_df.replace({"Node": indexing_dict})
	#and write out the results to a tab-separated text file
	node_summary_df_R.to_csv(output_db_R, sep = '\t', index=False)


#Part 4: Write out results to a tab-separated text file
node_summary_df.to_csv(output_db, sep = '\t', index=False)

```

Using it: 

```bash
#initial testing: 
#model: 
python parse_ALE_Nodes.py input_events
#applying it: 
python parse_ALE_Nodes.py SP_Mito3__Events_Final.txt
#and it works! so now on to the hard part, lol
###
#using the new version intended to work with the R version of the tree: 
#model: 
python parse_ALE_Nodes__v2.py input_events [R_indexing_file]
#applying it: 
python parse_ALE_Nodes__v2.py SP_Mito3__Events_Final.txt ALE_2R_Indexing.txt
###
#actually running it on the data: 
#model: 
python parse_ALE_Nodes__v2.py input_events [R_indexing_file]
#applying it: 
#Mito_OF/
python parse_ALE_Nodes__v2.py OF_Mito3__Events_Final.txt ALE_2R_Indexing.txt
#Mito_SP/ 
python parse_ALE_Nodes__v2.py SP_Mito3__Events_Final.txt ALE_2R_Indexing.txt
#Sec_OF/
python parse_ALE_Nodes__v2.py OF_Sec3__Events_Final.txt ALE_2R_Indexing.txt
#Sec_SP/
python parse_ALE_Nodes__v2.py SP_Sec3__Events_Final.txt ALE_2R_Indexing.txt

```

Script to visualize the per-node data on an annotated species tree showing numbers of event types per node (script saved to parse_ALE_Visualize.py): 

Actually, I think we're gonna do this in R. Because apparently Python just is not good at phylogenetic tree visualization. 
 - https://cran.r-project.org/web/packages/ape/vignettes/DrawingPhylogenies.pdf
 - https://bioc.ism.ac.jp/packages/3.3/bioc/vignettes/ggtree/inst/doc/treeAnnotation.html
 - https://dendropy.org/primer/chars.html
 - https://yulab-smu.top/treedata-book/chapter5.html
 - https://stackoverflow.com/questions/70825726/adding-multiple-labels-to-a-branch-in-a-phylogenetic-tree-using-geom-label

```R
#!/usr/bin/env Rscript

###


# Title: parse_ALE_VisualizeAnnot.R
# Date: 2022.06.25
# Author: Virág Varga
# With gratitude to: Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil)
# 
# Description:
#   This program annotates the nodes/branches of an input phylogenetic species tree with data 
#     derived from a summary data table showing information relevant to the nodes. 
#   The resulting tree is output to an SVG file. 
# 
# List of functions:
#   No functions are defined in this script.
# 
# List of standard and non-standard modules used:
#   tools
#   ape
#   svglite
# 
# Procedure:
#   1. Load necessary libraries; assign command-line arguments
#   2. Import node annotation data & species tree
#   3. Add node label annotations to tree
#   4. Write out results to file
# 
# Known bugs and limitations:
#   - There is no quality-checking integrated into the code.
#   - The output file name is not entirely user-defined, but is instead based on the input
#     file name plus a user-provided extension.
# 
# Citation: 
#   This program is a based off of the ALE parsing programs used in the ALE-pipeline 
# program written by Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil), 
# which can be found here: https://github.com/maxemil/ALE-pipeline
# 
# Usage: 
# ./parse_ALE_VisualizeAnnot.R tree_file events_file output_ext
# OR
# Rscript parse_ALE_VisualizeAnnot.R tree_file events_file output_ext
# 
# This script was written for R 4.2.1, in RStudio 2022.02.3 Build 492.


###


#Part 1: Load necessary libraries; assign command-line arguments

#setting up the workspace
#clear the environment
rm(list = ls())
#set working directory
setwd('C:/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/ALE')


#load libraries
library(tools)
library(ape)
library(svglite)


#load input files & determine output file names
#ref: https://stackoverflow.com/questions/750786/whats-the-best-way-to-use-r-scripts-on-the-command-line-terminal
#ref: https://www.r-bloggers.com/2015/09/passing-arguments-to-an-r-script-from-command-lines/
#set up R to take command-line input (ie. accept any input file)
#args = commandArgs(trailingOnly=TRUE)

#determine input files
#the tree file
#tree_file <- args[1]
#tree_file <- "SP_Mito3_Species_Tree.nwk"
#tree_file <- "OF_Mito3_Species_Tree.nwk"
#tree_file <- "SP_Sec3_Species_Tree.nwk"
tree_file <- "OF_Sec3_Species_Tree.nwk"

#the events dataframe
#events_file <- args[2]
#events_file <- "SP_Mito3__Events_Final_Nodes_R.txt"
#events_file <- "OF_Mito3__Events_Final_Nodes_R.txt"
#events_file <- "SP_Sec3__Events_Final_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_Nodes_R.txt"

#determining the output file names from the input file names
#usr_file_ext <- args[3]
usr_file_ext <- "ALL"
#strip the file extension
#ref: https://stackoverflow.com/questions/29113973/get-filename-without-extension-in-r
tree_base <- file_path_sans_ext(basename(tree_file))
out_base <- paste(tree_base, usr_file_ext, sep = "_")

#ref: https://www.rdocumentation.org/packages/svglite/versions/2.1.0/topics/svglite
svglite(paste(out_base, "svg", sep = "."), width = 80, height = 30)


#Part 2: Import node annotation data & species tree

#import node annotation data
#events_df = read.table(args[2], sep="\t", header=TRUE)
events_df = read.table(events_file, sep="\t", header=TRUE)
#ref: https://www.portfolioprobe.com/user-area/documentation/portfolio-probe-cookbook/data-basics/read-a-tab-separated-file-into-r/

#read in species tree using the ape library
#ref: https://cran.r-project.org/web/packages/TreeTools/vignettes/load-trees.html
spp_tree <- read.tree(tree_file)
#ref: https://www.geeksforgeeks.org/create-plot-window-of-particular-size-in-r/
#dev.new(width=200, height=100, unit="cm")
#the above was used during initial testing of label dimensions & details, prior to export
plot(spp_tree)

#obtain key node & branch numbers with: 
# nodelabels()
# tiplabels()
# edgelabels()


#Part 3: Add node label annotations to tree

#ref: https://search.r-project.org/CRAN/refmans/ape/html/nodelabels.html
#ref: https://stackoverflow.com/questions/18706665/extracting-value-based-on-another-column
#ref: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf
#label the root node
#this one needs special adjustments due to placement
nodelabels(paste(paste("Duplications", events_df$Duplications[events_df$Node==33], sep = ": "), paste("Transfers", events_df$Transfers[events_df$Node==33], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==33], sep = ": "), paste("Originations", events_df$Originations[events_df$Node==33], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==33], sep = ": "), sep = "; "), 33, adj = 0.2, cex = 0.8)
#labeling the tips of the tree
#ref: https://www.w3schools.com/r/r_for_loop.asp
for (y in 1:26) {
  tiplabels(paste(paste("Duplications", events_df$Duplications[events_df$Node==y], sep = ": "), paste("Transfers", events_df$Transfers[events_df$Node==y], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==y], sep = ": "), paste("Originations", events_df$Originations[events_df$Node==y], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==y], sep = ": "), sep = "; "), y, adj = -1, cex = 0.8, bg = "darkseagreen1")
}
for (y in 27:32) {
  tiplabels(paste(paste("Duplications", events_df$Duplications[events_df$Node==y], sep = ": "), paste("Transfers", events_df$Transfers[events_df$Node==y], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==y], sep = ": "), paste("Originations", events_df$Originations[events_df$Node==y], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==y], sep = ": "), sep = "; "), y, adj = -0.6, cex = 0.8, bg = "darkseagreen1")
}
#above, needed to pull the Giardias into a separate group, because I could not find a way
#to make R not cut off the node labels at the edge of the figure
#started with `cex = -0.8`, but needed -0.7 for multiple double-digits, & -0.6 for 4-digit tallies
#labeling the internal nodes of the tree
for (x in 34:63) {
  nodelabels(paste(paste("Duplications", events_df$Duplications[events_df$Node==x], sep = ": "), paste("Transfers", events_df$Transfers[events_df$Node==x], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==x], sep = ": "), paste("Originations", events_df$Originations[events_df$Node==x], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==x], sep = ": "), sep = "; "), x, cex = 0.8)
}


#Part 4: Write out results to file
dev.off()

```

Using it: 

```bash
#initial testing: 
#model: 
./parse_ALE_VisualizeAnnot.R tree_file events_file output_ext
#adapting it: 
./parse_ALE_VisualizeAnnot.R SP_Mito3_Species_Tree.nwk SP_Mito3__Events_Final_Nodes_R.txt CMD_TEST
Rscript parse_ALE_VisualizeAnnot.R SP_Mito3_Species_Tree.nwk SP_Mito3__Events_Final_Nodes_R.txt CMD_TEST
###
#really cannot figure out the command line stuff, actually, so we're doing it old-school
#ie. running each file from withing RStudio, instead of via the command line

```

Using the above script, the trees below were created, yielding the summary statistics found below: 

### Mitochondria 3 OF

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Duplications: 158
   - Transfers: 0
   - Losses: 2
   - Originations: 286
   - Copies: 443
 - Parabasalid/Anaeramoebid common ancestor: 
   - Duplications: 95
   - Transfers: 160
   - Losses: 54
   - Originations: 44
   - Copies: 688
 - Parabasalia: 
   - Duplications: 1458
   - Transfers: 103
   - Losses: 131
   - Originations: 151
   - Copies: 2268
 - Anaeramoebidae: 
   - Duplications: 124
   - Transfers: 69
   - Losses: 167
   - Originations: 19
   - Copies: 732
 - Preaxostyla: 
   - Duplications: 101
   - Transfers: 110
   - Losses: 150
   - Originations: 5
   - Copies: 748
 - Fornicata: 
   - Duplications: 146
   - Transfers: 41
   - Losses: 131
   - Originations: 30
   - Copies: 882
 - _Trichomonas vaginalis_: 
   - Duplications: 1441
   - Transfers: 70
   - Losses: 1427
   - Originations: 24
   - Copies: 3581
 - _Giardia_ spp.: 
   - Duplications: 43
   - Transfers: 19
   - Losses: 123
   - Originations: 36
   - Copies: 576

![ALE visualized results Mito OF](ALE/OF_Mito3_Species_Tree_ALL.svg)


### Mitochondria 3 SP

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Duplications: 112
   - Transfers: 0
   - Losses: 1
   - Originations: 298
   - Copies: 408
 - Parabasalid/Anaeramoebid common ancestor: 
   - Duplications: 72
   - Transfers: 98
   - Losses: 33
   - Originations: 51
   - Copies: 597
 - Parabasalia: 
   - Duplications: 942
   - Transfers: 63
   - Losses: 104
   - Originations: 146
   - Copies: 1644
 - Anaeramoebidae: 
   - Duplications: 101
   - Transfers: 55
   - Losses: 135
   - Originations: 20
   - Copies: 637
 - Preaxostyla: 
   - Duplications: 41
   - Transfers: 54
   - Losses: 119
   - Originations: 4
   - Copies: 505
 - Fornicata: 
   - Duplications: 84
   - Transfers: 24
   - Losses: 97
   - Originations: 30
   - Copies: 643
 - _Trichomonas vaginalis_: 
   - Duplications: 1538
   - Transfers: 35
   - Losses: 762
   - Originations: 17
   - Copies: 3177
 - _Giardia_ spp.: 
   - Duplications: 158
   - Transfers: 37
   - Losses: 12
   - Originations: 79
   - Copies: 447

![ALE visualized results Mito SP](ALE/SP_Mito3_Species_Tree_ALL.svg)


### Secretome 3 OF

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Duplications: 304
   - Transfers: 0
   - Losses: 3
   - Originations: 695
   - Copies: 996
 - Parabasalid/Anaeramoebid common ancestor: 
   - Duplications: 184
   - Transfers: 269
   - Losses: 104
   - Originations: 156
   - Copies: 1500
 - Parabasalia: 
   - Duplications: 2706
   - Transfers: 183
   - Losses: 259
   - Originations: 542
   - Copies: 4671
 - Anaeramoebidae: 
   - Duplications: 239
   - Transfers: 127
   - Losses: 403
   - Originations: 66
   - Copies: 1530
 - Preaxostyla: 
   - Duplications: 168
   - Transfers: 194
   - Losses: 262
   - Originations: 24
   - Copies: 1497
 - Fornicata: 
   - Duplications: 226
   - Transfers: 67
   - Losses: 287
   - Originations: 67
   - Copies: 1624
 - _Trichomonas vaginalis_: 
   - Duplications: 2602
   - Transfers: 107
   - Losses: 2812
   - Originations: 56
   - Copies: 7149
 - _Giardia_ spp.: 
   - Duplications: 79
   - Transfers: 27
   - Losses: 217
   - Originations: 45
   - Copies: 1062

![ALE visualized results Sec OF](ALE/OF_Sec3_Species_Tree_ALL.svg)


### Secretome 3 SP

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Duplications: 230
   - Transfers: 0
   - Losses: 2
   - Originations: 826
   - Copies: 1053
 - Parabasalid/Anaeramoebid common ancestor: 
   - Duplications: 144
   - Transfers: 188
   - Losses: 73
   - Originations: 139
   - Copies: 1452
 - Parabasalia: 
   - Duplications: 2094
   - Transfers: 144
   - Losses: 250
   - Originations: 467
   - Copies: 3907
 - Anaeramoebidae: 
   - Duplications: 223
   - Transfers: 111
   - Losses: 349
   - Originations: 64
   - Copies: 1500
 - Preaxostyla: 
   - Duplications: 101
   - Transfers: 108
   - Losses: 247
   - Originations: 14
   - Copies: 1271
 - Fornicata: 
   - Duplications: 166
   - Transfers: 43
   - Losses: 265
   - Originations: 53
   - Copies: 1435
 - _Trichomonas vaginalis_: 
   - Duplications: 3153
   - Transfers: 79
   - Losses: 1813
   - Originations: 23
   - Copies: 7156
 - _Giardia_ spp.: 
   - Duplications: 80
   - Transfers: 21
   - Losses: 146
   - Originations: 39
   - Copies: 991

![ALE visualized results Sec SP](ALE/SP_Sec3_Species_Tree_ALL.svg)


In order to provide more information on the functions of the proteins within each OG, PFam information was added to the large data files (script saved to add_PFam_ALE.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: add_PFam_ALE.py
Date: 2022.07.05
Author: Virág Varga

Description:
	This program adds PFam annotation information based on OG IDs to a concatenated
		ALE results table reporting OG information per gene family and per node.

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; defining inputs and outputs as command line
		arguments.
	2. Importing the data into Pandas dataframes.
	3. Reformatting the input dataframe to match the PFam dataframe's data formatting.
	4. Adding the PFam annotation information into the input dataframe, and 
		restructuring the column order.
	5. Writing out the results to a tab-delimited text file.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
    - The output file name is not user-defined, but is instead based on the input
        file name. 
	- This program requires the input of a file in the format [OG_ID]\tQuery\tSpecies_Id
		(created by either the og_prot_spp_list.py or filter_OG_profile.py scripts) and a
		file in the format [OG_ID]\t[PFam_Data] (created by the og2PFam_pivot__v2.py script).

Version:
	This program is modified from the create_counts_table.py script. The original was 
		written to create the data table that could be used as input for the Count 
		program, while this script adds annotations to the concatenated parsed 
		results of the ALE pipeline. 

Usage
	./add_PFam_ALE.py input_db input_pfams
	OR
	python add_PFam_ALE.py input_db input_pfams

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allow execution of code from the command line
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "SP_Mito3__Events_Final.txt"
input_pfams = sys.argv[2]
#input_pfams = "OG_Lists/OGs2PFams_SonicParanoid_OG_Counts.txt"

#determine output file name based on input file name
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_db = out_full + "_PFam.txt"


#Part 2: Import the data into Pandas dataframes

input_df = pd.read_csv(input_db, sep = '\t', header=0)
#read in the filtered OG_ID\tQuery\tSpecies_Id dataframe

pfam_df = pd.read_csv(input_pfams, sep = '\t', header=0)
#creating a dataframe for the OG_ID\tPFam
og_col = pfam_df.columns[0]
#extract the name of the OG ID columns


#Part 3: Reformat input dataframe to match PFam dataframe data formatting

input_df[og_col] = input_df['Gene_Family'] 
#duplicate the OG column of the input dataframe
#name the new column to match the PFam dataframe

input_df[og_col] = input_df[og_col].str.split('__').str.get(-1)
#split elements of the column based on the placement of the double underscore ("__")
#ref: https://stackoverflow.com/questions/63934605/pandas-dataframe-column-remove-string-before-the-first-specific-character


#Part 4: Adding the PFam information into the dataframe

annot_df = input_df.merge(pfam_df, on=og_col, how='left').fillna('-')
#want to do left merge, because the PFam reference dataframe has information on all OGs
#but we only need the information for the filtered OGs in the pivot table
#this adds the annotation column to the rightmost end, which is where it needs to be

annot_df.drop([og_col], axis=1, inplace=True)
#drop the now-unnecessary original OG name column

pfam_col = annot_df.columns[-1]
#extract the name of the PFam annotation column
#using a variable here enables both count-based and non-count-based PFam annotation
annot_df = annot_df[['Gene_Family', pfam_col, 'Node', 'Duplications', 'Transfers', 
					 'Losses', 'Originations', 'Copies']]
#rearrange the column order to pull PFam information into the second column


#Part 5: Writing out the results

annot_df.to_csv(output_db, sep='\t', index=False)
#write out results to a tab-delimited text file

```

Using it: 

```bash
#model: 
python add_PFam_ALE.py input_db input_pfams
#applying it: 
python add_PFam_ALE.py OF_Mito3__Events_Final.txt OG_Lists/OGs2PFams_OrthoFinder_OG.txt
python add_PFam_ALE.py OF_Sec3__Events_Final.txt OG_Lists/OGs2PFams_OrthoFinder_OG.txt
python add_PFam_ALE.py SP_Mito3__Events_Final.txt OG_Lists/OGs2PFams_SonicParanoid_OG.txt
python add_PFam_ALE.py SP_Sec3__Events_Final.txt OG_Lists/OGs2PFams_SonicParanoid_OG.txt

```

Script to parse large per-fitration & -program annotation files, in order extract data based on node ID or OG ID (script saved to parse_ALE_Annotation.py): 
 - The results file produced here should be compatible with the ~~parse_ALE_Visualize.py~~ parse_ALE_VisualizeAnnot.R program, to enable easier analysis & visualization of data. 
 - Might also be useful to create a results text file summarizing key statistics related to the query. 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_ALE_Annotation.py
Date: 2022.06.25
Author: Virág Varga
With gratitude to: Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil)

Description:
	This program iterates over a file data derived from ALE relating to the events
		occurring at any given node per gene family. It is intended to be used on 
		the larger data file consolidating this information between all gene families, 
		in order to extract a portion of the dataframe containing queried node or 
		OG IDs. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas
	datetime.datetime

Procedure:
	1. Loading required modules; assigning command-line arguments.
	2. Importing the contents of the DTL events data table into a Pandas dataframe. 
	3. Querying the reference dataframe for the desired data & writing out
		results to a tab-separated text file. 

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but is instead based on the input
		file name.  

Citation: 
	This program is a based off of the ALE parsing programs used in the ALE-pipeline 
		program written by Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil), 
		which can be found here: https://github.com/maxemil/ALE-pipeline

Usage:
	./parse_ALE_Annotation.py input_db query_type query_ids [ouptut_name]
	OR
	python parse_ALE_Annotation.py input_db query_type query_ids [output_name]
	
	Where the query_type can be either: "OG" OR "Node"
	Where the list of query_ids can be given in the following formats:
		- Singular OG ID provided on the command line
		- Comma-separated list of OG IDs provided on the command line (ex. `ID_1,ID_2,ID_3`)
		- File containing list of OG IDs in format: ID1\nID2 etc.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules, assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import pandas as pd #allows manipulation of dataframes in Python
import os #allow access to computer files
from datetime import datetime #access data from system regarding date & time


#designate input file name as variable
input_db = sys.argv[1]


#determine the type of query input being used
query_type = sys.argv[2]
if query_type == "OG":
	#if the input query type is an OG ID
	#designate the query column as the encoded dta column
	query_col = "Gene_Family"
	print("Query type: OG ID")
elif query_type == "Node":
	#if the input query type is a Node ID
	#designate the query column as the unencoded dta column
	query_col = "Node"
	print("Query type: Node ID")
else:
	#if the user does not determine the input query type as OG or Node ID
	#display this error message
	print("Please select query type: OG OR Node")
	#and exit the program
	sys.exit(1)

#save the list of query IDs to search for to a list
query_ids = sys.argv[3]
if os.path.isfile(query_ids):
	#if the input selection of OGs is a file
	with open(query_ids, 'r') as infile:
		#open the file for reading
		#and save the contents of the file (should be a column of protein query IDs) to a list
		query_list = [line.rstrip('\n') for line in infile]
else:
	#if the input protein query ID list is a string instead of a text file
	#save the contents of the comma-separated string to a list variable
	query_list = query_ids.split(",")

#define the output file
#determine output file name based on input file name
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#determine basename of input file
#and use that basename to determine the output file name
if len(sys.argv) == 4:
	#if no output file name is provided by the user
	#determine date & time of query
	now = datetime.now()
	time_now = now.strftime("%d-%m-%Y--%H%M%S")
	#and create the resulting outfile name
	output_db = out_full + "__QUERY_" + time_now + ".txt"
elif len(sys.argv) == 5:
	#if the user provides an output file name extension
	#then identify the extension given
	output_usr_ext = sys.argv[4]
	#and add that to the end of the basename
	output_db = out_full + "__" + output_usr_ext + ".txt"
else:
	#if the wrong number of command line arguments are used
	#display this error message
	print("The command-line input format is incorrect. It should be: \n \
	   python query_prot_ids.py input_db query_type query_ids [output_name]")
	#and exit the program
	sys.exit(1)


#Part 2: Import events data into Pandas dataframe

events_df = pd.read_csv(input_db, sep = '\t', header=0)
#the file is a tab-delimited text file, so the separator needs to be specified


#Part 3: Query the reference dataframe for the desired data & write out

#search the appropriate column to extract the rows of the dataframe where the query IDs are found
filt_events_df = events_df[events_df[query_col].isin(query_list)].copy()


filt_events_df.to_csv(output_db, sep = '\t', index=False)
#results will be written out to a tab-separated text file

```

Using it: 

```bash
#initial testing: 
#model: 
python parse_ALE_Annotation.py input_db query_type query_ids [output_name]
#applying it: 
python parse_ALE_Annotation.py SP_Mito3__Events_Final.txt Node 33 TEST1
python parse_ALE_Annotation.py SP_Mito3__Events_Final.txt Node 33,34 TEST2
python parse_ALE_Annotation.py SP_Mito3__Events_Final.txt OG SP_Mito3__OG_998 TEST3
#great, got it working! 
#the actual use: 
#model: 
python parse_ALE_Annotation.py input_db query_type query_ids [output_name]
#applying it: 
#Mito OF
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0000021 OG0000021
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0000095 OG0000095
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0002517 OG0002517
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0001149 OG0001149
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0001763 OG0001763
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0001107 OG0001107
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0000573 OG0000573
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0002422 OG0002422
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0000060 OG0000060
python parse_ALE_Annotation.py OF_Mito3__Events_Final_PFam.txt OG OF_Mito3__OG0000622 OG0000622
#Mito SP
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_14 OG_14
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_2705 OG_2705
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_4 OG_4
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_470 OG_470
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_141 OG_141
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_571 OG_571
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_504 OG_504
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_55 OG_55
python parse_ALE_Annotation.py SP_Mito3__Events_Final_PFam.txt OG SP_Mito3__OG_761 OG_761
#Sec OF
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000021 OG0000021
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000095 OG0000095
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000566 OG0000566
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000676 OG0000676
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001187 OG0001187
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001551 OG0001551
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0002331 OG0002331
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0002517 OG0002517
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0002575 OG0002575
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0002623 OG0002623
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0002903 OG0002903
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0003539 OG0003539
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0004167 OG0004167
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0006080 OG0006080
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0006972 OG0006972
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0007664 OG0007664
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0008051 OG0008051
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0008267 OG0008267
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0008999 OG0008999
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0013056 OG0013056
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0013137 OG0013137
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0013654 OG0013654
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0013661 OG0013661
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0016109 OG0016109
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0016279 OG0016279
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0017109 OG0017109
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0018812 OG0018812
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0018852 OG0018852
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0019887 OG0019887
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000911 OG0000911
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0005001 OG0005001
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0024821 OG0024821
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0004934 OG0004934
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0005930 OG0005930
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0011500 OG0011500
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0021737 OG0021737
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001012 OG0001012
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001149 OG0001149
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0018837 OG0018837
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0020781 OG0020781
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001763 OG0001763
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0012862 OG0012862
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0016908 OG0016908
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000308 OG0000308
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0012073 OG0012073
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0073298 OG0073298
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0069440 OG0069440
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000019 OG0000019
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0002700 OG0002700
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0000012 OG0000012
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001259 OG0001259
python parse_ALE_Annotation.py OF_Sec3__Events_Final_PFam.txt OG OF_Sec3__OG0001492 OG0001492
#Sec SP
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_10301 OG_10301
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1169 OG_1169
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_122 OG_122
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_12460 OG_12460
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1317 OG_1317
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1357 OG_1357
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_14 OG_14
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_226 OG_226
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_2278 OG_2278
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_2309 OG_2309
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_2407 OG_2407
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_2759 OG_2759
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_4 OG_4
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_5397 OG_5397
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_902 OG_902
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_9976 OG_9976
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_319 OG_319
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_4494 OG_4494
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_3306 OG_3306
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_3919 OG_3919
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_5234 OG_5234
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1015 OG_1015
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_14244 OG_14244
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_470 OG_470
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_9768 OG_9768
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_369 OG_369
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_60950 OG_60950
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_149 OG_149
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_2282 OG_2282
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1245 OG_1245
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_8 OG_8
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1519 OG_1519
python parse_ALE_Annotation.py SP_Sec3__Events_Final_PFam.txt OG SP_Sec3__OG_1610 OG_1610
###
#and now, need to convert all of these to R node versions in order to visualize them
#Mito OF
ls OF_Mito3__Events_Final_PFam__*.txt | while read file; do
	python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; 
done
#or in 1 line for Ubuntu... -_-
ls OF_Mito3__Events_Final_PFam__*.txt | while read file; do python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; done
#Mito SP
ls SP_Mito3__Events_Final_PFam__*.txt | while read file; do
	python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; 
done
#or in 1 line for Ubuntu... -_-
ls SP_Mito3__Events_Final_PFam__*.txt | while read file; do python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; done
#Sec OF
ls OF_Sec3__Events_Final_PFam__*.txt | while read file; do
	python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; 
done
#or in 1 line for Ubuntu... -_-
ls OF_Sec3__Events_Final_PFam__*.txt | while read file; do python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; done
#later ones to make up for lack of ALE results for some OGs
python parse_ALE_Nodes__v2.py OF_Sec3__Events_Final_PFam__OG0000012.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes__v2.py OF_Sec3__Events_Final_PFam__OG0001259.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes__v2.py OF_Sec3__Events_Final_PFam__OG0001492.txt ALE_2R_Indexing.txt
#Sec SP
ls SP_Sec3__Events_Final_PFam__*.txt | while read file; do
	python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; 
done
#or in 1 line for Ubuntu... -_-
ls SP_Sec3__Events_Final_PFam__*.txt | while read file; do python parse_ALE_Nodes__v2.py $file ALE_2R_Indexing.txt; done
#later ones to make up for lack of ALE results for some OGs
python parse_ALE_Nodes__v2.py SP_Sec3__Events_Final_PFam__OG_8.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes__v2.py SP_Sec3__Events_Final_PFam__OG_1519.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes__v2.py SP_Sec3__Events_Final_PFam__OG_1610.txt ALE_2R_Indexing.txt

```

Modifications/additions to the phylogentic tree display R script (parse_ALE_VisualizeAnnot.R) in order to visualize the files created above (code saved to visualize_ALE_selection.R): 

```R
###
#
# IMPORTANT NOTE!!!
#
# THIS SCRIPT IS NOT INTENDED TO BE SELF-CONTAINED
# IT CANNOT BE RUN ON ITS OWN
#
#
# This script (visualize_ALE_selection.R) was written to accompany the parse_ALE_VisualizeAnnot.R script. 
# This script contains the code modifications necessary (ie. unique file names & new working directory)
# to run the parse_ALE_VisualizeAnnot.R script for the selcted OGs analyzed in the final stage of the TrichoCompare project. 
# These additions to that script are kept separate to avoid cluttering up that script with unnecessary lines of code. 
#
###

#setting up the workspace
#clear the environment
rm(list = ls())

#set working directory
setwd('C:/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/Biol_Interpret/ALE_Data')


#determining the output file names from the input file names
#strip the file extension
#ref: https://stackoverflow.com/questions/29113973/get-filename-without-extension-in-r
out_base <- file_path_sans_ext(basename(events_file))

#ref: https://www.rdocumentation.org/packages/svglite/versions/2.1.0/topics/svglite
svglite(paste(out_base, "svg", sep = "."), width = 80, height = 30)


#Mito OF
events_file <- "OF_Mito3__Events_Final_PFam__OG0000021_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000095_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0002517_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0001149_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0001763_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0001107_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000573_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0002422_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000060_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000622_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000021_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000021_Nodes_R.txt"
events_file <- "OF_Mito3__Events_Final_PFam__OG0000021_Nodes_R.txt"

#Mito SP
events_file <- "SP_Mito3__Events_Final_PFam__OG_14_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_2705_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_4_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_470_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_141_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_571_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_504_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_55_Nodes_R.txt"
events_file <- "SP_Mito3__Events_Final_PFam__OG_761_Nodes_R.txt"

#Sec OF
events_file <- "OF_Sec3__Events_Final_PFam__OG0000019_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000021_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000676_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001551_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001187_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000095_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000566_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0002331_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0002517_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0002575_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0002623_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0002903_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0003539_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0004167_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0006080_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0006972_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0007664_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0008051_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0008267_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0008999_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0013056_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0013137_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0013654_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0013661_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0016109_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0016279_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0017109_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0018812_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0018852_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0019887_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000911_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0005001_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0024821_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0004934_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0005930_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0011500_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0021737_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001012_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001149_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0018837_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0020781_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001763_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0012862_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0016908_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000308_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0012073_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0002700_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0000012_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001259_Nodes_R.txt"
events_file <- "OF_Sec3__Events_Final_PFam__OG0001492_Nodes_R.txt"

#Sec SP
events_file <- "SP_Sec3__Events_Final_PFam__OG_1015_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_10301_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_1169_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_122_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_12460_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_1357_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_14_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_226_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_2278_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_2309_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_2407_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_2759_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_4_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_5397_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_902_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_9976_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_319_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_4494_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_3306_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_3919_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_5234_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_14244_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_470_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_9768_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_369_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_149_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_2282_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_1245_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_8_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_1519_Nodes_R.txt"
events_file <- "SP_Sec3__Events_Final_PFam__OG_1610_Nodes_R.txt"

```


## Data Analysis & Figures

### UpSet Plot

This script was used to create Upset plots in R (script saved to upSetR_plots.R): 

```R
#!/usr/bin/env Rscript

###


# Title: upSetR_plots.R
# Date: 2022.07.02
# Author: Virág Varga
# 
# Description:
#   This program creates upset plots using the binary presence/absence data of 
#       OGs per species. This binary data was obtained via the Count software. 
# 
# List of functions:
#   No functions are defined in this script.
# 
# List of standard and non-standard modules used:
#   tools
#   ape
#   svglite
# 
# Procedure:
#   1. Load necessary libraries; assign command-line arguments
#   2. Load input data into dataframe
#   3. Create UpSetR plot
#   4. Write out results to file
# 
# Known bugs and limitations:
#   - There is no quality-checking integrated into the code.
#   - The output file name is not entirely user-defined, but is instead based on the input
#     file name plus a user-provided extension.
# 
# Version: 
#   This program can be considered a Version 2.0 of the upSetR_etAll.R script prepared
#       for the preliminary exploratory project. 
# 
# Usage: 
# ./upSetR_plots.R input_data output_ext
# OR
# Rscript upSetR_plots.R input_data output_ext
# 
# This script was written for R 4.2.1, in RStudio 2022.02.3 Build 492.


###


#Part 1: Load necessary libraries; assign command-line arguments

#setting up the workspace
#clear the environment
rm(list = ls())
#set working directory
setwd('C:/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/Figures')


#load libraries
library(UpSetR)


#load input files & determine output file names
#ref: https://stackoverflow.com/questions/750786/whats-the-best-way-to-use-r-scripts-on-the-command-line-terminal
#ref: https://www.r-bloggers.com/2015/09/passing-arguments-to-an-r-script-from-command-lines/
#set up R to take command-line input (ie. accept any input file)
#args = commandArgs(trailingOnly=TRUE)

#determine input file - binary count data
#the tree file
#input_data <- args[1]
#input_data <- "Alanta_mito_3_OF_ALL__CountPivot__Binary.txt"
#input_data <- "Alanta_mito_3_SP_ALL__CountPivot__Binary.txt"
#input_data <- "Alanta_sec_3_OF_ALL__CountPivot__Binary.txt"
input_data <- "Alanta_sec_3_SP_ALL__CountPivot__Binary.txt"

#determining the output file names from the input file names
#usr_file_ext <- args[2]
usr_file_ext <- "UpSetR"
#strip the file extension
#ref: https://stackoverflow.com/questions/29113973/get-filename-without-extension-in-r
input_base <- file_path_sans_ext(basename(input_data))
out_base <- paste(input_base, usr_file_ext, sep = "_")
out_base


#Part 2: Load input data into dataframe

#can't use raw frequency data - need to use binary presence/absence data
binary_df <- read.table(input_data, header=TRUE)


#Part 3: Create UpSetR plot
#GitHub package ref: https://github.com/hms-dbmi/UpSetR
#other upset plot ref: https://jokergoo.github.io/ComplexHeatmap-reference/book/upset-plot.html
#upset(binary_df)
#the simple version above shows only a small subsection
#upset plot with all species shown below: 
upset(binary_df,
      sets = c("Anaeramoeba_lanta_160522", "BM_newprots_may21.anaeromoeba", "SC_newprots_may21.anaeromoeba", 
               "BS_newprots_may21.anaeromoeba", "Tetratrichomonas_gallinarum.5730.aa", "Pentatrichomonas_hominis.5728.aa", 
               "Trichomonas_vaginalis_GenBank.PRJNA16084", "Trichomonas_vaginalis_RefSeq.G3", "Dientamoeba_fragilis.43352.aa", 
               "Histomonas_meleagridis.PRJNA594289", "Histomonas_meleagridis.135588.aa", "Tritrichomonas_foetus.PRJNA345179", 
               "EP00708_Paratrimastix_pyriformis", "EP00771_Trimastix_marina", "EP00770_Monocercomonoides_exilis", 
               "EP00792_Barthelona_sp_PAP020", "EP00769_Ergobibamus_cyprinoides", "Carpediemonas_membranifera.PRJNA719540", 
               "EP00764_Aduncisulcus_paluster", "EP00766_Chilomastix_caulleryi", "EP00767_Chilomastix_cuspidata", 
               "Kipferlia_bialata.PRJDB5223",  "EP00768_Dysnectes_brevis", "Spironucleus_salmonicida.PRJNA60811", 
               "EP00703_Trepomonas_sp_PC1", "Giardia_muris.PRJNA524057", "GiardiaDB_GintestinalisEP15", 
               "GiardiaDB_GintestinalisADH", "EP00701_Giardia_intestinalis", "Giardia_intestinalis.PRJNA1439", 
               "GiardiaDB_GintestinalisBGS", "GiardiaDB_GintestinalisBGS_B" ),
      order.by = "freq")
#saved to: Count2_upsetR_Ver1.pdf


#Part 4: Save data to file

#Dimensions for a .PNG should be: 
#Width 3000; Height 1500
#Dimensions for a .PDF should be: 
#20 x 15 in Potrait mode

```

I realized much later that this version of parsing the ALE results is not comparable to results of the Count analysis, because Count summarizes gains, losses and number of _gene families_ whereas ALE does all of its calculations for proteins per gene families. The script below parses an ALE summary data table to create a version of the dataframe comparable to Count (script saved to parse_ALE_Nodes_GFam.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_ALE_Nodes_GFam.py
Date: 2022.08.23
Author: Virág Varga
With gratitude to: Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil)

Description:
	This program iterates over a file containing data on the types of events taking place
		at each node per OG, and consolidates it into a summary data table showing the 
		total number of each type of event occurring at each node. 
	The data to be used as input should be dreived from the results of the ALE program, 
		after the initial parsing of the data with the parse_ALE_Events.py script and subsequent
		consolidation, or a filtered version of the same (via the parse_ALE_Annotations.py
		program in this workflow). 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading required modules; assigning command-line arguments.
	2. Importing the contents of the DTL events data table into a Pandas dataframe.
		Optionally, create a indexing dictionary to enable conversion of nodes in 
			the dataframe to those used in R. 
	3. Simplifying the database to match the structure of the Count results. 
	4. Iterate over the database, converting the protein-based annotation to OG-based
	5. Consolidating the DTL events dataframe so that each row summarizes the number 
		of each event type occurring per node. 
	6. Writing out the results to a tab-separated text file. Optionally, converting 
		the dataframe to the R node format & writing out. 

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- The output file name is not user-defined, but is instead based on the input
		file name.  

Citation: 
	This program is a based off of the ALE parsing programs used in the ALE-pipeline 
		program written by Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil), 
		which can be found here: https://github.com/maxemil/ALE-pipeline

Version: 
	This is can be considered an alternate version of the parse_ALE_Nodes__v2.py
		program. This version of the program summarizes the extensive data produced 
		by the ALE program into a format comparable to the results of the Count 
		program. For each node, the following is reported: whether an OG is lost or 
		gained, and the total number of OGs present at that node. 

Usage
	./parse_ALE_Nodes_GFam.py input_events [R_indexing_file]
	OR
	python parse_ALE_Nodes_GFam.py input_events [R_indexing_file]

This script was written for Python 3.8.12, in Spyder 5.1.5.


"""


#Part 1: Import necessary modules; assign command-line arguments

#import necessary modules
import sys #allows assignment of command line arguments
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line arguments; load input and output files
input_events = sys.argv[1]
#input_events = "OF_Mito3__Events_Final__EXCERPT.txt"

#determine output file name based on input file name
base = os.path.basename(input_events)
out_full = os.path.splitext(base)[0]
#determine basename of input file
output_db = out_full + "_Nodes_GFam.txt"

if len(sys.argv) == 3: 
	#if the optional indexig file for node number conversion
	#to match the numbering system internal to R
	#is provided, then identify the indexng file
	R_indexing_file = sys.argv[2]
	#R_indexing_file = "ALE_2R_Indexing.txt"
	#and define a new output file
	output_db_R = out_full + "_Nodes_R_GFam.txt"


#Part 2: Import the contents of the DTL events data table into a Pandas dataframe
events_df = pd.read_csv(input_events, sep = '\t', header=0)
#the file is a tab-delimited text file, so the separator needs to be specified


if 'output_db_R' in vars(): 
	#check to see whether the option R-formatted node file has been requested
	#if so, create a an empty dictionary to store the node indexing information
	#ref: https://stackoverflow.com/questions/53961659/python-how-to-convert-txt-file-two-columns-to-dictionary-elements
	indexing_dict = {}
	with open(R_indexing_file, "r") as infile: 
		#open the indexing file for reading
		next(infile)
		#and skip the first line of the file  since it's a header line
		for line in infile: 
			#read through the indexing file line by line
			value, key = line.strip().split() 
			#split each line into two elements based on the tab placement
			#and remove the newline character at the end of the line
			#then save the node values to the indexing dictionary
			indexing_dict[key] = (value) 


#Part 3: Simplify the database to match the structure of the Count results

#first copy the relevant columns to a new dataframe
count_df = events_df[['Gene_Family', 'Node', 'Losses', 'Originations', 'Copies']].copy()
#note that the concepts of duplications and transfers
#are not relevant when discussing gene families, instead of proteins
#losses are not strictly speaking meaningful, either, 
#but we'll deal with that in a moment
#ALE doesn't predict multiple originations per OG, so that one will be fine

#round the values in the data columns
count_df = count_df.round({'Losses': 0, 'Originations': 0, 'Copies': 0})
#ref: https://stackoverflow.com/questions/31247763/round-float-columns-in-pandas-dataframe

#convert the Node column to string format for easier parsing
count_df = count_df.astype({"Node": str})
#ref: https://www.stackvidhya.com/pandas-change-column-type/#:~:text=Pandas%20Change%20Column%20Type%20To%20String,-In%20this%20section&text=You%20can%20use%20it%20by,be%20converted%20to%20String%20format.


#create dictionary in format: {Node_Number: Previous_Node_Number}
#this will be used to determine gains and losses of OGs across the tree
phylo_dict = {"Anaeramoeba-lanta-160522": 49, 
			  "BM-newprots-may21.anaeromoeba": 45, 
			  "BS-newprots-may21.anaeromoeba": 32, 
			  "Carpediemonas-membranifera.PRJNA719540": 33, 
			  "Dientamoeba-fragilis.43352.aa": 47, 
			  "EP00701-Giardia-intestinalis": 34, 
			  "EP00703-Trepomonas-sp-PC1": 35, 
			  "EP00708-Paratrimastix-pyriformis": 36, 
			  "EP00764-Aduncisulcus-paluster": 58, 
			  "EP00766-Chilomastix-caulleryi": 37, 
			  "EP00767-Chilomastix-cuspidata": 37, 
			  "EP00768-Dysnectes-brevis": 55, 
			  "EP00769-Ergobibamus-cyprinoides": 33, 
			  "EP00770-Monocercomonoides-exilis": 38, 
			  "EP00771-Trimastix-marina": 36, 
			  "EP00792-Barthelona-sp-PAP020": 60, 
			  "Giardia-intestinalis.PRJNA1439": 34, 
			  "Giardia-muris.PRJNA524057": 50, 
			  "GiardiaDB-GintestinalisADH": 39, 
			  "GiardiaDB-GintestinalisBGS": 40, 
			  "GiardiaDB-GintestinalisBGS-B": 40, 
			  "GiardiaDB-GintestinalisEP15": 41, 
			  "Histomonas-meleagridis.135588.aa": 42, 
			  "Histomonas-meleagridis.PRJNA594289": 42, 
			  "Kipferlia-bialata.PRJDB5223": 56, 
			  "Pentatrichomonas-hominis.5728.aa": 43, 
			  "SC-newprots-may21.anaeromoeba": 32, 
			  "Spironucleus-salmonicida.PRJNA60811": 35, 
			  "Tetratrichomonas-gallinarum.5730.aa": 43, 
			  "Trichomonas-vaginalis-GenBank.PRJNA16084": 44, 
			  "Trichomonas-vaginalis-RefSeq.G3": 44, 
			  "Tritrichomonas-foetus.PRJNA345179": 51, 
			  32: 45, 
			  33: 59, 
			  34: 39, 
			  35: 54, 
			  36: 38, 
			  37: 57, 
			  38: 61, 
			  39: 41, 
			  40: 46, 
			  41: 46, 
			  42: 47, 
			  43: 48, 
			  44: 48, 
			  45: 49, 
			  46: 50, 
			  47: 51, 
			  47: 51, 
			  48: 52, 
			  49: 53, 
			  50: 54, 
			  51: 52, 
			  52: 53, 
			  53: 62, 
			  54: 55, 
			  55: 56, 
			  56: 57, 
			  57: 58, 
			  58: 59, 
			  59: 60, 
			  60: 61, 
			  61: 62
			  #do not need to include node 62 as a key
			  #since the origin will not be undergoing the same style of analysis 
			  #as the other nodes in the tree
			  }

#convert all elements of the dictionary to strings
#ref: https://stackoverflow.com/questions/67600510/convert-all-the-values-in-a-dictionary-to-strings
for keys in phylo_dict:
	#iterated over the dictionary via its keys
	#and convert all elements of the dictionary to strings
	phylo_dict[keys] = str(phylo_dict[keys])
#strictly speaking, I could write fewer lines of code 
#if I individually put quotations around all of the numbers in the dictionary
#but this method is easier on my joints


#Part 4: Iterate over the database, converting the protein-based annotation to OG-based

#next, iterate over the database & convert protein loss to OG loss
for index, row in count_df.iterrows():
	#iterate through the dataframe row by row
	if row['Node'] != 62:
		#skip Node 62 (origin node) since there's nothing to compare to
			if row['Copies'] == 0: 
				#if the number of copies = 0, 
				#check to see if the OG was lost since the previous node
				for phylo_key in phylo_dict: 
					#iterate over the phylo_dict via its keys to obtain the parent node number
					if phylo_key == row['Node']: 
						#find the dictionary entry associated with the present node number
						prev_node = phylo_dict[phylo_key]
						#save the parent node number to the variable prev_node
						OG_ID = row['Gene_Family']
						#save the OG ID to a new variable
						#ref: https://thispointer.com/python-pandas-select-rows-in-dataframe-by-conditions-on-multiple-columns/
						filterinfDataframe = count_df[(count_df['Node'] == prev_node) & (count_df['Gene_Family'] == OG_ID) ]
						#filter out the portion of the count_df containing the previous node ID for that OG
						#and save that one line of the dataframe to a new "dataframe"
						if filterinfDataframe.iloc[0]['Copies'] > 0:  
							#if the parent node still had members of the OG
							prev_count = filterinfDataframe['Copies']
							#save the number of OG members at the parent node to a variable
							#use .iloc to get the value of the specific cell, 
							#since techinically this is still a dataframe
							#ref: https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index
							#replace the value of losses at the search node 
							#with the number of OG members that existed at the parent node
							count_df.at[index, 'Losses'] = prev_count
			else: 
				#if the number of copies > 0
				#ref: https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index
				#replace the value of Copies at the search node with 1
				#to show the presence of members of the OG at the node
				count_df.at[index, 'Copies'] = 1
				#note that this same procedure doesn't need to be repeated for originations
				#because there is never >1 origination per OG
				#but it does need to be repeated for the losses
				#since if the OG exists at a node, it hasn't been lost
				count_df.at[index, 'Losses'] = 0
	else: 
		#for the numbers at node 62 (the origin)
		if count_df.iloc[index]['Copies'] > 0: 
			#if the OG was present at the origin
			#replace the value of Copies at the origin with 1
			#to show the presence of members of the OG at the node
			count_df.at[index, 'Copies'] = 1
		if count_df.iloc[index]['Losses'] > 0: 
			#if the OG was "lost" at the origin
			#replace the value of Losses at the origin with 1
			#to show the "loss" of members of the OG at the node
			#airquotes used because this really shouldn't happen
			#if it does, it's an algorithmic error 
			#which should be kept in this output for clarity's sake
			count_df.at[index, 'Losses'] = 1
			#for the same reason, origination values should not be changed
			#there shouldn't be more than 1 origination
			#if there is, then the algorithm did something weird
			#and that's also important to know


#Part 5: Consolidate table so that each row summarizes the number of each event type per node

#drop the first column, containing the gene family IDs
events_df2 = count_df.iloc[: , 1:]

#group the rows based on the node IDs in the Node column
node_summary_df = events_df2.groupby(['Node']).sum()

#pull the nodes column out of the index
node_summary_df.reset_index(inplace=True)


#Part 6: Write out results to tab-separated text file
node_summary_df.to_csv(output_db, sep = '\t', index=False)


#optionally, convert the dataframe to the R node format & write out
if 'output_db_R' in vars(): 
	#check to see whether the option R-formatted node file has been requested
	#then use the dictionary to replace the ALE-derived node IDs 
	#with the R-generated node IDs
	#ref: https://sparkbyexamples.com/pandas/pandas-remap-values-in-column-with-a-dictionary-dict/#:~:text=Using%20Pandas%20DataFrame.-,replace(),regular%20expressions%20for%20regex%20substitutions.
	node_summary_df_R = node_summary_df.replace({"Node": indexing_dict})
	#and write out the results to a tab-separated text file
	node_summary_df_R.to_csv(output_db_R, sep = '\t', index=False)

```

Using it: 

```bash
#model: 
python parse_ALE_Nodes_GFam.py input_events [R_indexing_file]
#applying it: 
python parse_ALE_Nodes_GFam.py OF_Mito3__Events_Final.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes_GFam.py OF_Sec3__Events_Final.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes_GFam.py SP_Mito3__Events_Final.txt ALE_2R_Indexing.txt
python parse_ALE_Nodes_GFam.py SP_Sec3__Events_Final.txt ALE_2R_Indexing.txt

```

The script below creates new trees in R, with the new database format (script saved to parse_ALE_VisualizeAnnot_GFam.R): 

```R
#!/usr/bin/env Rscript

###


# Title: parse_ALE_VisualizeAnnot_GFam.R
# Date: 2022.08.24
# Author: Virág Varga
# With gratitude to: Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil)
# 
# Description:
#   This program annotates the nodes/branches of an input phylogenetic species tree with data 
#     derived from a summary data table showing information relevant to the nodes. 
#   The resulting tree is output to an SVG file. 
# 
# List of functions:
#   No functions are defined in this script.
# 
# List of standard and non-standard modules used:
#   tools
#   ape
#   svglite
# 
# Procedure:
#   1. Load necessary libraries; assign command-line arguments
#   2. Import node annotation data & species tree
#   3. Add node label annotations to tree
#   4. Write out results to file
# 
# Known bugs and limitations:
#   - There is no quality-checking integrated into the code.
#   - The output file name is not entirely user-defined, but is instead based on the input
#     file name plus a user-provided extension.
# 
# Citation: 
#   This program is a based off of the ALE parsing programs used in the ALE-pipeline 
# program written by Max Emil Schön (@maxemil on GitHub: https://github.com/maxemil), 
# which can be found here: https://github.com/maxemil/ALE-pipeline
# 
# Version: 
#   This can be considered an alternative version of the original parse_ALE_VisualizeAnnot.R program. 
# It makes the results data which has been reformatted to match the results of the Count 
# program by the parse_ALE_Nodes_GFam.py program.
# 
# Usage: 
# ./parse_ALE_VisualizeAnnot_GFam.R tree_file events_file output_ext
# OR
# Rscript parse_ALE_VisualizeAnnot_GFam.R tree_file events_file output_ext
# 
# This script was written for R 4.2.1, in RStudio 2022.02.3 Build 492.


###


#Part 1: Load necessary libraries; assign command-line arguments

#setting up the workspace
#clear the environment
rm(list = ls())
#set working directory
setwd('C:/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/ALE')


#load libraries
library(tools)
library(ape)
library(svglite)


#load input files & determine output file names
#ref: https://stackoverflow.com/questions/750786/whats-the-best-way-to-use-r-scripts-on-the-command-line-terminal
#ref: https://www.r-bloggers.com/2015/09/passing-arguments-to-an-r-script-from-command-lines/
#set up R to take command-line input (ie. accept any input file)
#args = commandArgs(trailingOnly=TRUE)

#determine input files
#the tree file
#tree_file <- args[1]
#tree_file <- "SP_Mito3_Species_Tree.nwk"
#tree_file <- "OF_Mito3_Species_Tree.nwk"
#tree_file <- "SP_Sec3_Species_Tree.nwk"
tree_file <- "OF_Sec3_Species_Tree.nwk"

#the events dataframe
#events_file <- args[2]
#events_file <- "SP_Mito3__Events_Final_Nodes_R_GFam.txt"
#events_file <- "OF_Mito3__Events_Final_Nodes_R_GFam.txt"
#events_file <- "SP_Sec3__Events_Final_Nodes_R_GFam.txt"
events_file <- "OF_Sec3__Events_Final_Nodes_R_GFam.txt"

#determining the output file names from the input file names
#usr_file_ext <- args[3]
usr_file_ext <- "ALL_GFam"
#strip the file extension
#ref: https://stackoverflow.com/questions/29113973/get-filename-without-extension-in-r
tree_base <- file_path_sans_ext(basename(tree_file))
out_base <- paste(tree_base, usr_file_ext, sep = "_")

#ref: https://www.rdocumentation.org/packages/svglite/versions/2.1.0/topics/svglite
svglite(paste(out_base, "svg", sep = "."), width = 80, height = 30)


#Part 2: Import node annotation data & species tree

#import node annotation data
#events_df = read.table(args[2], sep="\t", header=TRUE)
events_df = read.table(events_file, sep="\t", header=TRUE)
#ref: https://www.portfolioprobe.com/user-area/documentation/portfolio-probe-cookbook/data-basics/read-a-tab-separated-file-into-r/

#read in species tree using the ape library
#ref: https://cran.r-project.org/web/packages/TreeTools/vignettes/load-trees.html
spp_tree <- read.tree(tree_file)
#ref: https://www.geeksforgeeks.org/create-plot-window-of-particular-size-in-r/
#dev.new(width=200, height=100, unit="cm")
#the above was used during initial testing of label dimensions & details, prior to export
plot(spp_tree)

#obtain key node & branch numbers with: 
# nodelabels()
# tiplabels()
# edgelabels()


#Part 3: Add node label annotations to tree

#ref: https://search.r-project.org/CRAN/refmans/ape/html/nodelabels.html
#ref: https://stackoverflow.com/questions/18706665/extracting-value-based-on-another-column
#ref: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf
#label the root node
#this one needs special adjustments due to placement
nodelabels(paste(paste(paste("Originations", events_df$Originations[events_df$Node==33], sep = ": "), "Losses", events_df$Losses[events_df$Node==33], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==33], sep = ": "), sep = "; "), 33, adj = 0.2, cex = 0.8)
#labeling the tips of the tree
#ref: https://www.w3schools.com/r/r_for_loop.asp
for (y in 1:26) {
  tiplabels(paste(paste("Originations", events_df$Originations[events_df$Node==y], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==y], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==y], sep = ": "), sep = "; "), y, adj = -1.5, cex = 0.8, bg = "darkseagreen1")
}
for (y in 27:32) {
  tiplabels(paste(paste("Originations", events_df$Originations[events_df$Node==y], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==y], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==y], sep = ": "), sep = "; "), y, adj = -1.2, cex = 0.8, bg = "darkseagreen1")
}
#above, pulled the Giardias into a separate group
#labeling the internal nodes of the tree
for (x in 34:63) {
  nodelabels(paste(paste("Originations", events_df$Originations[events_df$Node==x], sep = ": "), paste("Losses", events_df$Losses[events_df$Node==x], sep = ": "), paste("Copies", events_df$Copies[events_df$Node==x], sep = ": "), sep = "; "), x, cex = 0.8)
}


#Part 4: Write out results to file
dev.off()

```

Using the above script, the trees below were created, yielding the summary statistics found below: 

### Mitochondria 3 OF

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Losses: 0
   - Originations: 294
   - Copies: 313
 - Parabasalid/Anaeramoebid common ancestor: 
   - Losses: 0
   - Originations: 16
   - Copies: 376
 - Parabasalia: 
   - Losses: 13
   - Originations: 168
   - Copies: 550
 - Anaeramoebidae: 
   - Losses: 30
   - Originations: 15
   - Copies: 383
 - Preaxostyla: 
   - Losses: 20
   - Originations: 4
   - Copies: 353
 - Fornicata: 
   - Losses: 9
   - Originations: 23
   - Copies: 398
 - _Trichomonas vaginalis_: 
   - Losses: 25
   - Originations: 23
   - Copies: 613
 - _Giardia_ spp.: 
   - Losses: 34
   - Originations: 35
   - Copies: 333

![ALE visualized results Mito OF](ALE/OF_Mito3_Species_Tree_ALL_GFam.svg)


### Mitochondria 3 SP

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Losses: 0
   - Originations: 301
   - Copies: 311
 - Parabasalid/Anaeramoebid common ancestor: 
   - Losses: 0
   - Originations: 19
   - Copies: 403
 - Parabasalia: 
   - Losses: 8
   - Originations: 162
   - Copies: 584
 - Anaeramoebidae: 
   - Losses: 18
   - Originations: 17
   - Copies: 417
 - Preaxostyla: 
   - Losses: 24
   - Originations: 1
   - Copies: 355
 - Fornicata: 
   - Losses: 7
   - Originations: 24
   - Copies: 409
 - _Trichomonas vaginalis_: 
   - Losses: 21
   - Originations: 16
   - Copies: 631
 - _Giardia_ spp.: 
   - Losses: 40
   - Originations: 33
   - Copies: 335

![ALE visualized results Mito SP](ALE/SP_Mito3_Species_Tree_ALL_GFam.svg)


### Secretome 3 OF

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Losses: 0
   - Originations: 669
   - Copies: 698
 - Parabasalid/Anaeramoebid common ancestor: 
   - Losses: 0
   - Originations: 53
   - Copies: 866
 - Parabasalia: 
   - Losses: 26
   - Originations: 607
   - Copies: 1518
 - Anaeramoebidae: 
   - Losses: 79
   - Originations: 63
   - Copies: 886
 - Preaxostyla: 
   - Losses: 27
   - Originations: 17
   - Copies: 853
 - Fornicata: 
   - Losses: 33
   - Originations: 47
   - Copies: 891
 - _Trichomonas vaginalis_: 
   - Losses: 48
   - Originations: 53
   - Copies: 1777
 - _Giardia_ spp.: 
   - Losses: 66
   - Originations: 44
   - Copies: 682

![ALE visualized results Sec OF](ALE/OF_Sec3_Species_Tree_ALL_GFam.svg)


### Secretome 3 SP

Summary statistics (ALE): 
 - Metamonad common ancestor: 
   - Losses: 0
   - Originations: 824
   - Copies: 847
 - Parabasalid/Anaeramoebid common ancestor: 
   - Losses: 0
   - Originations: 51
   - Copies: 1036
 - Parabasalia: 
   - Losses: 29
   - Originations: 517
   - Copies: 1582
 - Anaeramoebidae: 
   - Losses: 89
   - Originations: 57
   - Copies: 1028
 - Preaxostyla: 
   - Losses: 50
   - Originations: 11
   - Copies: 957
 - Fornicata: 
   - Losses: 59
   - Originations: 60
   - Copies: 1010
 - _Trichomonas vaginalis_: 
   - Losses: 50
   - Originations: 19
   - Copies: 1768
 - _Giardia_ spp.: 
   - Losses: 61
   - Originations: 38
   - Copies: 755

![ALE visualized results Sec SP](ALE/SP_Sec3_Species_Tree_ALL_GFam.svg)


### Collecting statistics

For the purposes of comparison, needed to collect some basic statistics on the numbers of proteins and OGs in the dataset: 

```bash
#finding the number of proteins in the overall dataset
wc -l Metamonada_Alanta_pred_OG_DB.txt
# 556048 Metamonada_Alanta_pred_OG_DB.txt
#meaning 556047 proteins in the entire dataset
#finding the number of proteins in the filtered datasets
wc -l Metamonada_PFamAignava__*
#    3547 Metamonada_PFamAignava__Mito3Filt.txt
#   13816 Metamonada_PFamAignava__Sec3Filt.txt
#   17363 total
#factoring in the header line: 
#3546 proteins in the mito dataset
#13815 proteins in the secretome dataset
#finding the number of proteins in OGs
#ref: https://stackoverflow.com/questions/60378679/print-the-value-of-last-not-null-column-in-awk
awk -F '|' '{sub(/[|]+$/, ""); print $NF}' file
#didn't end up using the above, because that overcomplicates matters
wc -l *_OGs_parsed_Alanta.txt
#   414192 Broccoli_OGs_parsed_Alanta.txt
#   481337 OF_OGs_parsed_Alanta.txt
#   342462 PO_OGs_parsed_Alanta.txt
#   380749 SP_OGs_parsed_Alanta.txt
#  1618740 total
#meaning: 
#Broccoli 414191
#OrthoFinder 481336
#ProteinOrtho 342461
#SonicParanoid 380748
###
#finding the numbers of OGs
awk -F '\t' '(NR>1) {print $3}' OF_OGs_parsed_Alanta.txt | sort | uniq | wc -l
# 78982
awk -F '\t' '(NR>1) {print $3}' PO_OGs_parsed_Alanta.txt | sort | uniq | wc -l
# 95438
awk -F '\t' '(NR>1) {print $3}' SP_OGs_parsed_Alanta.txt | sort | uniq | wc -l
# 66802
awk -F '\t' '(NR>1) {print $2}' Broccoli_OGs_parsed_Alanta.txt | sort | uniq | wc -l
# 39715
#finding the numbers of OGs in the filtrations
wc -l *_OGs.txt
#   1665 Alanta_mito_3_OF_OGs.txt
#   1431 Alanta_mito_3_SP_OGs.txt
#   5291 Alanta_sec_3_OF_OGs.txt
#   3878 Alanta_sec_3_SP_OGs.txt
#  12265 total
#all of these have a "-" at the top, so subract 1
#Mito OF 1664
#Mito SP 1430
#Sec OF 5290
#Sec SP 3877
#finally, finding the numbers of proteins & OGs that came from the control dataset
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/FinalFilt_S3M3/ directory
#Mito OF
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq | wc -l
# 442
#Sec OF
awk -F '\t' '(NR>1) {print $5}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort | uniq | wc -l
# 1600
#Mito SP
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_mito_ctrl.txt | sort | uniq | wc -l
# 470
#Sec SP
awk -F '\t' '(NR>1) {print $6}' Metamonada_pred_OG_DB__filt_scores-startAA-pfam__Tvag_sec_ctrl.txt | sort | uniq | wc -l
# 1634
#and the control protein numbers: 
wc -l encodingSummary_Ctrl_data_*
#   683 encodingSummary_Ctrl_data_mito_encoded.txt
#  2343 encodingSummary_Ctrl_data_sec_encoded.txt
#  3026 total

```


## Conclusions


Selected, at random, OGs from all 4 filtration categories (SonicParanoid & OrthoFinder; Mitochondria/Hydrogenosome & Secretome) for further analysis. 

The first batch of OGs was based on key PFams highlighted during the course of the preliminary research project, as well as the OGs of proteins identified from the control datasets. 

In order to better understand the evolutionary patterns of these genes, I constructed phylogenetic trees showing the data from ALE and Count for each individual analyzed OG. 

For Count, this process was quite simple: I reopened the saved .XML session files for the 4 filtrations, and checked the evolutionary processes of each of the key OGs. 

For ALE, I had to take a more roundabout route: I filtered the OG data in the R-formatted node versions into individual files, which I visualized in RStudio (script modifications for this are present above).

To help myself somewhat with data collection, I pulled the PFam information I had already pulled during the preliminary project. 

```bash
#ref: https://stackoverflow.com/questions/2696055/intersection-of-two-lists-in-bash
comm -12 <(sort PFam_List_ALL.txt) <(sort Prelim_PFam_List.txt)
# Ank_2
# Ank_3
# CitMHS
# DUF261
# HSP70
# Inhibitor_I29
# LRR_5
# LysM
# Mac-1
# NLPC_P60
# Na_sulph_symp
# Peptidase_C1
# Peptidase_M8
# Peptidase_S28
# Pkinase
# RCC1
# Ras
# SH3_3
# SPX
# TIG
# TPR_1
# TPR_11
# TPR_16
# TPR_2
# TPR_8
# WD40
comm -12 <(sort PFam_List_ALL.txt) <(sort Prelim_PFam_List.txt) | wc -l
# 26
comm -12 <(sort PFam_List_ALL.txt) <(sort Prelim_PFam_List.txt) > Overlap_PFams.txt

```

### Conserved Domain Searches

To better evaluate the key OGs I am examining, I performed conserved domain searches on the NCBI in batch form (https://www.ncbi.nlm.nih.gov/Structure/bwrpsb/bwrpsb.cgi), using the input files for the MAFFT aligments. An example of the options used can be seen below: 

![Batch Conserved Domain Search Example](Biol_Interpret/Key_OGs/Batch_CDD_Example.png)


For OGs examined in such detail, I decided to check how many of the sequences overlapped: 

```bash
#the SNARE-associate secretory OGs
#ie. OGs associated with S60331I7mDTbrdSs protein
comm -12 <(grep ">" OF_Sec3__OG0002700_MSAprep.fasta | sort) <(grep ">" SP_Sec3__OG_1245_MSAprep.fasta | sort)
# >A_flamelloides_BS__GAUxTMQKV3y9XUv8
# >A_flamelloides_BS__Kt3ETMrmS13pkcc9
# >A_flamelloides_BS__fNcJ1XoRGedqJ2jF
# >A_flamelloides_BS__sGN3pXxYwEkvrJNJ
# >A_flamelloides_SC__TbeNF5AYDNaOuYlB
# >A_flamelloides_SC__cwwupOtWsKN6SNuC
# >A_flamelloides_SC__pnEfcM5gshtjGZVs
# >A_ignava_BM__QhfKdizShhAcN2fC
# >A_lanta__W6o2NVbKOY1x9hah
# >A_lanta__hzeY7aHfcJQlZXkx
# >D_fragilis__3pi57PO1Y5YAU2H4
# >H_meleagridis_NEW__QUDyvb66CW72etou
# >H_meleagridis_NEW__RGexN28Wansp0Ru3
# >H_meleagridis_OLD__Xjyt8x6KBx4H4Rnx
# >M_exilis__XtCAeDllgUL301io
# >P_hominis__7zcaHDJmoCSs1u9z
# >P_pyriformis__Y113RECYwC761nXJ
# >T_foetus__lKgtuacsktEdHkC0
# >T_gallinarum__Yo22KTkr4qDcXmPg
# >T_gallinarum__lH8eR8NlLVRQmCNq
# >T_marina__lemstiJPTZx4mWfK
# >T_vaginalis_GenBank__S60331I7mDTbrdSs
# >T_vaginalis_RefSeq__AzuSdUtCN5qRXdOp
comm -12 <(grep ">" OF_Sec3__OG0002700_MSAprep.fasta | sort) <(grep ">" SP_Sec3__OG_1245_MSAprep.fasta | sort) | wc -l
# 23
comm -12 <(grep ">" OF_Sec3__OG0002700_MSAprep.fasta | sort) <(grep ">" SP_Sec3__OG_1245_MSAprep.fasta | sort) > Overlap_OF_Sec3_OG0002700__SP_Sec3_OG_1245.txt
#so nearly all of the OF OG is present in the SP OG
#real quick, let's find the 1 protein that isn't
#ref: https://stackoverflow.com/questions/18204904/fast-way-of-finding-lines-in-one-file-that-are-not-in-another
comm -23 <(grep ">" OF_Sec3__OG0002700_MSAprep.fasta | sort) <(grep ">" SP_Sec3__OG_1245_MSAprep.fasta | sort)
# >A_flamelloides_SC__sA2UI8haFiEqwsFc
#ref: https://askubuntu.com/questions/699216/how-to-find-the-shortest-line-in-a-script
#ref: https://unix.stackexchange.com/questions/174715/how-to-ignore-the-lines-starts-with-using-grep-awk
#ref: https://stackoverflow.com/questions/54266956/linux-command-to-find-lowest-value-in-a-particular-column-of-a-data-file
#OF_Sec3__OG0002700
#min
awk '/^[^>]/ {print length}' OF_Sec3__OG0002700_MSAprep.fasta | sort -n | head -1
# 72
#max
awk '/^[^>]/ {print length}' OF_Sec3__OG0002700_MSAprep.fasta | sort -n | tail -1
# 279
#average
#ref: https://coding-school.com/awk-line-length-and-average/
awk ' /^[^>]/ { thislen=length($0); printf("%-5s %d\n", NR, thislen); totlen+=thislen} END { printf("average: %d\n", totlen/NR); } ' OF_Sec3__OG0002700_MSAprep.fasta
# 114
#getting the actual list of assigned domains
#skip the frist 8 rows (header and column headers)
awk -F '\t' '(NR>8) {print $9}' OF_Sec3__OG0002700_hitdata.txt | sort |uniq
# COG5283
# COG5283 superfamily
# Ribosomal_L28e
# Ribosomal_L28e superfamily
# SNARE superfamily
# SNARE_Qb
# SNARE_SEC9N
# SNARE_SNAP23N
# SNARE_SNAP25N
# SNARE_SNAP25N_23N
# SNARE_SNAP25N_23N_29N_SEC9N
# SNARE_SNAP29N
# SNARE_SNAP47N
# SNARE_Vti1
# SNARE_Vti1b
# Sec20
# Sec20 superfamily
# Smc
# Smc superfamily
# V-SNARE
# V-SNARE superfamily
# V-SNARE_C
# V-SNARE_C superfamily
# t_SNARE
#SP_Sec3__OG_1245
#min
awk '/^[^>]/ {print length}' SP_Sec3__OG_1245_MSAprep.fasta | sort -n | head -1
# 99
#max
awk '/^[^>]/ {print length}' SP_Sec3__OG_1245_MSAprep.fasta | sort -n | tail -1
# 279
#average
awk ' /^[^>]/ { thislen=length($0); printf("%-5s %d\n", NR, thislen); totlen+=thislen} END { printf("average: %d\n", totlen/NR); } ' SP_Sec3__OG_1245_MSAprep.fasta
# 110
#getting the actual list of assigned domains
awk -F '\t' '(NR>8) {print $9}' SP_Sec3__OG_1245_hitdata.txt | sort |uniq
# COG5283
# COG5283 superfamily
# Ribosomal_L28e
# Ribosomal_L28e superfamily
# SMC_prok_B
# SMC_prok_B superfamily
# SNARE superfamily
# SNARE_Qb
# SNARE_SEC9N
# SNARE_SNAP23N
# SNARE_SNAP25N
# SNARE_SNAP25N_23N
# SNARE_SNAP25N_23N_29N_SEC9N
# SNARE_SNAP29N
# SNARE_SNAP47N
# SNARE_Vti1
# SNARE_Vti1b
# Sec20
# Sec20 superfamily
# Smc
# Smc superfamily
# Syntaphilin
# Syntaphilin superfamily
# V-SNARE
# V-SNARE superfamily
# V-SNARE_C
# V-SNARE_C superfamily
# t_SNARE
#gathering additional data on these OGs using the assessment script: 
python assess_OG_startAA_scores.py -h
# usage: assess_OG_startAA_scores.py [-h] -cat FILTRATION_CATEGORY -query QUERY_IDS -prog OG_PROGRAM [-val THRESHOLD]
#                                    [-score SCORE] [-incl INCLUDED_AA] [-out OUT_NAME] [-v] -i INPUT_FILE
# This program asseses the quality of OGs in the Metamonad database (or filtered version of the same, including at
# minimum the columns "Query", "Secretion_Score", "Mitochondria_Score", "StartAA" and the OG information column of the
# OG program desired to be used as input for the search), based on the desired representation criteria of the user. The
# following can be used to filter for representation: - Percent of proteins in a given OG which meet a given minimum
# prediction score for either the mitochondrial or secretory pathway - Percent of proteins in a given OG which start
# with Methionine, or with either Methionine or Leucine
# optional arguments:
#   -h, --help            show this help message and exit
#   -val THRESHOLD, --threshold_value THRESHOLD
#                         Integer value of minimum percent of proteins to meet the given test criteria in OGs. Test
#                         criteria are: - Percent proteins in OG predicted to score at or above given score threshold
#                         for given pathway - Percent proteins in OG achieving given completion level based on first
#                         amino acid in sequence (Default = 80)
#   -score SCORE, --filt_score SCORE
#                         This argument allows the user to define he scoring threshold that should be used for the given
#                         pathway scoring filtration. (Default = 4.0) This argument is automatically called for
#                         mitochondrial or sectretory pathway testing.
#   -incl INCLUDED_AA, --includedAA INCLUDED_AA
#                         This argument allows the user to define the sequence quality checking to be used. The options
#                         are: "Met" OR "Leu" Where "Met" will filter for percent of proteins that start with
#                         Methionine; while "Leu" will filter for percent of proteins that start with Methionine or
#                         Leucine. This argument is automatically called for completion quality testing. The default is
#                         "Met".
#   -out OUT_NAME, --outname OUT_NAME
#                         This argument allows the user to define an output file basename. The default basename is
#                         "Assessed_[FILTRATION_CATEGORY]_[SCORE/INCLUDED_AA]_[datetime]", and the output files are: -
#                         [BASENAME]_filt[THRESHOLD]_stats.txt: Summary statistics file -
#                         [BASENAME]_filt[THRESHOLD]_OGs.txt: File containing OGs meeting threshold in format OG1 OG2
#                         etc.
#   -v, --version         show program's version number and exit
# required arguments:
#   -cat FILTRATION_CATEGORY, --filtration_category FILTRATION_CATEGORY
#                         This argument requires the user to specify the type of data the filtration should be performed
#                         on. The options are: "Secretion_Score" OR "Mitochondria_Score" OR "StartAA".
#   -query QUERY_IDS, --query_ids QUERY_IDS
#                         Here specify the query protein IDs in one of the following formats: single protein ID OR
#                         comma-separated list of protein IDs (ex. `ID_1,ID_2,ID_3`) OR file with protein IDs separated
#                         by newlines When using unencoded query IDs, a key portion of the protein name is sufficient -
#                         The entirety of the protein header does not need to be used.
#   -prog OG_PROGRAM, --og_program OG_PROGRAM
#                         This argument requires the user to specify the OG program whose data the filtration should be
#                         performed on. The options are: "Br_Grouped_OGs" OR "Br_Single_OGs" OR "ProteinOrtho_OG" OR
#                         "OrthoFinder_OG" OR "SonicParanoid_OG".
#   -i INPUT_FILE, --input INPUT_FILE
#                         The input file should be the Metamonad database (or filtered version of the same, including at
#                         minimum the columns 'Query', 'Secretion_Score', 'Mitochondria_Score', 'StartAA' and the OG
#                         information column of the OG program desired to be used as input for the search).
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Key_OGs/ directory
#assessing the prediction scores of the OGs
python ../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score -query OG_1245 -prog SonicParanoid_OG -score 3 -out SP_Sec3__OG_1245_filtSec3-50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
python ../../Scripts/assess_OG_startAA_scores.py -cat Secretion_Score -query OG0002700 -prog OrthoFinder_OG -score 3 -out OF_Sec3__OG0002700_filtSec3-50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
#gathering data on the completion scores of the proteins
python ../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OG_1245 -prog SonicParanoid_OG -val 50 -incl Leu -out SP_Sec3__OG_1245_filtLeu50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
python ../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OG0002700 -prog OrthoFinder_OG -val 50 -incl Leu -out OF_Sec3__OG0002700_filtLeu50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
###
#OG assignments of iNYAPmAoVejoEKlk protein
comm -12 <(grep ">" OF_Mito3__OG0000060_MSAprep.fasta | sort) <(grep ">" SP_Mito3__OG_55_MSAprep.fasta | sort)
#ope, that's quite a lot
comm -12 <(grep ">" OF_Mito3__OG0000060_MSAprep.fasta | sort) <(grep ">" SP_Mito3__OG_55_MSAprep.fasta | sort) | wc -l
# 247
comm -12 <(grep ">" OF_Mito3__OG0000060_MSAprep.fasta | sort) <(grep ">" SP_Mito3__OG_55_MSAprep.fasta | sort) > Overlap_OF_Mito3_OG0000060__SP_Mito3_OG_55.txt
#so nearly all of the SP OG is present in the OF OG
#real quick, let's find the proteins that aren't
#ref: https://stackoverflow.com/questions/18204904/fast-way-of-finding-lines-in-one-file-that-are-not-in-another
comm -23 <(grep ">" SP_Mito3__OG_55_MSAprep.fasta | sort) <(grep ">" OF_Mito3__OG0000060_MSAprep.fasta | sort)
# >A_flamelloides_BS__0iUCTmjvJlLFuhFB
# >A_flamelloides_BS__F03T1Bzyssg5ri3g
# >A_flamelloides_BS__G9OTp4i74B4rbiGD
# >A_flamelloides_BS__PI9XK33zIIUJ0mgF
# >A_flamelloides_BS__cGxJWtt0SmPtZNsA
# >A_flamelloides_SC__7AO5sWliY5viQth9
# >A_flamelloides_SC__KBu9WN1o0OliuYlb
# >A_flamelloides_SC__Z7530xQIFd7YrRim
# >A_ignava_BM__zZjvVJHm9LTwFIcG
# >A_lanta__9fAce6w5eDOcYzFF
# >A_lanta__KOUPAvcKWavj7jTW
# >Barthelona_PAP020__jdEUston2esWaKxR
# >C_caulleryi__55QFuEgFMUupvCXL
# >C_cuspidata__cIgkyC3Sr3WZHHNr
# >C_membranifera__1v7flxljps2EuiDu
# >C_membranifera__Vhk5scFYbKrHSqX5
# >E_cyprinoides__79wrZusbhvzDwrTz
# >K_bialata__Md6JQMW0crSzZQfG
# >M_exilis__GcMxNuxDeCotyaA5
# >M_exilis__SW2eJtwyLLGFfqTM
# >M_exilis__gHzicsdWP0XhLRnh
# >P_pyriformis__0Ilu8z19RROyAIRN
# >P_pyriformis__GmlIvX5zkqQ62HoS
# >P_pyriformis__J4XkIjea42vxlFT6
# >S_salmonicida__XvtwVNHGCN9slnpk
# >T_foetus__EcGswAZHENacgfIM
# >T_marina__slkKfcmxCR68hZTR
# >Trepomonas_PC1__Jbf9li6fy2PY307L
comm -23 <(grep ">" SP_Mito3__OG_55_MSAprep.fasta | sort) <(grep ">" OF_Mito3__OG0000060_MSAprep.fasta | sort) | wc -l
# 28
#data on sequence lengths
#OF_Mito3__OG0000060
#min
awk '/^[^>]/ {print length}' OF_Mito3__OG0000060_MSAprep.fasta | sort -n head -1
# 69
#max
awk '/^[^>]/ {print length}' OF_Mito3__OG0000060_MSAprep.fasta | sort -n | tail -1
# 3666
#average
awk ' /^[^>]/ { thislen=length($0); printf("%-5s %d\n", NR, thislen); totlen+=thislen} END { printf("average: %d\n", totlen/NR); } ' OF_Mito3__OG0000060_MSAprep.fasta
# 285
#getting the actual list of assigned domains
awk -F '\t' '(NR>8) {print $9}' OF_Mito3__OG0000060_hitdata.txt | sort | uniq
#there's... quite a lot!
awk -F '\t' '(NR>8) {print $9}' OF_Mito3__OG0000060_hitdata.txt | sort | uniq | wc -l
# 354
#SP_Mito3__OG_55
#min
awk '/^[^>]/ {print length}' SP_Mito3__OG_55_MSAprep.fasta | sort -n | head -1
# 59
#max
awk '/^[^>]/ {print length}' SP_Mito3__OG_55_MSAprep.fasta | sort -n | tail -1
# 1717
#average
awk ' /^[^>]/ { thislen=length($0); printf("%-5s %d\n", NR, thislen); totlen+=thislen} END { printf("average: %d\n", totlen/NR); } ' SP_Mito3__OG_55_MSAprep.fasta
# 316
#getting the actual list of assigned domains
awk -F '\t' '(NR>8) {print $9}' SP_Mito3__OG_55_hitdata.txt | sort |uniq
#there's quite a lot
awk -F '\t' '(NR>8) {print $9}' SP_Mito3__OG_55_hitdata.txt | sort | uniq | wc -l
# 412
#gathering additional data on these OGs using the OG assessment script
#working in the /home/inf-47-2020/ThesisTrich/DB2_A_lanta/Key_OGs/ directory
#assessing the prediction scores of the OGs
python ../../Scripts/assess_OG_startAA_scores.py -cat Mitochondria_Score -query OG_55 -prog SonicParanoid_OG -score 3 -out SP_Mito3__OG_55_filtMito3-50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
python ../../Scripts/assess_OG_startAA_scores.py -cat Mitochondria_Score -query OG0000060 -prog OrthoFinder_OG -score 3 -out OF_Mito3__OG0000060_filtMito3-50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
#gathering data on the completion scores of the proteins
python ../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OG_55 -prog SonicParanoid_OG -val 50 -incl Leu -out SP_Mito3__OG_55_filtLeu50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
python ../../Scripts/assess_OG_startAA_scores.py -cat StartAA -query OG0000060 -prog OrthoFinder_OG -val 50 -incl Leu -out OF_Mito3__OG0000060_filtLeu50p -i ../Metamonada_Alanta_pred_OG_DB__filt_scores-startAA-Annot.txt
###
#definitve count of OGs
#Count
#working in the /mnt/c/Users/V/Documents/LundUni/Trich_Parab/Thesis_Work/Count/Alanta_ALL/ directory
wc -l *_ALL__CountPivot.txt
#    1665 Alanta_mito_3_OF_ALL__CountPivot.txt
#    1431 Alanta_mito_3_SP_ALL__CountPivot.txt
#    5291 Alanta_sec_3_OF_ALL__CountPivot.txt
#    3878 Alanta_sec_3_SP_ALL__CountPivot.txt
#   12265 total
#subtract 1 from all of these values 
#b/c `w -l` includes the header line in the line count
#ALE
#working in the /home/inf-47-2020/ThesisTrich/IQ-TREE_Results/ directory
#Mito_OF/
ls *_MSA_trim.fasta | wc -l
# 998
#subtract 1 from this number
#since OG0000000 was too large for ALE to analyze
#Mito_SP/
ls *_MSA_trim.fasta | wc -l
# 830
#Sec_OF/
ls *_MSA_trim.fasta | wc -l
# 3219
#subtract 1 from this number
#since OG0000000 was too large for ALE to analyze
#Sec_SP/
ls *_MSA_trim.fasta | wc -l
# 2386
#now to grab the protein numbers for the Count & ALE filtrations
#Count
#working in the /home/inf-47-2020/ThesisTrich/MAFFT_MSA/Input_FASTA/ directory
#Mito_OF/
cat *_MSAprep.fasta | grep -c ">"
# 67438
#Mito_SP/
cat *_MSAprep.fasta | grep -c ">"
# 49850
#Sec_OF/
cat *_MSAprep.fasta | grep -c ">"
# 134648
#Sec_SP/
cat *_MSAprep.fasta | grep -c ">"
# 109794
#ALE
#working in the /home/inf-47-2020/ThesisTrich/IQ-TREE_Results/ directory
#Mito_OF/
cat *_MSA_trim.fasta | grep -c ">"
# 65748
grep -c ">" OF_Mito3__OG0000000_MSA_trim.fasta
# 2842
#this needs to be subtracted from the larger number, 
#because this OG was too large for ALE to analyze
#total = 65748-2842 = 62906
#Mito_SP/
cat *_MSA_trim.fasta | grep -c ">"
# 48350
#Sec_OF/
cat *_MSA_trim.fasta | grep -c ">"
# 129529
grep -c ">" OF_Sec3__OG0000000_MSA_trim.fasta
# 2842
#this needs to be subtracted from the larger number, 
#because this OG was too large for ALE to analyze
#total = 129529-2842 = 126687
#Sec_SP/
cat *_MSA_trim.fasta | grep -c ">"
# 106401

```

For larger OGs, manually checking for PFams isn't entirely viable. For those, I checked for function using the KEGG database using the sequence below: 
 - Grabbed accession number of top hit in NCBI BLASTp: https://www.ncbi.nlm.nih.gov/protein/XP_001305709.1?report=genbank&log$=prottop&blast_rank=1&RID=EUE6N0U801R
   - 64kDa iron hydrogenase [Trichomonas vaginalis G3]
 - KEGG Database homepage: https://www.genome.jp/kegg/genes.html
 - Ref: KEGG organisms in the NCBI Taxonomy: 
   - https://www.genome.jp/brite/br08610
   - Trichomonas vaginalis [TAX:5722]
   - tva Trichomonas vaginalis
 - BlastKOALA (https://www.kegg.jp/blastkoala/)
   - Taxonomy group: Eukaryotes
   - KEGG GENES database file to search: family_eukaryotes
   - If there are hits, can use result_ko file as input to KEGG Mapper to reconstruct the results
 - Well, that yielded nothing. So let's try something else: 
 - KofamKOALA (https://www.genome.jp/tools/kofamkoala/)
   - Using the same aequence (below)
   - E-value = 0.01
 - Once again, no results. So...
 - Rerunning BlastKOALA & KofamKOALA with the OG FASTA files, to see if *any* of the sequences manage to get *any* hits
   - OF OG0000060: 
     - KofamKOALA: Found hits! Fascinatingly, the majority of the hits are in anaeramoebids (plus 1 _T. foetus_ hit)! 
       - KEGG Mapper reconstruction results: 
       - Pathway: Metabolism 
         - Global & overview maps: 01100 Metabolic pathways (1); 01110 Biosynthesis of secondary metabolites (1), 01240 Biosynthesis of cofactors (1)
         - Metabolism of cofactors & vitamins: 00860 Porphyrin metabolism (1)
       - Brite: Genes & Proteins
         - Orthologs, modules & networks: ko00001 KEGG Orthology (KO) (4)
         - Protein families: metabolism: ko01000 Enzymes (4)
     - BlastKOALA: 198/313 entries had hits (63.3%): 7 in "Energy metabolism" and 191 in "Unclassified" metabolism"
       - KEGG Mapper reconstruction results: 
       - Pathway: Metabolism
         - Global & overview maps: 01100 Metabolic pathways (2), 01120 Micrbial metabolism in diverse environments (1)
         - Energy Metabolism: 00190 Oxidative phosphorylation (1), 00920 Sulfur metabolism (1)
       - Brite: Genes & Proteins
         - Orthology, modules & networks: ko00001 KEGG Orthology (KO) (5)
         - Protein families: metabolism: ko01000 Enzymes (4)
   - SP OG_55: 
     - KofamKOALA: Primarily Anaeramoebid hits, except for 1 protein each for T. foetus, K. bialata & Barthelona
       - KEGG Mapper reconstruction results: 
       - Pathway: Metabolism 
         - Global & overview maps: 01100 Metabolic pathways (3); 01110 Biosynthesis of secondary metabolites (2), 01120 Microbial metabolism in diverse environments (2), 01200 Carbon metabolism (1), 01230 Biosynthesis of amino acids (1), 01240 Biosynthesis of cofactors (1)
         - Carbohydrate metabolism: 00630 Glyoxylate and dicarboxylate metabolism (1)
         - Energy metabolism: 00680 Methane metabolism (1), 00910 Nitrogen metabolism (1)
         - Amino acid metabolism: 00250 Alanine, aspartate & glutamate metabolism (1)
         - Metabolism of cofactors & vitamins: 00860 Porphyrin metabolism (1)
       - Brite: Genes & Proteins
         - Orthologs, modules & networks: ko00001 KEGG Orthology (KO) (5)
         - Protein families: metabolism: ko01000 Enzymes (4)
     - BlastKOALA: 180/275 entries had hits (65.5%): 4 in "Energy metabolism," 1 in "Carbohydrate metabolism" and 175 in "Unclassified: metabolism." 
       - KEGG Mapper reconstruction results: 
       - Pathway: Metabolism 
         - Global & overview maps: 01100 Metabolic pathways (4); 01110 Biosynthesis of secondary metabolites (1), 01120 Microbial metabolism in diverse environments (3), 01200 Carbon metabolism (1), 01230 Biosynthesis of amino acids (1)
         - Carbohydrate metabolism: 00630 Glyoxylate and dicarboxylate metabolism (1)
         - Energy metabolism: 00190 Oxidative phosphorylation (1), 00680 Methane metabolism (1), 00910 Nitrogen metabolism (1), Sulfur metabolism
         - Amino acid metabolism: 00250 Alanine, aspartate & glutamate metabolism (1)
       - Brite: Genes & Proteins
         - Orthologs, modules & networks: ko00001 KEGG Orthology (KO) (7)
         - Protein families: metabolism: ko01000 Enzymes (5)
 - KEGG Mapper (https://www.genome.jp/kegg/mapper/) can reconstruct the pathways found in the *_ko.txt results files output by KofamKOLA. Simply select the "Reconstruct" option on the page (takes you to: https://www.genome.jp/kegg/mapper/reconstruct.html), upload the *_ko.txt file, and click the button labeled "Exec".


OF_Mito3__OG0000060_MSAprep.fasta
>T_vaginalis_RefSeq__hPCT0aFWNQd4tcti
MLATASASTSNILRNITVTVNGRKLEAKKGETILELCDRNNIRIPRLCFHPNLPPKASCRVCLVECDGKWLAPACVTTVWDGLKIDTKSKMVKESVENNLKELLDCHDETCSSCVANHRCQFRDMNVAYSIKAETKEECSEEGIDESTNSIRLDTSKCVLCGRCIRACEEVAGQSAIIFGNRAKHMRIQPTFGQTLQDTSCIKCGQCTLYCPVGAITEKSQVKQALDILSNKGKKISVIQVAPAVRVALSEAFGYKEGSVTTGKMVSALKALGFDYVYDTNYSADLTIVEEAGELVQRLKNPNAVFPMFTSCCPAWVNYVEQSAPDFIPNLSSCRSPQGMLSSLVKNYLPKVLNIPVEDVLNFSIMPCTAKKDEIERPELRTKDGHKETDMVLTVRELVEMIKLSGIDFNNLPDTPFDSIFGFGSGAGQIFAATGGVMEAASRTAFEAVTGKKLTNVNIYPVRGMDGTRIAELDLDGTKLKIAVCHGIANTAKFLDRLRAKDPQLADLKFVEVMACPGGCVCGGGTPQPKNMMSLDNRLAAIYNIDAKMECRKSHENPLIKGIYKEFLGEPNGHLAHELLHTHYKHHPKF


Script to parse the *_hitdata.txt results files produced by the CDD batch searches (script saved to parse_CDDhitdata.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: parse_CDDhitdata.py
Date: 2022-08-06
Authors: Virág Varga

Description:
	This program parses the full version of the *_hitdata.txt results file that 
		can be downloaded after a search of the NCBI Conserved Domain Database. 

List of functions:
	No functions are defined in this script.

List of standard and non-standard modules used:
	sys
	os
	pandas

Procedure:
	1. Loading modules & determining input & output files.
	2. Importing the dataframe into Pandas & reformatting contents.
	3. Filtering the database to include only desired columns.
	4. Writing out the results to a tab-separated text file.

Known bugs and limitations:
	- This CDD results file parset is written specifically to suit the 
		full version of the hit data results file. 
	- The output file name is not user-defined; it is based on the input 
		file name. 

Usage
	./parse_CDDhitdata.py input_file
	OR
	python parse_CDDhitdata.py input_file

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Loading modules & determining input & output files

#import necessary modules
import sys #allow assignment of files from the command line
import os #allow access to computer files
import pandas as pd #facilitates manipulation of dataframes in Python


#assign command line argument
input_file = sys.argv[1]
#input_file = "OF_Mito3__OG0000060_hitdata.txt"
base = os.path.basename(input_file)
out_full = os.path.splitext(base)[0]
output_file = out_full + "_CDDparsed.txt"


#Part 2: Importing the dataframe into Pandas & reformatting contents

#read in the input text file, assigning the first row as a header row
cdd_df = pd.read_csv(input_file, sep='\t', header=6, index_col=(False))
#the header is in row 8 (pythonic index 7)
#however, the file skips the blank line, so need to go one lower for the header line argument

#edit the contents of the first column to only include the species ID & protein name
#first, remove the first prefix
#ref: https://stackoverflow.com/questions/64463816/pandas-dataframe-split-and-get-last-element-of-list
cdd_df['Query'] = cdd_df['Query'].str.split(">").str[-1]
#then, remove the suffix
#ref: https://stackoverflow.com/questions/42349572/remove-first-x-number-of-characters-from-each-row-in-a-column-of-a-python-datafr
cdd_df['Query'] = cdd_df['Query'].str[:-2]
#for ease of use, rename column headers with spaces in them
cdd_df.columns = cdd_df.columns.str.replace(' ', '_')
#ref: https://www.geeksforgeeks.org/remove-spaces-from-column-names-in-pandas/


#Part 3: Filter the database to include only desired columns

#select the relevant columns and copy them to a new dataframe
filt_cdd_df = cdd_df[['Query', 'Short_name', 'Accession', 'Hit_type']].copy()

#group the results into 1 row per protein
filt_cdd_df = filt_cdd_df.groupby('Query')[['Short_name', 'Accession', 'Hit_type']].agg(list).reset_index()
#the above creates lists in the columns, so need to remove the brackets by converting to strings
filt_cdd_df['Short_name'] = filt_cdd_df['Short_name'].apply(lambda x: ';'.join(map(str, x)))
filt_cdd_df['Accession'] = filt_cdd_df['Accession'].apply(lambda x: ';'.join(map(str, x)))
filt_cdd_df['Hit_type'] = filt_cdd_df['Hit_type'].apply(lambda x: ';'.join(map(str, x)))


#Part 4: Write out the results to a file

#write out the results to a tab-separated text file
filt_cdd_df.to_csv(output_file, sep='\t', index=False)

```

Extracting IPRScan annotations from the large metamonad database, on the basis of the PFam annotation extractions. 

Begin by creating OG to IPRScan annotation databases containing IPR accession and GO data (script saved to og2IPRS_pivot.py): 

```python
# -*- coding: utf-8 -*-
#!/bin/python
"""

Title: og2IPRS_pivot.py
Date: 2022.08.14
Author: Virág Varga

Description:
	This program uses the Metamonad database, or a filtered copy of the same containing
		all IPRScan annotations and desired OG information to create tables linking all 
		OGs to their associated IPRScan accession numbers and GO annotations.
		The columns used are: iprS_InterPro_annotations-accession & iprS_GO_annotations

List of functions:
	concat (Source: IPRpivot.py, Courtney Stairs)

List of standard and non-standard modules used:
	sys
	pandas
	os
	collections.Counter

Procedure:
	1. Importing necessary modules & function; determining inputs & outputs.
	2. Importing database into Pandas and extracting relevant columns.
	3. Pivoting the dataframes and removing unnecessary data.
	4. Creating dataframes with count data on IPRS accession numbers and GO
		annotations per OG. 
	5. Creating a simple dataframes showing OGs and associated IPRS and GO 
		annotations.
	6. Writing out the results to tab-separated text files.

Known bugs and limitations:
	- There is no quality-checking integrated into the code.
	- This program requires the input of a version of the Metamonad database which
		includes both IPRS and OG assignments. The IPRS columns used by this program
		are iprS_InterPro_annotations-accession & iprS_GO_annotations.
	- The output file names are not user-defined.
	- IMPORTANT: The parsing of the GO terms is somewhat buggy! If only 1 GO term 
		is returned for a query, it will return as a "list" of characters instead
		of as a proper GO term. This is something that I hope to fix as I have time.
		Something similar to what was used to parse the IPRS data should hopefully work.

Version:
	This program is based off of the og2PFam_pivot__v2.py program. It has been modified 
		to extract IPRScan data from the Metamonad database, instead of the PFam 
		annotations. 

Usage
	./og2IPRS_pivot.py input_db og_col
	OR
	python og2IPRS_pivot.py input_db og_col

	Where og_col should be the name of the OG column for which the PFam domains should
		be aggregated.

This script was written for Python 3.8.12, in Spyder 5.1.5.

"""


#Part 1: Import necessary modules & function, determine inputs & outputs

#import necessary modules
import sys #allow execution of code from the command line
import pandas as pd #facilitates manipulation of dataframes in Python
import os #allow access to computer files
from collections import Counter #enables easy counting of elements


#make function for joining strings with ','
#Source: IPRpivot.py, Courtney Stairs
def concat(str):
	return ','.join(str)


#assign command line arguments; load input and output files
input_db = sys.argv[1]
#input_db = "Metamonada_pred_OG_DB__200prots.txt"

og_col = sys.argv[2]
#og_col = "SonicParanoid_OG"


#output versions will contain information from IPRS associated with each OG
base = os.path.basename(input_db)
out_full = os.path.splitext(base)[0]
#first extract base file name
#and then define the output file names
#first the IPRS accession databases
output_iprs = "OGs2IPRS_" + og_col + ".txt"
output_iprs_counts = "OGs2IPRS_" + og_col + "_Counts.txt"
#then the GO annotation databases
output_go = "OGs2GOs_" + og_col + ".txt"
output_go_counts = "OGs2GOs_" + og_col + "_Counts.txt"


#Part 2: Import database into Pandas, extract relevant columns

#read in the input OG database file, assigning the first row as a header row
input_df = pd.read_csv(input_db, sep = '\t', header=0, low_memory=False)
# sys:1: DtypeWarning: Columns (6,7,27,31) have mixed types.Specify dtype option on import or set low_memory=False.


#select the columns that will be used to create the pivot tables
iprs_og_RAW_df = input_df[['iprS_InterPro_annotations-accession', og_col]].copy()
#copy those columns only into a new dataframe
go_og_RAW_df = input_df[['iprS_GO_annotations', og_col]].copy()


#Part 3: Pivot the dataframes and remove unnecessary data

#pivot the tables
iprs_og_WIP_df = pd.pivot_table(iprs_og_RAW_df, index = og_col, aggfunc=concat).reset_index()
#use `.reset_index()` to pull the OG ID column out of the index
go_og_WIP_df = pd.pivot_table(go_og_RAW_df, index = og_col, aggfunc=concat).reset_index()

#the above method leaves extraneous whitespace, which needs to be removed
iprs_og_WIP_df['iprS_InterPro_annotations-accession'] = iprs_og_WIP_df['iprS_InterPro_annotations-accession'].str.replace(' ', '')
#then remove the row of the dataframe containing the "OG" of "-"
iprs_og_df = iprs_og_WIP_df[iprs_og_WIP_df[og_col] != "-"].copy()
#use `.copy()` to allow manipulation of the new dataframe
#doing it again for the GO data...
go_og_WIP_df['iprS_GO_annotations'] = go_og_WIP_df['iprS_GO_annotations'].str.replace(' ', '')
#then remove the row of the dataframe containing the "OG" of "-"
go_og_df = go_og_WIP_df[go_og_WIP_df[og_col] != "-"].copy()
#use `.copy()` to allow manipulation of the new dataframe


#Part 4: Create dataframes with count data on annotations

#create version of dataframe with count data (ie. non-set)
#starting with the IPR accession data
iprs_count_df = iprs_og_df.copy()
iprs_count_df['iprS_InterPro_annotations-accession'] = iprs_count_df['iprS_InterPro_annotations-accession'].apply(lambda x: ','.join(sorted(x.split(','))))
#now remove non-hits from the dataframe
iprs_count_df['iprS_InterPro_annotations-accession'] = iprs_count_df['iprS_InterPro_annotations-accession'].str.replace('-,', '')
#also remove OGs without PFam hits
iprs_count_prep_df = iprs_count_df[iprs_count_df['iprS_InterPro_annotations-accession'] != "-"].copy()
iprs_count_prep_df['iprS_InterPro_annotations-accession'] = iprs_count_prep_df['iprS_InterPro_annotations-accession'].apply(lambda x: x.split(','))
iprs_count_prep_df.set_index(og_col, inplace=True)
#and now do all the same for the GO annotations
go_count_df = go_og_df.copy()
go_count_df['iprS_GO_annotations'] = go_count_df['iprS_GO_annotations'].apply(lambda x: ','.join(sorted(x.split(','))))
#now remove non-hits from the dataframe
go_count_df['iprS_GO_annotations'] = go_count_df['iprS_GO_annotations'].str.replace('-,', '')
#also remove OGs without PFam hits
go_count_prep_df = go_count_df[go_count_df['iprS_GO_annotations'] != "-"].copy()
go_count_prep_df['iprS_GO_annotations'] = go_count_prep_df['iprS_GO_annotations'].apply(lambda x: x.split(','))
go_count_prep_df.set_index(og_col, inplace=True)


#create empty dictionaries to store the data
iprs_count_dict = {}
go_count_dict = {}

#first create the IPRS dictionary
for index, row in iprs_count_prep_df.iterrows(): 
	#iterate over the dataframe row by row
	og_iprs_list = row[0]
	#save the list of IPR hits to a variable
	og_iprs_counts = str(dict(Counter(og_iprs_list)))
	#save the results of the counts to a variable as a string
	og_iprs_counts = og_iprs_counts.replace('{', '')
	#remove the {braces} currently in the string
	og_iprs_counts = og_iprs_counts.replace('}', '')
	#remove spaces from the string
	og_iprs_counts = og_iprs_counts.replace(' ', '')
	#remove the single quotes from inside the strin
	#need to use "\" character to prevent EOF parsing error
	og_iprs_counts = og_iprs_counts.replace('\'', '')
	#now populate the dictionary
	iprs_count_dict[index] = og_iprs_counts

#then create the GO counts dictionary
for index, row in go_count_prep_df.iterrows(): 
	#iterate over the dataframe row by row
	og_go_list = row[0]
	#save the list of GO hits to a variable
	og_go_counts = str(dict(Counter(og_go_list)))
	#save the results of the counts to a variable as a string
	og_go_counts = og_go_counts.replace('{', '')
	#remove the {braces} currently in the string
	og_go_counts = og_go_counts.replace('}', '')
	#remove spaces from the string
	og_go_counts = og_go_counts.replace(' ', '')
	#remove the single quotes from inside the strin
	#need to use "\" character to prevent EOF parsing error
	og_go_counts = og_go_counts.replace('\'', '')
	#now populate the dictionary
	go_count_dict[index] = og_go_counts

#convert dictionaries back to dataframes
#first the IPR accession data
final_iprs_count_df = pd.DataFrame.from_dict(iprs_count_dict, orient='index', columns=['iprS_InterPro_annotations-accession'])
#and pull the OG data out of the index
final_iprs_count_df.reset_index(inplace=True)
#before renaming it according to the OG program being used
final_iprs_count_df.rename(columns={'index': og_col}, inplace=True)
#then the GO annotation data
final_go_count_df = pd.DataFrame.from_dict(go_count_dict, orient='index', columns=['iprS_GO_annotations'])
#and pull the OG data out of the index
final_go_count_df.reset_index(inplace=True)
#before renaming it according to the OG program being used
final_go_count_df.rename(columns={'index': og_col}, inplace=True)


#Part 5: Create simple dataframes showing OGs and associated PFams

#aggregate IDs into a comma-separated string, and remove duplicate hits
iprs_og_df['iprS_InterPro_annotations-accession'] = iprs_og_df['iprS_InterPro_annotations-accession'].apply(lambda x: ','.join(sorted(list(set(x.split(','))))))
go_og_df['iprS_GO_annotations'] = go_og_df['iprS_GO_annotations'].apply(lambda x: ','.join(sorted(list(set(x.split(','))))))

#remove "-" "hits" from the column of data
iprs_og_df['iprS_InterPro_annotations-accession'] = iprs_og_df['iprS_InterPro_annotations-accession'].str.replace('-,', '')
go_og_df['iprS_GO_annotations'] = go_og_df['iprS_GO_annotations'].str.replace('-,', '')
#also remove OGs without annotation hits
final_iprs_og_df = iprs_og_df[iprs_og_df['iprS_InterPro_annotations-accession'] != "-"].copy()
final_go_og_df = go_og_df[go_og_df['iprS_GO_annotations'] != "-"].copy()


#Part 6: Write out results

#write out count data to tab-separated text files
final_iprs_count_df.to_csv(output_iprs_counts, sep='\t', index=False)
final_go_count_df.to_csv(output_go_counts, sep='\t', index=False)

#write out OGs to annotations simplified data to tab-separated text files
final_iprs_og_df.to_csv(output_iprs, sep='\t', index=False)
final_go_og_df.to_csv(output_go, sep='\t', index=False)

```

Using it: 

```bash
#working in the /home/inf-47-2020/ThesisTrich/DB_Construct/ directory
#model: 
python og2IPRS_pivot.py input_db og_col
#applying it: 
python ../Scripts/og2IPRS_pivot.py Metamonada_Alanta_pred_OG_DB.txt Br_Grouped_OGs
python ../Scripts/og2IPRS_pivot.py Metamonada_Alanta_pred_OG_DB.txt Br_Single_OGs
python ../Scripts/og2IPRS_pivot.py Metamonada_Alanta_pred_OG_DB.txt ProteinOrtho_OG
python ../Scripts/og2IPRS_pivot.py Metamonada_Alanta_pred_OG_DB.txt OrthoFinder_OG
python ../Scripts/og2IPRS_pivot.py Metamonada_Alanta_pred_OG_DB.txt SonicParanoid_OG

```

Did the above because Courtney asked me to grab the IPR accession numbers (and I wanted to grab the GO terms) associated with the key PFam domains: 

```bash
#Iron hydrogenase OG
grep "OG0000060" OGs2IPRS_OrthoFinder_OG.txt
# OG0000060       IPR000283,IPR000315,IPR001041,IPR001094,IPR001226,IPR001433,IPR001680,IPR001709,IPR002110,IPR003097,IPR003149,IPR004108,IPR008254,IPR008953,IPR009016,IPR011041,IPR011047,IPR011990,IPR013352,IPR013783,IPR014756,IPR015943,IPR017868,IPR017896,IPR017900,IPR017927,IPR017938,IPR019574,IPR019775,IPR020472,IPR023173,IPR029039,IPR036010,IPR036322,IPR036770,IPR036991,IPR039261
grep "OG0000060" OGs2GOs_OrthoFinder_OG.txt
# OG0000060       0,1,3,5,6,:,G,GO:0003824,GO:0005506,GO:0005515,GO:0008137,GO:0008270,GO:0008901,GO:0009055,GO:0010181,GO:0016020,GO:0016491,GO:0042597,GO:0042773,GO:0051536,O
#realizing that the above is definitely wrong!!! 
#I think the script is doing that thing where single results get split into single letters -_-;;

grep "OG_55" OGs2GOs_SonicParanoid_OG.txt
# OG_55   GO:0003824,GO:0005506,GO:0005515,GO:0008137,GO:0008270,GO:0008901,GO:0009055,GO:0010181,GO:0016020,GO:0016491,GO:0042597,GO:0042773,GO:0051536
grep "OG_55" OGs2IPRS_SonicParanoid_OG.txt
# OG_55   IPR000283,IPR000315,IPR000813,IPR001041,IPR001094,IPR001226,IPR001433,IPR001680,IPR001709,IPR003097,IPR003149,IPR004108,IPR006004,IPR008254,IPR008953,IPR009016,IPR009051,IPR011041,IPR011047,IPR013352,IPR013783,IPR014756,IPR015943,IPR017868,IPR017896,IPR017900,IPR017927,IPR017938,IPR019574,IPR019775,IPR020472,IPR023173,IPR023753,IPR027631,IPR028261,IPR029039,IPR036010,IPR036188,IPR036322,IPR036991,IPR039261

#SNARE OG
grep "OG0002700" OGs2GOs_OrthoFinder_OG.txt
# OG0002700       GO:0005484,GO:0005794,GO:0006886,GO:0006890,GO:0016020,GO:0016192,GO:0031201
grep "OG0002700" OGs2IPRS_OrthoFinder_OG.txt
# OG0002700       IPR000727,IPR005606,IPR007705,IPR010989,IPR027027,IPR029004,IPR038407,IPR044766
grep "OG_1245" OGs2GOs_SonicParanoid_OG.txt
# OG_1245 GO:0005484,GO:0005794,GO:0006886,GO:0006890,GO:0016020,GO:0016192,GO:0031201
grep "OG_1245" OGs2IPRS_SonicParanoid_OG.txt
# OG_1245 IPR000727,IPR005606,IPR007705,IPR010989,IPR027027,IPR029004,IPR038407,IPR044766

```

Finding an Fe hydrgenase model to use for the OG figure: 
 - AlphaFold database search: 
   - https://alphafold.ebi.ac.uk/
   - Search: "Fe hydrogenase trichomonas vaginalis" when filtered by organism for _Trichomonas vaginalis_ yielded 48861 results: https://alphafold.ebi.ac.uk/search/text/Fe%20hydrogenase%20trichomonas%20vaginalis?organismScientificName=Trichomonas%20vaginalis
   - The second entry was for a protein that had a TVAG Gene ID: TVAG_320010 (https://alphafold.ebi.ac.uk/entry/A2DQE9)
 - Searching through my own files to find project encoded protein ID: 
   - TrichDB-57_TvaginalisG3_GeneAliases.txt: TVAG_320010	91127.t00012	EAY17406
   - encodingSummary_ref.txt: o1UmT2yBXVMaNB1b	EAY17406.1 iron hydrogenase [Trichomonas vaginalis G3]
 - Searching for presence in key OGs: 
   - Present in OF_Mito3_OG0000060_MSAprep.fasta: 
     - >T_vaginalis_GenBank__o1UmT2yBXVMaNB1b
     - MLASSATAMKGFANSLRMKDYSSTGINFDMTKCINCQSCVRACTNIAGQNVLKSLTVNGKSVVQTVTGKPLAETNCISCGQCTLGCPKFTIFEADAINPVKEVLTKKNGRIAVCQIAPAIRINMAEALGVPAGTISLGKVVTALKRLGFDYVFDTNFAADMTIVEEATELVQRLSDKNAVLPMFTSCCPAWVNYVEKSDPSLIPHLSSCRSPMSMLSSVIKNVFPKKIGTTADKIYNVAIMPCTAKKDEIQRSQFTMKDGKQETGAVLTSRELAKMIKEAKINFKELPDTPCDNFYSEASGGGAIFCATGGVMEAAVRSAYKFLTKKELAPIDLQDVRGVASGVKLAEVDIAGTKVKVAVAHGIKNAMTLIKKIKSGEEQFKDVKFVEVMACPGGCVVGGGSPKAKTKKAVQARLNATYSIDKSSKHRTSQDNPQLLQLYKESFEGKFGGHVAHHLLHTHYKNRKVNP
   - Present in SP_Mito3__OG_55_MSAprep.fasta: 
     - >T_vaginalis_GenBank__o1UmT2yBXVMaNB1b
     - MLASSATAMKGFANSLRMKDYSSTGINFDMTKCINCQSCVRACTNIAGQNVLKSLTVNGKSVVQTVTGKPLAETNCISCGQCTLGCPKFTIFEADAINPVKEVLTKKNGRIAVCQIAPAIRINMAEALGVPAGTISLGKVVTALKRLGFDYVFDTNFAADMTIVEEATELVQRLSDKNAVLPMFTSCCPAWVNYVEKSDPSLIPHLSSCRSPMSMLSSVIKNVFPKKIGTTADKIYNVAIMPCTAKKDEIQRSQFTMKDGKQETGAVLTSRELAKMIKEAKINFKELPDTPCDNFYSEASGGGAIFCATGGVMEAAVRSAYKFLTKKELAPIDLQDVRGVASGVKLAEVDIAGTKVKVAVAHGIKNAMTLIKKIKSGEEQFKDVKFVEVMACPGGCVVGGGSPKAKTKKAVQARLNATYSIDKSSKHRTSQDNPQLLQLYKESFEGKFGGHVAHHLLHTHYKNRKVNP
 - Therefore, while this is not a model of the control protein that I used to identify these OGs, I think it works as a suitable replacement. 
   - Dowloaded PDB file: AF-A2DQE9-F1-model_v3.pdb
   - Visualized using UCSF ChimeraX v. 1.4
 - Or, on second try, using the TVAG Gene ID, it turns out that the control protein actually is modeled in the AlphaFold database: 
 - TVAG ID: TVAG_037570
   - Search results: https://alphafold.ebi.ac.uk/search/text/TVAG_037570
   - Protein results page: https://alphafold.ebi.ac.uk/entry/A2FCW4
   - Dowloaded PDB file: AF-A2FCW4-F1-model_v3.pdb
   - Visualized using UCSF ChimeraX v. 1.4


## Citations




## Versions

f
